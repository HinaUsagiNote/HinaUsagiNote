# 성능개선 기법 실습

# 모듈 정의

## train.py
- 모델 학습과 검증 함수 정의


```python
import os
os.makedirs('module', exist_ok=True)
```


```python
# %%writefile module/train.py

import torch
import time

# multi와 binary 정확도 계산이 다르다.

def test_multi_classification(dataloader, model, loss_fn, device="cpu") -> tuple:
    """
    다중 분류 검증/평가 함수
    
    [parameter]
        dataloader: DataLoader - 검증할 대상 데이터로더
        model: 검증할 모델
        loss_fn: 모델 추정값과 정답의 차이를 계산할 loss 함수.
        device: str - 연산을 처리할 장치. default-"cpu", gpu-"cuda"
    [return]
        tuple: (loss, accuracy)
    """
    model.eval() # 모델을 평가모드로 변환
    size = len(dataloader.dataset) # 전체 데이터수
    num_batches = len(dataloader)  #  step 수
    
    test_loss, test_accuracy = 0., 0.
    
    with torch.no_grad():
        for X, y in dataloader:
            X, y = X.to(device), y.to(device)
            pred = model(X)
            test_loss += loss_fn(pred, y).item() # batch 별 loss의 누적
            # 정확도 계산
            pred_label = torch.argmax(pred, axis=-1)
            test_accuracy += torch.sum(pred_label == y).item() # 맞은 개수의 누적
            
        test_loss /= num_batches # batch_size로 나눈다
        test_accuracy /= size  #전체 개수로 나눈다.
    return test_loss, test_accuracy

def test_binary_classification(dataloader, model, loss_fn, device="cpu") -> tuple:
    """
    이진 분류 검증/평가 함수
    
    [parameter]
        dataloader: DataLoader - 검증할 대상 데이터로더
        model: 검증할 모델
        loss_fn: 모델 추정값과 정답의 차이를 계산할 loss 함수.
        device: str - 연산을 처리할 장치. default-"cpu", gpu-"cuda"
    [return]
        tuple: (loss, accuracy)
    """
    model.eval() # 모델을 평가모드로 변환
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    
    test_loss, test_accuracy = 0., 0.
    
    with torch.no_grad():
        for X, y in dataloader:
            X, y = X.to(device), y.to(device)
            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            ## 정확도 계산
            pred_label = (pred >= 0.5).type(torch.int32)
            test_accuracy += (pred_label == y).sum().item() 
            
        test_loss /= num_batches
        test_accuracy /= size   #전체 개수로 나눈다.
    return test_loss, test_accuracy    

def train(dataloader, model, loss_fn, optimizer, device="cpu", mode:"binary or multi"='binary'):
    """
    모델을 1 epoch 학습시키는 함수

    [parameter]
        dataloader: DataLoader - 학습데이터셋을 제공하는 DataLoader
        model - 학습대상 모델
        loss_fn: 모델 추정값과 정답의 차이를 계산할 loss 함수.
        optimizer - 최적화 함수
        device: str - 연산을 처리할 장치. default-"cpu", gpu-"cuda"
        mode: str - 분류 종류. binary 또는 multi
    [return]
        tuple: 학습후 계산한 Train set에 대한  train_loss, train_accuracy
    """
    model.train()
    size = len(dataloader.dataset) #총 데이터수

    for batch, (X, y) in enumerate(dataloader):
        X, y = X.to(device), y.to(device)
        pred = model(X)

        loss = loss_fn(pred, y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
    if mode == 'binary':
        train_loss, train_accuracy = test_binary_classification(dataloader, model, loss_fn, device)
    else:
        train_loss, train_accuracy = test_multi_classification(dataloader, model, loss_fn, device)
    return train_loss, train_accuracy



def fit(train_loader, val_loader, model, loss_fn, optimizer, epochs, save_best_model=True, save_model_path=None, 
        early_stopping=True, patience=10, device='cpu',  mode:"binary or multi"='binary'
        lr_scheduler=None
       ):
    """
    모델을 학습시키는 함수

    [parameter]
        train_loader (Dataloader): Train dataloader
        test_loader (Dataloader): validation dataloader
        model (Module): 학습시킬 모델
        loss_fn (_Loss): Loss function
        optimizer (Optimizer): Optimizer
        epochs (int): epoch수
        
        save_best_model (bool, optional): 학습도중 성능개선시 모델 저장 여부. Defaults to True.
        save_model_path (str, optional): save_best_model=True일 때 모델저장할 파일 경로. Defaults to None.
        early_stopping (bool, optional): 조기 종료 여부. Defaults to True.
        patience (int, optional): 조기종료 True일 때 종료전에 성능이 개선될지 몇 epoch까지 기다릴지 epoch수. Defaults to 10.
        device (str, optional): device. Defaults to 'cpu'.
        mode(str, optinal): 분류 종류. "binary(default) or multi
        lr_scheduler: Learning Rate 스케쥴러 객체.
    [return]
        tuple: 에폭 별 성능 리스트. (train_loss_list, train_accuracy_list, validation_loss_list, validataion_accuracy_list)
    """

    train_loss_list = []
    train_accuracy_list = []
    val_loss_list = []
    val_accuracy_list = []
    
        
    if save_best_model:
        best_score_save = torch.inf

    ############################
    # early stopping
    #############################
    if early_stopping:
        trigger_count = 0
        best_score_es = torch.inf
    
    # 모델 device로 옮기기
    model = model.to(device)
    s = time.time()
    for epoch in range(epochs):
        
        ###### Train() 함수 호출
        train_loss, train_accuracy = train(train_loader, model, loss_fn, optimizer, device=device, mode=mode)
        
        ###### 검증 - Test_xxx_classification() 함수호출
        if mode == "binary":
            val_loss, val_accuracy = test_binary_classification(val_loader, model, loss_fn, device=device)
        else:
            val_loss, val_accuracy = test_multi_classification(val_loader, model, loss_fn, device=device)

        ########### LR Scheduler에게 lr 변경요청.
        if lr_scheduler:
            lr_scheduler.step()
        
        # 현 에폭의 검증겨로가를 각 list에 추가
        train_loss_list.append(train_loss)
        train_accuracy_list.append(train_accuracy)
        val_loss_list.append(val_loss)
        val_accuracy_list.append(val_accuracy)

        # 로그 출력
        print(f"Epoch[{epoch+1}/{epochs}] - Train loss: {train_loss:.5f} Train Accucracy: {train_accuracy:.5f} || Validation Loss: {val_loss:.5f} Validation Accuracy: {val_accuracy:.5f}")
        print('='*100)
        
        # 모델 저장
        if save_best_model:
            if val_loss < best_score_save:
                torch.save(model, save_model_path)
                print(f"저장: {epoch+1} - 이전 : {best_score_save}, 현재: {val_loss}")
                best_score_save = val_loss
        
        # early stopping 처리            
        if early_stopping:
            if val_loss < best_score_es: 
                best_score_es = val_loss  
                trigger_count = 0
                                
            else:
                trigger_count += 1                
                if patience == trigger_count:
                    print(f"Early stopping: Epoch - {epoch}")
                    break
            
    e = time.time()
    print(e-s, "초")
    return train_loss_list, train_accuracy_list, val_loss_list, val_accuracy_list
```

## data.py
- dataset 생성 함수 제공 모듈


```python
# %%writefile module/data.py

from torchvision import datasets, transforms
from torch.utils.data import DataLoader

def load_mnist_dataset(root_path, batch_size, is_train=True):
    """
    mnist dataset dataloader 제공 함수
    [parameter]
        root_path: str|Path - 데이터파일 저장 디렉토리
        batch_size: int
        is_train: bool = True - True: Train dataset, False - Test dataset
    [return]
        DataLoader 
    """
    transform = transforms.Compose([
        transforms.ToTensor()
    ])
    dataset = datasets.MNIST(root=root_path, train=is_train, download=True, transform=transform)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=is_train)  # shuffle: train이면 True, test면 False 할 것이므로 is_train을 넣음.
    
    return dataloader

def load_fashion_mnist_dataset(root_path, batch_size, is_train=True):
    """
    fashion mnist dataset dataloader 제공 함수
    [parameter]
        root_path: str|Path - 데이터파일 저장 디렉토리
        batch_size: int
        is_train: bool = True - True: Train dataset, False - Test dataset
    [return]
        DataLoader
    """
    transform = transforms.Compose([
        transforms.ToTensor()
    ])
    dataset = datasets.FashionMNIST(root=root_path, train=is_train, download=True, transform=transform)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=is_train)  # shuffle: train이면 True, test면 False 할 것이므로 is_train을 넣음.
    
    return dataloader
```


```python
# 임포트
from module import data
```


```python
mnist_trainloader = data.load_mnist_dataset('datasets', # root
                                        200,            # batch_size
                                        True)           # download
mnist_testloader = data.load_mnist_dataset('datasets', 200, False)
```


```python
# 데이터수
len(mnist_trainloader.dataset), len(mnist_testloader.dataset)
```




    (60000, 10000)




```python
# step 수
len(mnist_trainloader), len(mnist_testloader)
```




    (300, 50)



## import


```python
import torch
import torch.nn as nn

import torchinfo

from module import train, data

import os
import matplotlib.pyplot as plt

device = 'cuda' if torch.cuda.is_available() else 'cpu'
device
```




    'cpu'



## 하이퍼파라미터, 변수 정의


```python
LR = 0.001
N_EPOCH = 10
BATCH_SIZE = 200

# DATASET, MODEL 저장할 ROOT 디렉토리 경로
DATASET_ROOT_PATH = 'datasets'
MODEL_SAVE_ROOT_PATH = 'models'
```

# Data 준비

## mnist 데이터 로딩


```python
train_loader = data.load_mnist_dataset(DATASET_ROOT_PATH, BATCH_SIZE, True)
test_loader = data.load_mnist_dataset(DATASET_ROOT_PATH, BATCH_SIZE, False)
```

# 모델의 크기 변경에 따른 성능변화


```python
class SmallSizeModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.output = nn.Linear(784, 10) # in: freature - (1*28*28), out: class 개수(10)

    def forward(self, X):
        out = nn.Flatten()(X)
        out = self.output(out)
        return out
```


```python
small_model = SmallSizeModel()
torchinfo.summary(small_model, (BATCH_SIZE, 1, 28, 28))
```




    ==========================================================================================
    Layer (type:depth-idx)                   Output Shape              Param #
    ==========================================================================================
    SmallSizeModel                           [200, 10]                 --
    ├─Linear: 1-1                            [200, 10]                 7,850
    ==========================================================================================
    Total params: 7,850
    Trainable params: 7,850
    Non-trainable params: 0
    Total mult-adds (M): 1.57
    ==========================================================================================
    Input size (MB): 0.63
    Forward/backward pass size (MB): 0.02
    Params size (MB): 0.03
    Estimated Total Size (MB): 0.67
    ==========================================================================================




```python
# loss
loss_fn = nn.CrossEntropyLoss()
# optimizer
optimizer = torch.optim.Adam(small_model.parameters(), lr=LR)
```


```python
# Train -> module.fit()
# train.fit? # train.fit에 대한 파라미터 정보 및 확인

train_loss_list, train_acc_list, valid_loss_list, valid_acc_list = train.fit(train_loader, test_loader, small_model, loss_fn, optimizer, N_EPOCH, 
                                                                        save_best_model = True,
                                                                        save_model_path = os.path.join(MODEL_SAVE_ROOT_PATH, 'small_model.pth'),
                                                                        device = device,
                                                                        mode = 'multi'
                                                                       )
```

    Epoch[1/10] - Train loss: 0.25697 Train Accucracy: 0.92845 || Validation Loss: 0.26678 Validation Accuracy: 0.92490
    ====================================================================================================
    저장: 1 - 이전 : inf, 현재: 0.2667800550162792
    Epoch[2/10] - Train loss: 0.25571 Train Accucracy: 0.92933 || Validation Loss: 0.26564 Validation Accuracy: 0.92600
    ====================================================================================================
    저장: 2 - 이전 : 0.2667800550162792, 현재: 0.2656371323391795
    Epoch[3/10] - Train loss: 0.25386 Train Accucracy: 0.92945 || Validation Loss: 0.26377 Validation Accuracy: 0.92720
    ====================================================================================================
    저장: 3 - 이전 : 0.2656371323391795, 현재: 0.26376935966312887
    Epoch[4/10] - Train loss: 0.25162 Train Accucracy: 0.93032 || Validation Loss: 0.26275 Validation Accuracy: 0.92710
    ====================================================================================================
    저장: 4 - 이전 : 0.26376935966312887, 현재: 0.262746288664639
    Epoch[5/10] - Train loss: 0.25132 Train Accucracy: 0.93035 || Validation Loss: 0.26568 Validation Accuracy: 0.92650
    ====================================================================================================
    Epoch[6/10] - Train loss: 0.24862 Train Accucracy: 0.93142 || Validation Loss: 0.26272 Validation Accuracy: 0.92770
    ====================================================================================================
    저장: 6 - 이전 : 0.262746288664639, 현재: 0.26272219385951756
    Epoch[7/10] - Train loss: 0.24762 Train Accucracy: 0.93178 || Validation Loss: 0.26294 Validation Accuracy: 0.92690
    ====================================================================================================
    Epoch[8/10] - Train loss: 0.24775 Train Accucracy: 0.93152 || Validation Loss: 0.26431 Validation Accuracy: 0.92690
    ====================================================================================================
    Epoch[9/10] - Train loss: 0.24701 Train Accucracy: 0.93122 || Validation Loss: 0.26319 Validation Accuracy: 0.92650
    ====================================================================================================
    Epoch[10/10] - Train loss: 0.24443 Train Accucracy: 0.93243 || Validation Loss: 0.26091 Validation Accuracy: 0.92810
    ====================================================================================================
    저장: 10 - 이전 : 0.26272219385951756, 현재: 0.2609139801561832
    201.4175112247467 초
    


```python
plt.figure(figsize=(10, 5))
plt.subplot(1, 2 ,1)
plt.plot(train_loss_list, label='Trainset')
plt.plot(valid_loss_list, label='Validationset')
plt.title('Loss')
plt.legend()

plt.subplot(1, 2 ,2)
plt.plot(train_acc_list, label='Trainset')
plt.plot(valid_acc_list, label='Validationset')
plt.title('Accuracy')
plt.legend()

plt.show()
```


    
![png](output_22_0.png)
    



```python
## nn.Sequential()을 이용해 Layer block 정의하기
### 여러 Layer들을 묶어준다.
layer_block = nn.Sequential(nn.Linear(20, 30), nn.ReLU())

a = torch.ones(1, 20)
r = layer_block(a)
r.shape
r
```




    tensor([[0.4321, 0.0000, 0.0225, 0.0000, 1.0276, 0.3930, 0.0000, 0.0000, 1.0382,
             0.1719, 0.0000, 0.3929, 0.0000, 0.0000, 0.0000, 1.1328, 0.4481, 0.0000,
             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
             0.0000, 0.3957, 0.2792]], grad_fn=<ReluBackward0>)




```python
class BigSizeModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.b1 = nn.Sequential(nn.Linear(784, 2048), nn.ReLU())
        self.b2 = nn.Sequential(nn.Linear(2048, 1024), nn.ReLU())
        self.b3 = nn.Sequential(nn.Linear(1024, 512), nn.ReLU())
        self.b4 = nn.Sequential(nn.Linear(512, 256), nn.ReLU())
        self.b5 = nn.Sequential(nn.Linear(256, 128), nn.ReLU())
        self.b6 = nn.Sequential(nn.Linear(128, 64), nn.ReLU())

        self.output = nn.Linear(64, 10)
    
    def forward(self, X):
        X = nn.Flatten()(X)
        out = self.b1(X)
        out = self.b2(out)
        out = self.b3(out)
        out = self.b4(out)
        out = self.b5(out)
        out = self.b6(out)

        return self.output(out)
```


```python
big_model = BigSizeModel()
torchinfo.summary(big_model, (BATCH_SIZE, 1, 28, 28))
```




    ==========================================================================================
    Layer (type:depth-idx)                   Output Shape              Param #
    ==========================================================================================
    BigSizeModel                             [200, 10]                 --
    ├─Sequential: 1-1                        [200, 2048]               --
    │    └─Linear: 2-1                       [200, 2048]               1,607,680
    │    └─ReLU: 2-2                         [200, 2048]               --
    ├─Sequential: 1-2                        [200, 1024]               --
    │    └─Linear: 2-3                       [200, 1024]               2,098,176
    │    └─ReLU: 2-4                         [200, 1024]               --
    ├─Sequential: 1-3                        [200, 512]                --
    │    └─Linear: 2-5                       [200, 512]                524,800
    │    └─ReLU: 2-6                         [200, 512]                --
    ├─Sequential: 1-4                        [200, 256]                --
    │    └─Linear: 2-7                       [200, 256]                131,328
    │    └─ReLU: 2-8                         [200, 256]                --
    ├─Sequential: 1-5                        [200, 128]                --
    │    └─Linear: 2-9                       [200, 128]                32,896
    │    └─ReLU: 2-10                        [200, 128]                --
    ├─Sequential: 1-6                        [200, 64]                 --
    │    └─Linear: 2-11                      [200, 64]                 8,256
    │    └─ReLU: 2-12                        [200, 64]                 --
    ├─Linear: 1-7                            [200, 10]                 650
    ==========================================================================================
    Total params: 4,403,786
    Trainable params: 4,403,786
    Non-trainable params: 0
    Total mult-adds (M): 880.76
    ==========================================================================================
    Input size (MB): 0.63
    Forward/backward pass size (MB): 6.47
    Params size (MB): 17.62
    Estimated Total Size (MB): 24.71
    ==========================================================================================




```python
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(big_model.parameters(), lr=LR)
```


```python
# 학습
train_loss_list2, train_acc_list2, valid_loss_list2, valid_acc_list2 = train.fit(train_loader, test_loader, big_model, loss_fn, optimizer, N_EPOCH, 
                                                                        save_best_model = True,
                                                                        save_model_path = os.path.join(MODEL_SAVE_ROOT_PATH, 'big_model.pth'),
                                                                        device = device,
                                                                        mode = 'multi'
                                                                       )
```

    Epoch[1/10] - Train loss: 0.12701 Train Accucracy: 0.96365 || Validation Loss: 0.13668 Validation Accuracy: 0.95990
    ====================================================================================================
    저장: 1 - 이전 : inf, 현재: 0.13668256090953945
    Epoch[2/10] - Train loss: 0.08261 Train Accucracy: 0.97612 || Validation Loss: 0.11028 Validation Accuracy: 0.96620
    ====================================================================================================
    저장: 2 - 이전 : 0.13668256090953945, 현재: 0.11027811136096716
    Epoch[3/10] - Train loss: 0.06124 Train Accucracy: 0.98188 || Validation Loss: 0.09488 Validation Accuracy: 0.97310
    ====================================================================================================
    저장: 3 - 이전 : 0.11027811136096716, 현재: 0.09488440541084855
    Epoch[4/10] - Train loss: 0.04865 Train Accucracy: 0.98540 || Validation Loss: 0.10949 Validation Accuracy: 0.97190
    ====================================================================================================
    Epoch[5/10] - Train loss: 0.02825 Train Accucracy: 0.99143 || Validation Loss: 0.07965 Validation Accuracy: 0.97960
    ====================================================================================================
    저장: 5 - 이전 : 0.09488440541084855, 현재: 0.07964686637074919
    Epoch[6/10] - Train loss: 0.02424 Train Accucracy: 0.99297 || Validation Loss: 0.08496 Validation Accuracy: 0.97690
    ====================================================================================================
    Epoch[7/10] - Train loss: 0.02400 Train Accucracy: 0.99278 || Validation Loss: 0.07517 Validation Accuracy: 0.97990
    ====================================================================================================
    저장: 7 - 이전 : 0.07964686637074919, 현재: 0.07516641994036036
    Epoch[8/10] - Train loss: 0.02564 Train Accucracy: 0.99178 || Validation Loss: 0.07979 Validation Accuracy: 0.97880
    ====================================================================================================
    Epoch[9/10] - Train loss: 0.01986 Train Accucracy: 0.99365 || Validation Loss: 0.09614 Validation Accuracy: 0.97800
    ====================================================================================================
    Epoch[10/10] - Train loss: 0.03558 Train Accucracy: 0.98858 || Validation Loss: 0.11282 Validation Accuracy: 0.97350
    ====================================================================================================
    492.53137588500977 초
    


```python
plt.figure(figsize=(10, 5))
plt.subplot(1, 2 ,1)
plt.plot(train_loss_list2, label='Trainset')
plt.plot(valid_loss_list2, label='Validationset')
plt.title('Loss')
plt.legend()

plt.subplot(1, 2 ,2)
plt.plot(train_acc_list2, label='Trainset')
plt.plot(valid_acc_list2, label='Validationset')
plt.title('Accuracy')
plt.legend()

plt.show()
```


    
![png](output_28_0.png)
    


# Dropout 예제
- dropout 각 레이어에 적용
    - dropout은 nn.Dropout 객체를 사용
    - 객체 생성시 dropout_rate 설정: 0.2 ~ 0.5
    - Drop시킬 노드를 가진 Layer 뒤에 추가한다.


```python
import torch
import torch.nn as nn

import torchinfo

from module import train, data

import os
import matplotlib.pyplot as plt
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device
```




    'cpu'




```python
LR = 0.001
N_EPOCH = 10
BATCH_SIZE = 200
```


```python
# DATASET, MODEL 저장할 ROOT 디렉토리 경로
DATASET_ROOT_PATH = 'datasets'
MODEL_SAVE_ROOT_PATH = 'models'

train_loader = data.load_mnist_dataset(DATASET_ROOT_PATH, BATCH_SIZE, True)
test_loader = data.load_mnist_dataset(DATASET_ROOT_PATH, BATCH_SIZE, False)
```


```python
class DropoutModel(nn.Module):

    def __init__(self, drop_rate=0.5):
        super().__init__()
        self.b1 = nn.Sequential(nn.Linear(784, 256),
                                nn.ReLU(),
                                nn.Dropout(p=drop_rate)
                               )
        self.b2 = nn.Sequential(nn.Linear(256, 256), nn.ReLU(), nn.Dropout(p=drop_rate))
        self.b3 = nn.Sequential(nn.Linear(256, 128), nn.ReLU(), nn.Dropout(p=drop_rate))
        self.b4 = nn.Sequential(nn.Linear(128, 128), nn.ReLU(), nn.Dropout(p=drop_rate))
        self.output = nn.Sequential(nn.Linear(128, 10), nn.Dropout(p=drop_rate))
    
    def forward(self, X):
        out = nn.Flatten()(X)
        out = self.b1(out)
        out = self.b2(out)
        out = self.b3(out)
        out = self.b4(out)
        out = self.output(out)
        return out
```


```python
d_model = DropoutModel().to(device) # drop_rate=0.5(Default)
torchinfo.summary(d_model, (BATCH_SIZE, 1, 28, 28))
```




    ==========================================================================================
    Layer (type:depth-idx)                   Output Shape              Param #
    ==========================================================================================
    DropoutModel                             [200, 10]                 --
    ├─Sequential: 1-1                        [200, 256]                --
    │    └─Linear: 2-1                       [200, 256]                200,960
    │    └─ReLU: 2-2                         [200, 256]                --
    │    └─Dropout: 2-3                      [200, 256]                --
    ├─Sequential: 1-2                        [200, 256]                --
    │    └─Linear: 2-4                       [200, 256]                65,792
    │    └─ReLU: 2-5                         [200, 256]                --
    │    └─Dropout: 2-6                      [200, 256]                --
    ├─Sequential: 1-3                        [200, 128]                --
    │    └─Linear: 2-7                       [200, 128]                32,896
    │    └─ReLU: 2-8                         [200, 128]                --
    │    └─Dropout: 2-9                      [200, 128]                --
    ├─Sequential: 1-4                        [200, 128]                --
    │    └─Linear: 2-10                      [200, 128]                16,512
    │    └─ReLU: 2-11                        [200, 128]                --
    │    └─Dropout: 2-12                     [200, 128]                --
    ├─Sequential: 1-5                        [200, 10]                 --
    │    └─Linear: 2-13                      [200, 10]                 1,290
    │    └─Dropout: 2-14                     [200, 10]                 --
    ==========================================================================================
    Total params: 317,450
    Trainable params: 317,450
    Non-trainable params: 0
    Total mult-adds (M): 63.49
    ==========================================================================================
    Input size (MB): 0.63
    Forward/backward pass size (MB): 1.24
    Params size (MB): 1.27
    Estimated Total Size (MB): 3.14
    ==========================================================================================




```python
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(d_model.parameters(), lr=LR)

result = train.fit(train_loader, test_loader, d_model, loss_fn, optimizer, N_EPOCH, save_best_model=False, early_stopping=False, device=device, mode='multi')
```

    Epoch[1/10] - Train loss: 0.29718 Train Accucracy: 0.93165 || Validation Loss: 0.29076 Validation Accuracy: 0.93140
    ====================================================================================================
    Epoch[2/10] - Train loss: 0.20175 Train Accucracy: 0.95115 || Validation Loss: 0.20255 Validation Accuracy: 0.94890
    ====================================================================================================
    Epoch[3/10] - Train loss: 0.16657 Train Accucracy: 0.96088 || Validation Loss: 0.17110 Validation Accuracy: 0.95880
    ====================================================================================================
    Epoch[4/10] - Train loss: 0.12829 Train Accucracy: 0.96875 || Validation Loss: 0.14193 Validation Accuracy: 0.96140
    ====================================================================================================
    Epoch[5/10] - Train loss: 0.12563 Train Accucracy: 0.97037 || Validation Loss: 0.13734 Validation Accuracy: 0.96710
    ====================================================================================================
    Epoch[6/10] - Train loss: 0.11393 Train Accucracy: 0.97392 || Validation Loss: 0.12805 Validation Accuracy: 0.97030
    ====================================================================================================
    Epoch[7/10] - Train loss: 0.11319 Train Accucracy: 0.97408 || Validation Loss: 0.13106 Validation Accuracy: 0.96800
    ====================================================================================================
    Epoch[8/10] - Train loss: 0.09571 Train Accucracy: 0.97757 || Validation Loss: 0.11463 Validation Accuracy: 0.97140
    ====================================================================================================
    Epoch[9/10] - Train loss: 0.09665 Train Accucracy: 0.97868 || Validation Loss: 0.11790 Validation Accuracy: 0.97390
    ====================================================================================================
    Epoch[10/10] - Train loss: 0.08790 Train Accucracy: 0.97927 || Validation Loss: 0.11196 Validation Accuracy: 0.97440
    ====================================================================================================
    208.18259477615356 초
    


```python
# 표준정규
a = torch.randn(5, 3)
ln = nn.Linear(3, 4)
b = ln(a)
do = nn.Dropout(0.5)
c = do(b)
```


```python
a # input
b # linear
```




    tensor([[-0.0559, -0.1409, -0.3585,  0.1669],
            [-0.4429,  1.1053, -0.6790,  0.6994],
            [-0.5131, -0.1918, -0.8159,  0.4282],
            [-0.2911,  0.0528, -0.5525,  0.4402],
            [-0.9957, -0.8803, -1.1406,  1.0986]], grad_fn=<AddmmBackward0>)




```python
c
```




    tensor([[-0.1118, -0.0000, -0.0000,  0.0000],
            [-0.0000,  2.2106, -0.0000,  1.3987],
            [-1.0261, -0.0000, -1.6318,  0.0000],
            [-0.0000,  0.0000, -1.1051,  0.0000],
            [-1.9914, -0.0000, -0.0000,  2.1971]], grad_fn=<MulBackward0>)



# Batch Normalization
- Dense => BN => Activation


```python
a = torch.rand(10, 5) # (10-data수, 5-feature수)
bn = nn.BatchNorm1d(5) # features => 1차원: BatchNorm1d(), 3차원(이미지): BatchNorm2d()
## BatchNorm1d(feature수)
## BatchNorm2d(channel수)
b = bn(a)
```


```python
# featuer별 평균/표준편차
print('a의 평균:', a.mean(axis=0))
print('a의 표준편차', a.std(axis=0))
```

    a의 평균: tensor([0.4685, 0.5811, 0.4800, 0.5975, 0.4469])
    a의 표준편차 tensor([0.2842, 0.2615, 0.2714, 0.2332, 0.2747])
    


```python
# BatchNorm1d 결과의 featur 별 평균/표준편차
print('b의 평균:', b.mean(axis=0))
print('b의 표준편차', b.std(axis=0))
```

    b의 평균: tensor([3.5763e-08, 8.3447e-08, 5.9605e-08, 1.1921e-07, 1.1921e-08],
           grad_fn=<MeanBackward1>)
    b의 표준편차 tensor([1.0540, 1.0540, 1.0540, 1.0540, 1.0540], grad_fn=<StdBackward0>)
    


```python
a
```




    tensor([[0.6175, 0.7865, 0.4116, 0.1729, 0.2551],
            [0.2601, 0.8940, 0.5634, 0.5865, 0.6799],
            [0.7627, 0.7826, 0.4996, 0.5271, 0.6931],
            [0.7150, 0.8986, 0.8027, 0.5736, 0.4079],
            [0.1409, 0.5249, 0.1900, 0.6231, 0.9438],
            [0.7002, 0.4319, 0.2278, 0.9505, 0.0989],
            [0.5890, 0.5548, 0.6405, 0.5524, 0.3799],
            [0.1221, 0.1146, 0.0641, 0.7287, 0.6095],
            [0.0679, 0.5399, 0.4732, 0.3502, 0.1469],
            [0.7099, 0.2835, 0.9272, 0.9102, 0.2539]])




```python
b
```




    tensor([[ 0.5523,  0.8276, -0.2657, -1.9193, -0.7357],
            [-0.7729,  1.2611,  0.3241, -0.0497,  0.8940],
            [ 1.0908,  0.8121,  0.0762, -0.3183,  0.9447],
            [ 0.9139,  1.2794,  1.2535, -0.1081, -0.1498],
            [-1.2148, -0.2266, -1.1266,  0.1156,  1.9062],
            [ 0.8590, -0.6014, -0.9798,  1.5954, -1.3350],
            [ 0.4468, -0.1060,  0.6233, -0.2041, -0.2570],
            [-1.2845, -1.8804, -1.6156,  0.5930,  0.6237],
            [-1.4856, -0.1660, -0.0264, -1.1177, -1.1508],
            [ 0.8949, -1.1997,  1.7370,  1.4133, -0.7402]],
           grad_fn=<NativeBatchNormBackward0>)




```python
# Linear -> BatchNorm -> Activation (-> Dropout)
class BNModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.b1 = nn.Sequential(nn.Linear(784, 256), 
                                nn.BatchNorm1d(256),
                                nn.ReLU()
                               )
        self.b2 = nn.Sequential(nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU())
        self.b3 = nn.Sequential(nn.Linear(128, 64), nn.BatchNorm1d(64), nn.ReLU())
        self.output = nn.Linear(64, 10)

    def forward(self, X):
        out = nn.Flatten()(X)
        out = self.b1(out)
        out = self.b2(out)
        out = self.b3(out)
        out = self.output(out)
        return out
```


```python
# 모델 생성
bn_model = BNModel().to(device)
torchinfo.summary(bn_model, (1, 28, 28))
```




    ==========================================================================================
    Layer (type:depth-idx)                   Output Shape              Param #
    ==========================================================================================
    BNModel                                  [1, 10]                   --
    ├─Sequential: 1-1                        [1, 256]                  --
    │    └─Linear: 2-1                       [1, 256]                  200,960
    │    └─BatchNorm1d: 2-2                  [1, 256]                  512
    │    └─ReLU: 2-3                         [1, 256]                  --
    ├─Sequential: 1-2                        [1, 128]                  --
    │    └─Linear: 2-4                       [1, 128]                  32,896
    │    └─BatchNorm1d: 2-5                  [1, 128]                  256
    │    └─ReLU: 2-6                         [1, 128]                  --
    ├─Sequential: 1-3                        [1, 64]                   --
    │    └─Linear: 2-7                       [1, 64]                   8,256
    │    └─BatchNorm1d: 2-8                  [1, 64]                   128
    │    └─ReLU: 2-9                         [1, 64]                   --
    ├─Linear: 1-4                            [1, 10]                   650
    ==========================================================================================
    Total params: 243,658
    Trainable params: 243,658
    Non-trainable params: 0
    Total mult-adds (M): 0.24
    ==========================================================================================
    Input size (MB): 0.00
    Forward/backward pass size (MB): 0.01
    Params size (MB): 0.97
    Estimated Total Size (MB): 0.99
    ==========================================================================================




```python
# train
# loss
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(bn_model.parameters(), lr=LR)

result_bn = train.fit(train_loader, test_loader, bn_model, loss_fn, optimizer, N_EPOCH, save_best_model=False, early_stopping=False, device=device, 
                      mode='multi')
```

    Epoch[1/10] - Train loss: 0.08985 Train Accucracy: 0.97643 || Validation Loss: 0.10999 Validation Accuracy: 0.96890
    ====================================================================================================
    Epoch[2/10] - Train loss: 0.04720 Train Accucracy: 0.98740 || Validation Loss: 0.07929 Validation Accuracy: 0.97690
    ====================================================================================================
    Epoch[3/10] - Train loss: 0.02970 Train Accucracy: 0.99152 || Validation Loss: 0.06659 Validation Accuracy: 0.98020
    ====================================================================================================
    Epoch[4/10] - Train loss: 0.02131 Train Accucracy: 0.99358 || Validation Loss: 0.06555 Validation Accuracy: 0.98060
    ====================================================================================================
    Epoch[5/10] - Train loss: 0.01699 Train Accucracy: 0.99527 || Validation Loss: 0.07176 Validation Accuracy: 0.97890
    ====================================================================================================
    Epoch[6/10] - Train loss: 0.01147 Train Accucracy: 0.99698 || Validation Loss: 0.06305 Validation Accuracy: 0.98120
    ====================================================================================================
    Epoch[7/10] - Train loss: 0.01237 Train Accucracy: 0.99647 || Validation Loss: 0.06570 Validation Accuracy: 0.98230
    ====================================================================================================
    Epoch[8/10] - Train loss: 0.00880 Train Accucracy: 0.99773 || Validation Loss: 0.06468 Validation Accuracy: 0.98100
    ====================================================================================================
    Epoch[9/10] - Train loss: 0.00667 Train Accucracy: 0.99813 || Validation Loss: 0.06635 Validation Accuracy: 0.98140
    ====================================================================================================
    Epoch[10/10] - Train loss: 0.00842 Train Accucracy: 0.99750 || Validation Loss: 0.07005 Validation Accuracy: 0.98080
    ====================================================================================================
    229.6033055782318 초
    

# Learning rate decay

### Optimizer와 Learning rate scheduler의 속성, 메소드 확인
- 파이토치는 `torch.optim` 모듈에서 다양한 Learning rate 알고리즘을 제공한다.


```python
optimizer = torch.optim.Adam(bn_model.parameters(), lr=0.001)
```


```python
# 옵티마이저 객체 관련 여러 정보들을 조회 => optimizer.param_groups
print(type(optimizer.param_groups), len(optimizer.param_groups))
print(type(optimizer.param_groups[0]))
```

    <class 'list'> 1
    <class 'dict'>
    


```python
info_dict = optimizer.param_groups[0]
info_dict.keys()
```




    dict_keys(['params', 'lr', 'betas', 'eps', 'weight_decay', 'amsgrad', 'maximize', 'foreach', 'capturable', 'differentiable', 'fused'])




```python
info_dict['lr'] # 현재 learning rate
```




    0.001




```python
# 학습률의 변화흐름을 시각화 하는 함수
def plot_lr(title, lr_list):
    # title - 그래프 제목
    # lr_list - 에폭별 적용된 learning rate 리스트
    plt.figure(figsize=(10, 5))
    plt.plot(range(len(lr_list)), lr_list)
    
    plt.title(title)
    plt.xticks([x for x in range(len(lr_list)) if x % 5 == 0], rotation=45)
    plt.xlabel('Epochs')
    plt.ylabel('LR')
    plt.grid(True, axis='x', linestyle=':')
    plt.show()
```


```python
# 테스트
plot_lr('MyScheduler', list(range(1, 1000, 10)))
```


    
![png](output_55_0.png)
    


#### StepLR


```python
optimizer = torch.optim.Adam(bn_model.parameters(), lr=0.001)
# lr_scheduler: optimizer의 lr 는 초기 확습률
step_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,      # 학습률을 변화시킬 Optimizer
                                                 step_size=30,   # 몇 step/epoch 마다 LR를 변경시킬지 간격
                                                 gamma=0.5,      # 변경 비율. new_lr = lr * gamma
                                                )
# optimizer의 lr를 30 에폭/step 마다 0.5 배씩 변경
```


```python
# 현재 LR 조회
optimizer.param_groups[0]['lr'], step_scheduler.get_last_lr()
```




    (0.001, [0.001])




```python
N_EPOCH = 200
STEP_SIZE = 10
steplr_list = [] # epoch별 lr 를 저장할 변수

for epoch in range(N_EPOCH):

    # step
    for stpe in range(STEP_SIZE): # for x, y in dataloader:
        # 학습
        # 모델 예측 - loss 계산 -> 파라미터 업데이트
        optimizer.step()
        # learning rate 변경 요청.
        ## step단위로 변경
        # step_scheduler.step() 
    step_scheduler.step() # epoch 단위로 변경
    steplr_list.append(step_scheduler.get_last_lr()[0]) # 현재 epoch의 lr를 저장.
```


```python
plot_lr('Step LR', steplr_list)
```


    
![png](output_60_0.png)
    


#### CosineAnnealingLR
cosine 그래프를 그리면서 learning rate를 변경 하는 방식.  
최근에는 learning rate를 단순히 감소시키기 보다는 감소와 증가를 반복하여 진동하는 방식으로 최적점을 찾아가는 알고리즘을 많이 사용한다. 이러한 방법 중 가장 간단하면서도 많이 사용되는 방법이 CosineAnnealingLR이다.


```python
# Annealing: 가열 <-> 냉각
optimizer = torch.optim.Adam(bn_model.parameters(), lr=0.001)
ca_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,
                                                          T_max=10, # 한 cyclr 주기(단위: 에폭, step)
                                                          eta_min=0.00001, # 최소 학습률                                                          
                                                         )
# 범위: 초기 LR ~ eta_min
```


```python
ca_lr_list = []
for epoch in range(N_EPOCH):
    # 한 에폭
    for step in range(STEP_SIZE):
        # 한 스텝
        optimizer.step()
    
    ca_scheduler.step()
    ca_lr_list.append(ca_scheduler.get_last_lr()[0])
```


```python
plot_lr('CosineAnnealingLR', ca_lr_list)
```


    
![png](output_64_0.png)
    


#### CosineAnnealingWarmRestarts

cosine annealing의 스케쥴링에 cosine 주기의 에폭을 점점 늘리거나 줄일 수 있다. (보통 늘린다.)


```python
optimizer = torch.optim.Adam(bn_model.parameters(), lr=0.001)
caws_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 
                                                                     T_0=10,  # 초기 변화 주기 (cycle)
                                                                     T_mult=2, # 변화주기를 어떤 비율로 변경할지
                                                                     eta_min=1e-5, # 0.00001. 최소학습률
                                                                     verbose=True # 학습률 변경할때마다 로그 출력
                                                                     )
# 변화 범위: 초기 학습률 ~ eat_min
```

    Epoch 00000: adjusting learning rate of group 0 to 1.0000e-03.
    


```python
caws_lr_list = []

for epoch in range(N_EPOCH):
    # 한 에폭
    for step in range(STEP_SIZE):
        # 한 스텝
        optimizer.step()
    caws_scheduler.step()
    caws_lr_list.append(caws_scheduler.get_last_lr()[0])
```


```python
plot_lr('CosineAnnealingWarmRestart', caws_lr_list)
```


    
![png](output_68_0.png)
    


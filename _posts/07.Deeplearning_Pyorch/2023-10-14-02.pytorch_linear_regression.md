# LinearRegression from Scratch


# 구현할 것
- 공부시간과 성적간의 관계를 모델링한다.
    - **머신러닝 모델(모형)이란** 수집한 데이터를 기반으로 입력값(Feature)와 출력값(Target)간의 관계를 하나의 공식으로 정의한 함수이다. 그 공식을 찾는 과정을 **모델링**이라고 한다.
    - 이 예제에서는 공부한 시간으로 점수를 예측하는 모델을 정의한다.
    - 입력값과 출력값 간의 관계를 정의할 수있는 다양한 함수(공식)이 있다. 여기에서는 딥러닝과 관계가 있는 **Linear Regression** 을 사용해본다.

# 데이터 확인
- 입력데이터: 공부시간
- 출력데이터: 성적

|공부시간|점수|
|-|-|
|1|20|
|2|40|
|3|60|

우리가 수집한 공부시간과 점수 데이터를 바탕으로 둘 간의 관계를 식으로 정의 할 수 있으면 **내가 몇시간 공부하면 점수를 얼마 받을 수 있는지 예측할 수 있게 된다.**   
수집한 데이터를 기반으로 앞으로 예측할 수있는 모형을 만드는 것이 머신러닝 모델링이다.



## 학습(훈련) 데이터셋 만들기
- 모델을 학습시키기 위한 데이터셋을 구성한다.
- 입력데이터와 출력데이터을 따로 행렬로 구성한다.
- 같은 데이터 포인트의 입력, 출력 데이터를 같은 index에 정의한다.

# 모델링

## 모델 정의

- Feature와 Target간의 관계를 수식으로 정의한다.
- 여기서는 공부시간(Feature)와 점수(Target)간의 관계를 정의하는데 **선형회귀(Linear Regression) 모델** 을 가설로 세우고 모델링을 한다.
    - 많은 머신러닝 연구자들이 다양한 종류의 데이터에 관계를 예측할 수 있는 여러 알고리즘을 연구했다.
    - 선형회귀 모델은 입력데이터와 출력데이터가 선형관계(비례 또는 반비례 관계)일때 좋은 성능을 나타낸다.
  
> ### 가설
> - 아직은 이 식이 맞는지 틀린지는 알 수없기 때문에 **이 식을 가설(hypothesis) 라고 한다.**
> - 가설을 세우고 모델링을 한 뒤 검증을 해서 좋은 예측결과를 내면 그 가설을 최종 결과 모델로 결정한다. 예측결과가 좋지 않을 경우 새로운 가설로 모델링을 한다.
 
  

### 선형회귀 (Linear Regression)
- Feature들의 가중합을 이용해 Target을 추정한다.
- Feature에 곱해지는 가중치(weight)들은 각 Feature가 Target 얼마나 영향을 주는지 영향도가 된다.
    - 음수일 경우는 target값을 줄이고 양수일 경우는 target값을 늘린다.
    - 가중치가 0에 가까울 수록 target에 영향을 주지 않는 feature이고 0에서 멀수록 target에 많은 영향을 주는 target이 된다.
- 모델 학습과정에서 가장 적절한 Feature의 가중치를 찾아야 한다.
      
 
$$
\hat{y} = Wx + b
$$
<center>$\hat{y}$: 모델추정값<br>W: 가중치<br>x: Feature<br>b: bias(편향)</center>


## [경사하강법을 이용한 최적화](03_2_gradient_decending.ipynb)




```python
import torch
```


```python
# 훈련(train)데이터 정의
## 입력데이터 변수명: X, 출력데이터 변수명: y
X_train = torch.tensor([[1],[2],[3]], dtype=torch.float32)
y_train = torch.tensor([[20],[40],[60]], dtype=torch.float32)
```


```python
print(X_train.shape, y_train.shape)
# [3, 1] -> [데이터개수, 개별데이터의 shape] => 3개의 데이터. 각데이터는 1개의 값을 구성된 1차원 배열
```

    torch.Size([3, 1]) torch.Size([3, 1])
    


```python
#### 모델정의
torch.manual_seed(0)

## weight와 bias를 정의
weight = torch.rand(1, 1, requires_grad=True) # (1: 입력 값의개수, 1: 출력 값의개수)
bias = torch.rand(1, requires_grad=True)

print('초기 파라미터')
print('weight:', weight)
print('bias:', bias)

def linear_model(X):
    pred = X @ weight + bias
    return pred
```

    초기 파라미터
    weight: tensor([[0.4963]], requires_grad=True)
    bias: tensor([0.7682], requires_grad=True)
    


```python
### 예측
pred_train = linear_model(X_train)
pred_train
```




    tensor([[1.2645],
            [1.7607],
            [2.2570]], grad_fn=<AddBackward0>)




```python
# 실제 점수
y_train
```




    tensor([[20.],
            [40.],
            [60.]])




```python
### 오차함수 정의 -> 모델이 추론한 값과 정답사이의 차이를 계산하는 함수
# 평균 제곱 오차 (Mean Squared Error : MSE)
def mse_loss_fn(pred:'예측값', y:'정답'):
    return torch.mean((pred - y)**2)
```


```python
loss = mse_loss_fn(pred_train, y_train)
print('오차:', loss)
```

    오차: tensor(1715.8387, grad_fn=<MeanBackward0>)
    


```python
### weight와 bias에 대한 gradient 계산
loss.backward()
```


```python
print('weight의 grad:', weight.grad)
print('bias의 grad', bias.grad)
```

    weight의 grad: tensor([[-178.9621]])
    bias의 grad tensor([-76.4785])
    


```python
### 최적화 -> 경사하강법 ==> 파이토치의 함수를 사용
optimizer = torch.optim.SGD([weight, bias], # 최적화할 대상 => requires_grad=True
                            lr = 0.001,     # 학습률                           
                           )
### 경사하강법 연산.
print('Udate전 weight:', weight)
optimizer.step()
print('Udate후 weight:', weight)
optimizer.zero_grad()
```

    Udate전 weight: tensor([[0.4963]], requires_grad=True)
    Udate후 weight: tensor([[0.6752]], requires_grad=True)
    


```python
#### 업데이트후 로스계산
## 추론
pred = linear_model(X_train)
## 오차계산
loss2 = mse_loss_fn(pred, y_train)
print('이전', loss)
print('업데이트후', loss2)
```

    이전 tensor(1715.8387, grad_fn=<MeanBackward0>)
    업데이트후 tensor(1678.1724, grad_fn=<MeanBackward0>)
    

### 학습
1. 모델을 이용해 추정한다.
   - pred = model(input)
1. loss를 계산한다.
   - loss = loss_fn(pred, target)
1. 계산된 loss를 파라미터에 대해 미분하여 계산한 gradient 값을 각 파라미터에 저장한다.
   - loss.backward()
1. optimizer를 이용해 파라미터를 update한다.
   - optimizer.step()  
1. 파라미터의 gradient(미분값)을 0으로 초기화한다.
   - optimizer.zero_grad()
- 위의 단계를 반복한다.   


```python
import torch
```


```python
# 돌려봐야지 알아야한다 -> 많이 돌려도 효율이 나쁠수도 있다.
STEPS = 100 # 파라미터(weight, bias)를 업데이트를 할 횟수
for _ in range(STEPS):
    # 1. 추론(예측)
    pred = linear_model(X_train)
    # 2. 오차계산
    loss = mse_loss_fn(pred, y_train)
    # 3. weight, bias 대한 gradient 계산
    loss.backward()
    # 4. 최적화 -> optimizer를 이용. ==> 경사하강법(알고리즘)
    optimizer.step()
    # 5. 계산된 gradient 값 초기화
    optimizer.zero_grad()
```


```python
### weight, bias 값
print('weight:', weight)
print('bias:', bias)
```

    weight: tensor([[16.2360]], requires_grad=True)
    bias: tensor([7.0396], requires_grad=True)
    


```python
### 오차 계산
pred3 = linear_model(X_train)
loss3 = mse_loss_fn(pred3, y_train)
```


```python
# torch.sqrt: 제곱근 구하기
print(loss3, torch.sqrt(loss3))
```

    tensor(9.6839, grad_fn=<MeanBackward0>) tensor(3.1119, grad_fn=<SqrtBackward0>)
    


```python
loss**(1/2)
```




    tensor(3.1200, grad_fn=<PowBackward0>)



# 다중 입력, 다중 출력
- 다중입력: Feature가 여러개인 경우
- 다중출력: Output 결과가 여러개인 경우

다음 가상 데이터를 이용해 사과와 오렌지 수확량을 예측하는 선형회귀 모델을 정의한다.  
[참조](https://www.kaggle.com/code/aakashns/pytorch-basics-linear-regression-from-scratch)


|온도(F)|강수량(mm)|습도(%)|사과생산량(ton)|오렌지생산량|
|-|-|-|-:|-:|
|73|67|43|56|70|
|91|88|64|81|101|
|87|134|58|119|133|
|102|43|37|22|37|
|69|96|70|103|119|

```
사과수확량  = w11 * 온도 + w12 * 강수량 + w13 * 습도 + b1
오렌지수확량 = w21 * 온도 + w22 * 강수량 + w23 *습도 + b2
```

- `온도`, `강수량`, `습도` 값이 사과와, 오렌지 수확량에 어느정도 영향을 주는지 가중치를 찾는다.
    - 모델은 사과의 수확량, 오렌지의 수확량 **두개의 예측결과를 출력**해야 한다.
    - 사과에 대해 예측하기 위한 weight 3개와 오렌지에 대해 예측하기 위한 weight 3개 이렇게 두 묶음, 총 6개의 weight를 정의하고 학습을 통해 가장 적당한 값을 찾는다.
        - 이 묶음을 딥러닝에서는 **Node, Unit, Neuron** 이라고 한다.
- 목적은 우리가 수집한 train 데이터셋을 이용해 **정확한 예측을 위한 weight와 bias**를 찾는 것이다.

## Training Data
- Train data는 feature와 target를 각각 따로 2개의 행렬로 구성한다.
- Feature의 행은 관측치(개별 데이터)를 열을 Feature(특성, 변수)를 표현한다.
- Target은 모델이 예측할 대상으로 행은 개별 관측치, 열은 각 항목에 대한 정답으로 이 예제에서는 사과수확량과 오렌지 수확량 값을 가진다.


```python
import torch
```


```python
# Input (temp, rainfall, humidity) : (5, 3)
inputs = torch.tensor([[73, 67, 43], 
                   [91, 88, 64], 
                   [87, 134, 58], 
                   [102, 43, 37], 
                   [69, 96, 70]], dtype=torch.float32)
```


```python
# Targets: 생산량 - (apples, oranges) - (5, 2)
targets = torch.tensor([[56, 70], 
                    [81, 101], 
                    [119, 133], 
                    [22, 37], 
                    [103, 119]], dtype=torch.float32)
```

## Linear Regression Model (from scratch)

### weight와 bias
- weight 는 각 feature에 곱해지는 가중치로 target 값에 얼마나 영향을 주는지를 나타낸다. 직선의 방정식에서 기울기이다.
- bias는 각 feature와 weight간의 가중합에 더해주는 값으로 모든 feature가 0일 경우 target의 값을 나타낸다. 직선의 방정식에서 절편이다.
- weight와 bias는 각각 random 값을 초기값으로 가지는 matrix로 정의한다.
- weight의 shape: (2, 3)
- bias의 shape: (2, ) 


```python
import torch
torch.manual_seed(0)
```




    <torch._C.Generator at 0x24fe1eaf490>




```python
### weight와 bias를 생성
weights = torch.randn(3, 2, requires_grad=True) # 파라미터 -> 학습을 통해 찾아야 하는 변수 -> requires_grad=True
# [3, 2] => [Input Feature개수, Output 개수] => [[온도W, 강수량W, 습도W], [사과, 오렌지]]
bias = torch.randn(2, requires_grad=True) # [2] => [Output 개수=> 사과, 오렌지]
```


```python
print(weights.shape, bias.shape)
print(weights)
print(bias)
```

    torch.Size([3, 2]) torch.Size([2])
    tensor([[ 1.5410, -0.2934],
            [-2.1788,  0.5684],
            [-1.0845, -1.3986]], requires_grad=True)
    tensor([0.4033, 0.8380], requires_grad=True)
    

### Linear Regression model
모델은 weights `w`와 inputs `x`의 내적(dot product)한 값에 bias `b`를 더하는 간단한 함수이다. 

$$
\hspace{2.5cm} X \hspace{1.1cm} \cdot \hspace{1.2cm} W^T \hspace{1.2cm}  + \hspace{1cm} b \hspace{2cm}
$$

$$
\left[ \begin{array}{cc}
73 & 67 & 43 \\
91 & 88 & 64 \\
\vdots & \vdots & \vdots \\
69 & 96 & 70
\end{array} \right]
%
\cdot
%
\left[ \begin{array}{cc}
w_{11} & w_{21} \\
w_{12} & w_{22} \\
w_{13} & w_{23}
\end{array} \right]
%
+
%
\left[ \begin{array}{cc}
b_{1} & b_{2} \\
b_{1} & b_{2} \\
\vdots & \vdots \\
b_{1} & b_{2} \\
\end{array} \right]
$$


```python
print(weights.shape, bias.shape)
print(weights)
print(bias)
```


```python
def model(X):
    return X @ weights + bias
```


```python
targets
```




    tensor([[ 56.,  70.],
            [ 81., 101.],
            [119., 133.],
            [ 22.,  37.],
            [103., 119.]])




```python
# inputs.shape
pred = model(inputs)
```


```python
print(pred.shape)
pred
```

    torch.Size([5, 2])
    




    tensor([[ -79.7173,  -42.6370],
            [-120.5089,  -65.3522],
            [-220.3900,  -29.6390],
            [  23.7697,  -56.3972],
            [-178.3483,  -62.7408]], grad_fn=<AddBackward0>)



### Loss Function
모델이 예측한 값과 정답간의 차이를 비교하는 메소드. 


```python
def mse_loss_fn(preads, gargets):
    squared_error = (pred - targets)**2
    return torch.mean(squared_error)
```


```python
loss = mse_loss_fn(pred, targets) # (2, ) => 0: 사과생산량 오차, 1: 오렌지생산량 오차
loss
```




    tensor(36193.4961, grad_fn=<MeanBackward0>)



### Gradients 계산
loss에 대한 weight와 bias의 gradients (미분계수)를 계산한다. **Pytorch의 자동미분**을 이용한다. (**graident를 구하려는 tensor는 requires_grad=True로 설정한다.**)


```python
loss.backward()
```


```python
print(weights)
print(weights.grad)
```

    tensor([[ 1.5410, -0.2934],
            [-2.1788,  0.5684],
            [-1.0845, -1.3986]], requires_grad=True)
    tensor([[-15400.8271, -11915.3555],
            [-19847.4902, -13088.5010],
            [-11609.1875,  -8220.1104]])
    


```python
new_weight_0_0 = weights[0, 0] - weights.grad[0, 0] * 0.001 # Lr
new_weight_0_0
```




    tensor(16.9418, grad_fn=<SubBackward0>)




```python
print(bias)
print(bias.grad)
```

    tensor([0.4033, 0.8380], requires_grad=True)
    tensor([-191.2390, -143.3533])
    

### 모델 최적화
gradient decent 알고리즘을 이용해 loss를 줄여 모델의 추론 성능을 높인다. 이를 위해 좋은 성능을 낼 수 있도록 **경사하강법(gradient decent)** 을 이용해 weight와 bias를 update한다. 

1. 추론하기
2. loss 계산하기
3. weight와 bias에 대한 gradient계산하기
4. 계산된 gradient에 비례한 값을 학습률을 곱해 작게 만든 뒤 wegith에서 빼서 조정한다.
5. gradient를 0으로 초기화


```python
# step(파라미터 업데이트)수 지정
STEPS = 100

# 학습률 (Learning Rate)
LR = 0.0001 # 1e-4

# Optimizer 생성
optimizer = torch.optim.SGD([weights, bias], lr=LR) # 경사하강법처리. gradient값 초기화
```


```python
for i in range(STEPS):
    # 추론 
    pred = model(inputs)
    # 오차계산
    loss = mse_loss_fn(pred, targets)
    # gradient 계산
    loss.backward()
    # 최적화 -> 파라미터(weights, bias) 값들 업데이트
    optimizer.step()
    # 계산된 gradient 값 초기화
    optimizer.zero_grad()
    # 10 step당 loss 출력
    if i % 10 == 0 or i == (STEPS-1):
        print(f'Step: {i} - loss:{loss.item()}') #  tensor객체.item(): scalar나 원소가 1개인 tensor의 값을 추출(파이썬 타입 값)
```

    Step: 0 - loss:5.865939140319824
    Step: 10 - loss:4.954825401306152
    Step: 20 - loss:4.199545860290527
    Step: 30 - loss:3.57344126701355
    Step: 40 - loss:3.054426670074463
    Step: 50 - loss:2.6241672039031982
    Step: 60 - loss:2.2674918174743652
    Step: 70 - loss:1.971828818321228
    Step: 80 - loss:1.7267286777496338
    Step: 90 - loss:1.5235397815704346
    Step: 99 - loss:1.3705735206604004
    


```python
print('학습전 파라미터')
print(weights)
print(bias)
# 온도, 강수량, 습도
```

    학습전 파라미터
    tensor([[ 1.5410, -0.2934],
            [-2.1788,  0.5684],
            [-1.0845, -1.3986]], requires_grad=True)
    tensor([0.4033, 0.8380], requires_grad=True)
    


```python
print('학습후 300번 파라미터')
print(weights)
print(bias)
# 온도, 강수량, 습도
```

    학습후 300번 파라미터
    tensor([[-0.3910, -0.2828],
            [ 0.8682,  0.8374],
            [ 0.6338,  0.7962]], requires_grad=True)
    tensor([0.3987, 0.8516], requires_grad=True)
    


```python
pred_value = model(inputs)
```


```python
# 예측 정답 - 300번 학습
pred_value
```




    tensor([[ 57.2775,  70.5489],
            [ 81.7814,  99.7641],
            [119.4813, 134.6364],
            [ 21.2970,  37.4738],
            [101.1330, 117.4618]], grad_fn=<AddBackward0>)




```python
# 정답
targets
```




    tensor([[ 56.,  70.],
            [ 81., 101.],
            [119., 133.],
            [ 22.,  37.],
            [103., 119.]])



# pytorch built-in 모델을 사용해 Linear Regression 구현


```python
import torch
import torch.nn as nn # 모델들을 제공하는 모듈
```


```python
inputs = torch.tensor([[73, 67, 43], 
                   [91, 88, 64], 
                   [87, 134, 58], 
                   [102, 43, 37], 
                   [69, 96, 70]], dtype=torch.float32)
```


```python
targets = torch.tensor([[56, 70], 
                    [81, 101], 
                    [119, 133], 
                    [22, 37], 
                    [103, 119]], dtype=torch.float32)
```

## nn.Linear
Pytorch는 nn.Linear 클래스를 통해 Linear Regression 모델을 제공한다.  
nn.Linear에 입력 feature의 개수와 출력 값의 개수를 지정하면 random 값으로 초기화한 weight와 bias들을 생성해 모델을 구성한다.


```python
# Linear(input feature개수, output개수) 모델 정의
model = nn.Linear(3, 2)
```


```python
## weight와 bias 조회
print(model.weight)
print(model.bias)
```

    Parameter containing:
    tensor([[-0.2020, -0.4732, -0.1228],
            [ 0.1234, -0.3761, -0.0296]], requires_grad=True)
    Parameter containing:
    tensor([ 0.4133, -0.0594], requires_grad=True)
    

## Optimizer와 Loss 함수 정의
- torch.optim 모듈에 다양한 Optimizer 클래스가 구현되있다. 그 중에서 Adam를 사용한다.
- torch.nn 또는 torch.nn.functional 모듈에 다양한 Loss 함수가 제공된다. 이중 mse_loss() 를 사용한다.


```python
# LOSS 함수 정의
# torch.nn.functional.mse_Loss()
loss_fc = nn.MSELoss()
```


```python
# Optimizer
## ([최적화대상-모델의 파라미터], lr=학습률)
optimizer = torch.optim.SGD(model.parameters(), lr=0.0001) 
```


```python
### 학습전에 추론 -> 오차 계산
## 추론
pred = model(inputs)
pred
```




    tensor([[-51.3168, -17.5246],
            [-67.4688, -23.8240],
            [-87.6917, -41.4416],
            [-45.0803,  -4.7409],
            [-67.5479, -29.7258]], grad_fn=<AddmmBackward0>)




```python
targets
```




    tensor([[ 56.,  70.],
            [ 81., 101.],
            [119., 133.],
            [ 22.,  37.],
            [103., 119.]])




```python
## 오차 계산
loss = loss_fc(pred, targets) # (모델예측결과, 정답)
loss
```




    tensor(18740.0840, grad_fn=<MseLossBackward0>)



## Model Train
주어진 epoch 만큼 학습하는 `fit`  함수를 정의한다.


```python
def fit(num_epochs:'몇번 반복할지', model:'학습시킬모델', loss_fc:'오차함수', optim:'옵티마이저', inputs:'입력데이터', targets:'출력데이터'):
    for epoch in range(num_epochs):
        # 추론
        pred = model(inputs)
        # 오차 계산
        loss = loss_fc(pred, targets)
        # gradient 계산
        loss.backward()
        # 파라미터 update
        optim.step()
        # 계산된 gradient값 초기화
        optim.zero_grad()
        ### loss(결과) 출력
        if epoch % 10 == 0 or epoch == (num_epochs-1):
            print(f'Step: {epoch}/{num_epochs}: train loss:{loss.item():.5f}')
```


```python
fit(200, model, loss_fc, optimizer, inputs, targets)
```

    Step: 0/200: train loss:18740.08398
    Step: 10/200: train loss:393.48126
    Step: 20/200: train loss:71.74210
    Step: 30/200: train loss:26.01782
    Step: 40/200: train loss:12.91758
    Step: 50/200: train loss:8.43894
    Step: 60/200: train loss:6.44737
    Step: 70/200: train loss:5.26142
    Step: 80/200: train loss:4.40389
    Step: 90/200: train loss:3.72701
    Step: 100/200: train loss:3.17505
    Step: 110/200: train loss:2.71996
    Step: 120/200: train loss:2.34337
    Step: 130/200: train loss:2.03138
    Step: 140/200: train loss:1.77280
    Step: 150/200: train loss:1.55844
    Step: 160/200: train loss:1.38076
    Step: 170/200: train loss:1.23347
    Step: 180/200: train loss:1.11136
    Step: 190/200: train loss:1.01015
    Step: 199/200: train loss:0.93394
    


```python
# 학습후 모델 추론 결과 확인
p = model(inputs)
p
```




    tensor([[ 57.2727,  70.3651],
            [ 81.8452, 100.1246],
            [119.3441, 134.1090],
            [ 21.2584,  37.3385],
            [101.2700, 117.9887]], grad_fn=<AddmmBackward0>)




```python
l = loss_fc(p, targets)
l.item()
```




    0.9262372255325317




```python
targets
```




    tensor([[ 56.,  70.],
            [ 81., 101.],
            [119., 133.],
            [ 22.,  37.],
            [103., 119.]])



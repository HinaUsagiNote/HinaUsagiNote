---
layout: single
title: '도형 그리기'
typora-root-url: ../
categories: Pytorch.03.Gradient_Decendiong
tag: Pytorch
toc: true
---

# 최적화 (Optimize)
- 모델을 학습하는 과정이 최적화 과정이다.
- 모델의 예측값과 실제 값의 차이 즉 **오차**를 계산하는 함수를 만들고 그 오차를 가장 적게 만드는 파라미터를 찾는 작업을 한다.

## 최적화 문제
- 어떤 함수가 반환할 수 있는 가장 작은 또는 가장 큰 값을 반환하도록 하는 파라미터 값을 찾는 문제를 최적화 문제라고 한다.
- 머신러닝은 Loss 함수(오차계산함수)가 반환하는 값(오차)를 최소화 할 수 있는 파라미터를 찾는 작업을 학습 과정에서 진행한다.
$$
w_{i} = \arg \min_w f(w) 
$$

<center>
 f(): 손실함수 
 w: 파라미터
</center>    

### 손실함수(Loss Function), 비용함수(Cost Function), 목적함수(Object Function), 오차함수(Error Function)
- 모델의 예측한 값과 정답값 간의 차이(오차)를 계산하는 함수. 
- 모델을 학습시키는 것은 이 함수의 반환값(Loss)을 최소화 하는 파라미터을 찾는 과정이다.
- 문제 유형에 따라 Loss를 계산하는 방법이 다르다.
    - Classification(분류)의 경우 log loss(cross entropy)를 사용한다.
    - Regression(회귀)의 경우 MSE(Mean Squared Error)를 사용한다.

## 최적화 문제를 해결하는 방법
- Loss 함수 최적화 함수를 찾는다.
    - Loss를 최소화하는 weight들을 찾는 함수(공식)을 찾는다.
    - Feature와 sample 수가 많아 질 수록 계산량이 급증한다.
    - 최적화 함수가 없는 Loss함수도 있다.
    
- **경사하강법 (Gradient Descent)**
    - 값을 조금씩 조금씩 조정해나가면서 최소값을 찾는다.

## 경사하강법 (Gradient Descent)
- 다양한 종류의 문제에서 최적의 해법을 찾을 수 있는 **일반적인 최적화 알고리즘**. 특히 최적화 함수가 없는 모델의 경우 경사하강법을 사용한다.
- 손실함수를 최소화하는 파라미터를 찾기위해 반복해서 조정해 나간다. 
    1. 파라미터 $W$에 대해 손실함수의 현재 gradient(경사,기울기,순간변화율)를 계산한다.
          - gradient는 파라미터에 대한 손실함수의 순간변화율 즉 미분해서 구한다.
    2. gradient가 감소하는 방향으로 파라미터 $W$를 변경한다.
    3. gradient가 **0**이 될때 까지 반복 1,2 를 반복한다.
- Gradient의 부호에 따른 새로운 파라미터값 계산
    - gradient가 양수이면 loss와 weight가 비례관계란 의미이므로 loss를 더 작게 하려면 weight가 작아져야 한다.    
    - gradient가 음수이면 loss와 weight가 반비례관계란 의미이므로 loss를 더 작게 하려면 weight가 커져야 한다.

![image.png](image.png)

### 파라미터 조정 -> 경사하강법

$$
W_{new} = W-\alpha\frac{\partial}{\partial {W}}cost(W)
$$


<center>
    $W$: 파라미터<br>$\alpha$:학습률<br>
</center> 

1. 현재 파라미터에 대해 Loss를 미분 한다.  
2. 1의 값에 Learning rate를 곱한 뒤 현재 파라미터값에서 뺀다.

> - 학습률 (Learning rate)
>     - 기울기에 따라 이동할 step의 크기. 경사하강법 알고리즘에서 지정해야하는 하이퍼 파라미터이다.
>     - 학습률을 너무 작게 잡으면 최소값에 수렴하기 위해 많은 반복을 진행해야해 시간이 오래걸린다.
>     - 학습률을 너무 크게 잡으면 왔다 갔다 하다가 오히려 더 큰 값으로 발산하여 최소값에 수렴하지 못하게 된다.


```python
# 가상의 loss 함수
def loss(weight):
    return (weight-1)**2 + 2 # 리턴값: Loss(오차)
```


```python
#  위의 loss함수의 도함수
def derived_loss(weight):
    return 2*(weight-1)
```


```python
print('w=1, 오차:', loss(1))
print('w=1, 기울기:', derived_loss(1)) 
```

    w=1, 오차: 2
    w=1, 기울기: 0
    


```python
loss(5), derived_loss(5)
```




    (18, 8)




```python
#초기 weight
weight = 5 
# 학습율
lr = 0.1

new_weight = weight - lr*derived_loss(weight)
new_weight
```




    4.2



##### 반복문을 이용해 gradient가 0이 되는 지점의 weight 찾기


```python
import numpy as np
import torch
np.random.seed(0)

learning_rate = 0.4
learning_rate = 0.01
learning_rate = 10

#최적의 weight를 찾기위한 최대 반복횟수.
max_iter = 50   

#첫번째(시작) weight => random하게 잡는다.
weight =  np.random.randint(-2,3)

weight_list = [weight]  # 새로 계산된 weight들을 저장할 리스트
iter_cnt = 0 # 반복횟수를 저장할 변수

while True:
    # loss함수에 대한 미분값을 구해서 0이면 반복을 멈춘다.
    if derived_loss(weight) == 0:
        break
    if iter_cnt == max_iter: # 현재 반복이 max_iter라면 멈춘다.
        break
    # 새로운 weight값을 계산
    weight = weight - learning_rate * derived_loss(weight)
    weight_list.append(weight) 
    iter_cnt += 1
```


```python
iter_cnt
```




    50




```python
weight
```




    8663234049605954426644038200675212212900743262211018069459689002




```python
loss(weight)
```




    75051624198251984443456989853061891539043939434909537798332873934101480896578056472849915762891214746171016655874432115640378003




```python
weight_list
```




    [2,
     -18,
     362,
     -6858,
     130322,
     -2476098,
     47045882,
     -893871738,
     16983563042,
     -322687697778,
     6131066257802,
     -116490258898218,
     2213314919066162,
     -42052983462257058,
     799006685782884122,
     -15181127029874798298,
     288441413567621167682,
     -5480386857784802185938,
     104127350297911241532842,
     -1978419655660313589123978,
     37589973457545958193355602,
     -714209495693373205673756418,
     13569980418174090907801371962,
     -257829627945307727248226067258,
     4898762930960846817716295277922,
     -93076495688256089536609610280498,
     1768453418076865701195582595329482,
     -33600614943460448322716069311260138,
     638411683925748518131605316913942642,
     -12129821994589221844500501021364910178,
     230466617897195215045509519405933293402,
     -4378865740046709085864680868712732574618,
     83198449060887472631428936505541918917762,
     -1580770532156861979997149793605296459437458,
     30034640110980377619945846078500632729311722,
     -570658162108627174778971075491512021856922698,
     10842505080063916320800450434338728415281531282,
     -206007596521214410095208558252435839890349094338,
     3914144333903073791808962606796280957916632792442,
     -74368742344158402044370289529129338200416023056378,
     1413006104539009638843035501053457425807904438071202,
     -26847115986241183138017674520015691090350184323352818,
     510095203738582479622335815880298130716653502143703562,
     -9691808871033067112824380501725664483616416540730367658,
     184144368549628275143663229532787625188711914273876985522,
     -3498743002442937227729601361122964878585526371203662724898,
     66476117046415807326862425861336332693125001052869591773082,
     -1263046223881900339210386091365390321169375020004522243688538,
     23997878253756106444997335735942416102218125380085922630082242,
     -455959686821366022454949378982905905942144382221632529971562578,
     8663234049605954426644038200675212212900743262211018069459689002]



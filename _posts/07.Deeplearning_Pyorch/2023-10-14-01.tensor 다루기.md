# Tensor 생성
- 파이토치에서 데이터를 저장하는 자료구조
- ndarray와 성격, 사용법이 유사하다.

##  원하는 형태(shape) 텐서 생성
- **torch.tensor(자료구조 \[, dtype\])**
    - 지정한 dtype(Data type)에 맞는 Tensor객체를 생성해서 반환한다.
    - 
## 특정 타입의 Tensor를 직접 생성
- torch.tensor()로 생성하면서 dtype을 지정하면 아래 타입의 Tensor객체가 생성된다.
- 원하는 Type의 Tensor클래스를 이용해 직접 생성해도 된다.
- **torch.FloatTensor(자료구조)**
    - float32 타입 텐서 생성
- **torch.LongTensor(자료구조)** 
    - int64 타입 텐서생성
- 그외
    - BoolTensor(bool), CharTensor(int8), ShortTensor(int16), IntTensor(int32), DoubleTensor(float64)
    
## tensor 상태 조회
- **tensor.shape, tensor.size(\[축번호\])**
    -  tensor의 shape조회
- **tensor.dtype, tensor.type()**
    - tensor 원소들의 데이터타입 조회
    - dtype은 **data type**을 type()은 tensor **객체의 클래스 타입**을 반환한다.
- **tensor.ndim, tensor.dim()**  : tensor 차원
- **tensor.numel()**: 전체 원소 개수



```python
import torch

torch.__version__
```




    '2.1.0+cpu'




```python
a = torch.tensor([[1,2],[3,4]], dtype=torch.float32)

print("shape:", a.shape, a.size()) # 전체 shape
print("축별 크기:", a.shape[0], a.size(0)) # 축별 size
print("type:", a.type(), a.dtype)
print('차원크기:', a.dim(), a.ndim)
print('원소개수:', a.numel())
print("device:", a.device) 
```

    shape: torch.Size([2, 2]) torch.Size([2, 2])
    축별 크기: 2 2
    type: torch.FloatTensor torch.float32
    차원크기: 2 2
    원소개수: 4
    device: cpu
    


```python
print(a)
```

    tensor([[1., 2.],
            [3., 4.]])
    


```python
torch.tensor(range(10))
```




    tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])




```python
#Float, Double(32, 64bit 실수)/Int, Long(32, 64 bit 정수) type Tensor
b = torch.FloatTensor([1,3,7])  #float32
print(b.dtype)
c = torch.IntTensor([10,20,30]) # int64
print(c.dtype)
d = torch.DoubleTensor([1, 2, 3]) # torch.tensor([], dtype=torch.float64)
print(d.dtype)
e = torch.LongTensor([10, 20, 30, 40]) # torch.tensor([], dtype=torch.int64)
print(e.dtype)
```

    torch.float32
    torch.int32
    torch.float64
    torch.int64
    

## 특정 값으로 구성된 Tensor 생성
- **torch.zeros(\*size), zeros_like(텐서)**: 0으로 구성된 tensor 생성
- **torch.ones(\*size), ones_like(텐서)**: 1로 구성된 tensor생성
- **torch.full(\*size, fill_value), full_like(텐서, fill_value)**: 지정한 값으로 구성된 tensor생성
    


```python
torch.zeros(3,2,3) # 3 X 2 X 3 3차원, 0으로 구성
torch.ones(2,3) # 2 X 3 2차원, 1으로 구성
torch.full((3,2), fill_value=100) # 3 X 2 2차원. 100으로 구성

```




    tensor([[100, 100],
            [100, 100],
            [100, 100]])




```python
a = torch.tensor([[1, 2],[3,4]]) # 2 X 2
print(a.shape)
b = torch.zeros_like(a) # a와 동일한 shape의 tensor를 생성
b = torch.ones_like(a)
b = torch.full_like(a, 20)
b.shape
```

    torch.Size([2, 2])
    




    torch.Size([2, 2])



## 동일한 간격으로 떨어진 값들로 구성된 배열생성
- **torch.arange(start=0, end, step=1)** 
- **torch.linspace(start, end, steps,)** : steps - 원소개수


```python
torch.arange(10) # 0 ~ 10-1
torch.arange(0, 1, 0.1) # 0 ~ 1, 증갑: 0.1
torch.arange(10, 1, -1) # 10 ~ 1+1, 증감: -1
```




    tensor([10,  9,  8,  7,  6,  5,  4,  3,  2])




```python
torch.linspace(0, 10, 5) # 0 ~ 10, 등분한값: 5개
torch.linspace(0, 1, 11) # 0 ~ 1, 등분한값: 10개

```




    tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,
            0.9000, 1.0000])



## 빈 tensor 생성
- **torch.empty(\*size)**


```python
torch.empty(3,2) # 의미없는 값들로 채운다
```




    tensor([[0., 0.],
            [0., 0.],
            [0., 0.]])



## 난수를 이용한 생성

- **torch.rand(\*size)**: 0 ~ 1사이 실수로 구성된 배열을 생성. 각 값은 균등분포를 따른다.
- **torch.randn(\*size)**: 표준정규분포(평균:0, 표준편차:1)를 따르는 실수로 구성된 배열 생성
- **torch.randint(low=0, high, size)**: 지정한 범위의 정수로 구성된 배열 생성
- **torch.randperm(n)**: 0 ~ n 사이의 정수를 랜덤하게 섞은 값을 원소로 가지는 배열 생성 (데이터들을 섞을때)
  


```python
torch.manual_seed(1004)  # seed 설정
torch.rand(100) # 0 ~ 1사이 100개 생성
torch.randn(3,3) # 평균 - 표준편차 x 2 ~ 평균 + 표준편차 x 2
torch.randint(1, 10, (3,3))  
torch.randperm(5) # 0, 1, 2, 3, 4
```




    tensor([1, 3, 0, 4, 2])




```python
a = torch.arange(1, 10)
print(a)
# index를 섞어서 데이터를 섞는다.(shuffle)
idx = torch.randperm(9)
a[idx]
```

    tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])
    




    tensor([3, 6, 4, 8, 2, 7, 5, 1, 9])



## tensor를 상수로 변환
- tensor객체.item()
    - Scalar(상수) tensor를 python 상수로 변환


```python
# tensor.item() -> torch.Tensor객체 -> 파이썬 상수로 변환
a = torch.tensor(10) # 0차원
print(a)
print(a.item()) # 파이썬 정수로 변환
```

    tensor(10)
    10
    


```python
b = torch.tensor([20]) # 2차원
print(b)
print(b.item()) #원소가 하나인 배열 변환 가능
```

    tensor([20])
    20
    


```python
c = torch.tensor([1, 10, 100])
print(c)
# print(c.item()) #원소가 여러개일 경우 Exception발생
```

    tensor([  1,  10, 100])
    


```python
d = torch.tensor([10], device='cpu') # device='cuda' => VRAM(GPU의 RAM)에 저장
# VRAM에 저장된 값도 추출가능
print(d)
print(d.item())
```

    tensor([10])
    10
    

## ndarray 호환

- ndarray를 tensor로 생성
    - **torch.tensor(ndarray)**
    - **torch.from_numpy(ndarray)**
- tensor를 ndarray로 변환
    - **tensor.numpy()**
    - tensor가 gpu에 있을 경우 cpu로 옮긴 뒤 변환해야 한다.


```python
import numpy as np
import torch
```


```python
torch.tensor([1,2,3])
```




    tensor([1, 2, 3])




```python
# ndarray -> tensor
arr = np.arange(1,10)

torch.tensor(arr)
torch.from_numpy(arr)
```




    tensor([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)




```python
# tensor -> ndarray
t = torch.randn(3,3)
print(t)
b = t.numpy()
print(type(b))
b
```

    tensor([[-0.1761,  2.3487,  1.1318],
            [ 0.1235,  0.7647,  0.5572],
            [ 0.6796, -0.4447, -1.1951]])
    <class 'numpy.ndarray'>
    




    array([[-0.17609052,  2.3487213 ,  1.1318233 ],
           [ 0.12350701,  0.764688  ,  0.5571773 ],
           [ 0.67956996, -0.44465092, -1.1950685 ]], dtype=float32)




```python
t2 = torch.randn(2,2, device="cpu")
t2
```




    tensor([[-1.6682, -1.7317],
            [-0.5429, -0.5746]])




```python
# VRAM에 있는 Tensor객체는 ndarray로 변환이 안됨
# t2.numpy() 
## VRAM의 Tensor를 RAM(CPU)로 이동시킨뒤에 변환
t2.to("cpu").numpy()
```




    array([[-1.6682048 , -1.7316618 ],
           [-0.5429196 , -0.57457906]], dtype=float32)



## Tensor gpu/cpu 메모리로 옮기기

- pytorch는 데이터셋인 tensor를 cpu메모리와 gpu 메모리로 서로 옮길 수 있다.
    - 데이터에 대한 연산처리를 어디서 하느냐에 따라 메모리를 선택한다.
    - 장치는 문자열로 설정한다.
        - CPU 사용: "cpu"
        - nvida GPU: "cuda"
        - Apple m1: "mps"
            - pytorch 1.12 부터 지원
- 옮기기
    - tensor 생성시 `device` 파라미터를 이용해 설정
    - `tensor.to(device)`를 이용해 설정
- 현재 실행환경에서 어떤 장비를 사용할 수 있는지 확인
    - nvidia gpu 사용가능확인
        - `torch.cuda.is_available()` - nvida gpu 사용가능 여부
        - `torch.backends.mps.is_available()` - M1 사용가능 여부


```python
# torch.cuda.is_avilable()
# torch.backends.mps.is_available()
```


```python
# device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device
```




    'cpu'




```python
t = torch.tensor([1, 2, 3], dtype=torch.float32, device=device)
print(t.device)
t

t2 = t.to("cpu")
t2
```

    cpu
    




    tensor([1., 2., 3.])



# 원소 조회및 변경 - indexing/slicing

- 대부분 Numpy 와 동일
    - **slicing에서 step을 <u>음수로 지정할 수 없다.</u>**



```python
import torch
```


```python
t = torch.randint(-10, 10, (100, ))
t.shape
t
```




    tensor([  6,   2,   0,  -4,  -4,  -4,   6,  -3,  -6,   8,  -3,   6,   9, -10,
            -10,   5,   5,   2,   6,   0,  -7,   8,   1,  -6,   8,   2,   7,   4,
             -3,  -1,   2,   2,   8,   6,  -3,  -6,   2,   8,  -8,   3,   9,  -6,
             -1,   6,   2, -10,  -4,  -2,   5,   2,   3,   4,  -5,  -4,  -1,  -9,
             -1,   0,   6,   9,   1,   4,   3,  -9,   4,  -3,  -5,  -2,   3,  -8,
              0,  -7,   2,  -4,   8,  -6,  -8,   9,   4,   2,   8, -10,  -9,   1,
             -2,  -5,  -4,   3,  -3,  -7,  -6,   0,   4,   6,   2,  -1,  -3,  -2,
             -7,  -6])




```python
t[0] # indexing
# t[0].item()
t[[1, 5, -1]] # 여러개 조회 -> fancy Indexing
```




    tensor([ 2, -4, -6])




```python
print(t[:5])
print(t[10:15])
print(t[90:])
print(t[3:30:3])
# t[10:1:-2]  #에러
t[1:10:2].flip(dims=(0,)) # reverse, dims=axis
```

    tensor([ 6,  2,  0, -4, -4])
    tensor([ -3,   6,   9, -10, -10])
    tensor([-6,  0,  4,  6,  2, -1, -3, -2, -7, -6])
    tensor([-4,  6,  8,  9,  5,  6,  8,  8,  4])
    




    tensor([ 8, -3, -4, -4,  2])




```python
# boolean index
t[t > 0] # 0보다 큰 값들만 조회
```




    tensor([6, 2, 6, 8, 6, 9, 5, 5, 2, 6, 8, 1, 8, 2, 7, 4, 2, 2, 8, 6, 2, 8, 3, 9,
            6, 2, 5, 2, 3, 4, 6, 9, 1, 4, 3, 4, 3, 2, 8, 9, 4, 2, 8, 1, 3, 4, 6, 2])




```python
t
```




    tensor([  6,   2,   0,  -4,  -4,  -4,   6,  -3,  -6,   8,  -3,   6,   9, -10,
            -10,   5,   5,   2,   6,   0,  -7,   8,   1,  -6,   8,   2,   7,   4,
             -3,  -1,   2,   2,   8,   6,  -3,  -6,   2,   8,  -8,   3,   9,  -6,
             -1,   6,   2, -10,  -4,  -2,   5,   2,   3,   4,  -5,  -4,  -1,  -9,
             -1,   0,   6,   9,   1,   4,   3,  -9,   4,  -3,  -5,  -2,   3,  -8,
              0,  -7,   2,  -4,   8,  -6,  -8,   9,   4,   2,   8, -10,  -9,   1,
             -2,  -5,  -4,   3,  -3,  -7,  -6,   0,   4,   6,   2,  -1,  -3,  -2,
             -7,  -6])




```python
# 변경
t[0] = 100
t
```




    tensor([100,   2,   0,  -4,  -4,  -4,   6,  -3,  -6,   8,  -3,   6,   9, -10,
            -10,   5,   5,   2,   6,   0,  -7,   8,   1,  -6,   8,   2,   7,   4,
             -3,  -1,   2,   2,   8,   6,  -3,  -6,   2,   8,  -8,   3,   9,  -6,
             -1,   6,   2, -10,  -4,  -2,   5,   2,   3,   4,  -5,  -4,  -1,  -9,
             -1,   0,   6,   9,   1,   4,   3,  -9,   4,  -3,  -5,  -2,   3,  -8,
              0,  -7,   2,  -4,   8,  -6,  -8,   9,   4,   2,   8, -10,  -9,   1,
             -2,  -5,  -4,   3,  -3,  -7,  -6,   0,   4,   6,   2,  -1,  -3,  -2,
             -7,  -6])




```python
t = torch.arange(1, 10).reshape(3,3) # 2차원: index - 2방향.
t
```




    tensor([[1, 2, 3],
            [4, 5, 6],
            [7, 8, 9]])




```python
# t[ 0축, 1축 ]
print(t[0])
print(t[0, 0])
print(t[1, 1])
```

    tensor([1, 2, 3])
    tensor(1)
    tensor(5)
    


```python
t[[1,0], [2,2]]  #(1,2), (0,2)
# t[[ 첫번째값, 두번째값 ]-0축,[ 첫번째값, 두번째값 ]-1축]
```




    tensor([6, 3])



# Reshape

## shape 변경
- tensor객체.reshape(\*shape) / view(\*shape) 이용
    - 변환 후 값을 변경하면 원본 배열의 값도 같이 바뀐다.
 > tensor.clone(): tensor를 복제한다.


```python
import torch
```


```python
a = torch.rand(12) # (12, )
a2 = a.reshape(3,4) # 3 X 4
a3 = a.reshape((3,2,2)) #  3 X 2 X 2
a4 = a.reshape((3,2,-1))  # 3 X 2 X 2 한 개 axis는 -1로 설정가능하고 그럼 계산해서 알아서 설정해 준다.
print(a.shape, a2.size(), a3.shape, a4.shape)
```

    torch.Size([12]) torch.Size([3, 4]) torch.Size([3, 2, 2]) torch.Size([3, 2, 2])
    


```python
a
```




    tensor([0.9451, 0.1354, 0.8463, 0.9047, 0.8599, 0.9885, 0.2477, 0.0244, 0.8585,
            0.5705, 0.4402, 0.9828])




```python
a2
```




    tensor([[0.9451, 0.1354, 0.8463, 0.9047],
            [0.8599, 0.9885, 0.2477, 0.0244],
            [0.8585, 0.5705, 0.4402, 0.9828]])




```python
a5 = a.view(3,4)
a6 = a.view((3,2,2))
a7 = a.view((3,2,-1))  #한개 axis는 -1로 설정가능
print(a.shape, a5.size(), a6.shape, a7.shape)
```

    torch.Size([12]) torch.Size([3, 4]) torch.Size([3, 2, 2]) torch.Size([3, 2, 2])
    


```python
a5[0, 0] = 12.1
a2[0, 1] = 15.1

print(a)
```

    tensor([12.1000, 15.1000,  0.8463,  0.9047,  0.8599,  0.9885,  0.2477,  0.0244,
             0.8585,  0.5705,  0.4402,  0.9828])
    


```python
a5
```




    tensor([[12.1000, 15.1000,  0.8463,  0.9047],
            [ 0.8599,  0.9885,  0.2477,  0.0244],
            [ 0.8585,  0.5705,  0.4402,  0.9828]])




```python
a2
```




    tensor([[12.1000, 15.1000,  0.8463,  0.9047],
            [ 0.8599,  0.9885,  0.2477,  0.0244],
            [ 0.8585,  0.5705,  0.4402,  0.9828]])




```python
# tensor복사: clone() 메소드
r = a.clone().reshape(3,4)
r[0,0] = 100.1
print(a)
```

    tensor([12.1000, 15.1000,  0.8463,  0.9047,  0.8599,  0.9885,  0.2477,  0.0244,
             0.8585,  0.5705,  0.4402,  0.9828])
    


```python
r
```




    tensor([[1.0010e+02, 1.5100e+01, 8.4631e-01, 9.0473e-01],
            [8.5985e-01, 9.8847e-01, 2.4774e-01, 2.4445e-02],
            [8.5851e-01, 5.7048e-01, 4.4019e-01, 9.8278e-01]])



## dummy 축 늘리기

- None을 이용 (numpy의 newaxis 대신 None을 사용한다.)
- unsqueeze(dim=축번호)


```python
import torch
```


```python
a = torch.tensor([[10,20],[10,20]])
print(a.shape)

a1, a2 = a[None, :], a.unsqueeze(dim=0)
print(a1.shape, a2.shape)

a3, a4 = a[:, :, None], a.unsqueeze(dim=-1) 
print(a3.shape, a4.shape)

a5, a6 = a3[:,None,:,:], a3.unsqueeze(dim=1)
print(a5.shape, a6.shape)
```

    torch.Size([2, 2])
    torch.Size([1, 2, 2]) torch.Size([1, 2, 2])
    torch.Size([2, 2, 1]) torch.Size([2, 2, 1])
    torch.Size([2, 1, 2, 1]) torch.Size([2, 1, 2, 1])
    

## dummy 축 제거
- squeeze(\[dim=축번호\]) 이용


```python
t = torch.rand(3, 1, 3, 1)
print(t.shape)

r1 = t.squeeze()  # 축을 명시하지않으면 모두 제거
print(r1.shape)

r2 = t.squeeze(dim=1) # 특정 axis 제거
print(r2.shape)

r3 = t.squeeze(dim=[1,3]) # 여러 axis의 dummy 축 제거
print(r3.shape)
```

    torch.Size([3, 1, 3, 1])
    torch.Size([3, 3])
    torch.Size([3, 3, 1])
    torch.Size([3, 3])
    

## tensor 합치기
torch.cat([tensorA, tensorB, ...], dim=0)


```python
a = torch.arange(10).reshape(2,5) # 0 ~ 9
b = torch.arange(10,20).reshape(2,5) # 10 ~ 19
c = torch.arange(20,30).reshape(2,5) # 20 ~ 29
d = torch.arange(10,19).reshape(3,3) # 10 ~ 18
print(a)
print(b)
print(c)
print(d)
```

    tensor([[0, 1, 2, 3, 4],
            [5, 6, 7, 8, 9]])
    tensor([[10, 11, 12, 13, 14],
            [15, 16, 17, 18, 19]])
    tensor([[20, 21, 22, 23, 24],
            [25, 26, 27, 28, 29]])
    tensor([[10, 11, 12],
            [13, 14, 15],
            [16, 17, 18]])
    


```python
torch.cat([a, b], dim=0) #.shape
```




    tensor([[ 0,  1,  2,  3,  4],
            [ 5,  6,  7,  8,  9],
            [10, 11, 12, 13, 14],
            [15, 16, 17, 18, 19]])




```python
torch.cat([a, b, c], dim=0) #.shape
```




    tensor([[ 0,  1,  2,  3,  4],
            [ 5,  6,  7,  8,  9],
            [10, 11, 12, 13, 14],
            [15, 16, 17, 18, 19],
            [20, 21, 22, 23, 24],
            [25, 26, 27, 28, 29]])




```python
torch.cat([a, b], axis=1) # dim 대신 axis사용가능
```




    tensor([[ 0,  1,  2,  3,  4, 10, 11, 12, 13, 14],
            [ 5,  6,  7,  8,  9, 15, 16, 17, 18, 19]])




```python
torch.cat([a, b, c], axis=1)
```




    tensor([[ 0,  1,  2,  3,  4, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24],
            [ 5,  6,  7,  8,  9, 15, 16, 17, 18, 19, 25, 26, 27, 28, 29]])




```python
torch.cat([a, b], axis=-1) # 음수: 뒤에서부터 . -1: 마지막축 기준
```




    tensor([[ 0,  1,  2,  3,  4, 10, 11, 12, 13, 14],
            [ 5,  6,  7,  8,  9, 15, 16, 17, 18, 19]])




```python
# 합치려는 기준축 이외의 축 size는 같이야 한다.
# torch.cat([a, d])  #Error - (2, 5) 와 (3, 3) 기준: 0축 (기본)
```

## 값의 위치(index) 변경
- tensor 원소의 축별 index의 위치를 바꾼다.
- `tensor.transpose(axis1, axis2)` 
    - 두 축의 자리만 변경 할 수 있다.
- `tensor.permute(axis1, axis2, axis3, ..)`
    - 두 개 이상의 축 자리를 변경한다.


```python
a = torch.arange(12).reshape(4, 3)
b = a.transpose(1, 0) # index의 1축을 0축으로, 0축을 1축으로 바꾼다.
print(a.shape, b.shape)
# index
# [0, 0] -> [0, 0]
# [0, 1] -> [1, 0]
# [0, 2] -> [2, 0]
```

    torch.Size([4, 3]) torch.Size([3, 4])
    


```python
a
```




    tensor([[ 0,  1,  2],
            [ 3,  4,  5],
            [ 6,  7,  8],
            [ 9, 10, 11]])




```python
b
```




    tensor([[ 0,  3,  6,  9],
            [ 1,  4,  7, 10],
            [ 2,  5,  8, 11]])




```python
X = torch.arange(24).reshape(2, 3, 4)
print(X.shape)

y = X.transpose(1, 2)
print(y.shape)

z = X.permute(2, 0, 1)
print(z.shape)
```

    torch.Size([2, 3, 4])
    torch.Size([2, 4, 3])
    torch.Size([4, 2, 3])
    


```python
X
```




    tensor([[[ 0,  1,  2,  3],
             [ 4,  5,  6,  7],
             [ 8,  9, 10, 11]],
    
            [[12, 13, 14, 15],
             [16, 17, 18, 19],
             [20, 21, 22, 23]]])




```python
y
```




    tensor([[[ 0,  4,  8],
             [ 1,  5,  9],
             [ 2,  6, 10],
             [ 3,  7, 11]],
    
            [[12, 16, 20],
             [13, 17, 21],
             [14, 18, 22],
             [15, 19, 23]]])




```python
X # 22: 1, 2, 2
```




    tensor([[[ 0,  1,  2,  3],
             [ 4,  5,  6,  7],
             [ 8,  9, 10, 11]],
    
            [[12, 13, 14, 15],
             [16, 17, 18, 19],
             [20, 21, 22, 23]]])




```python
z # 2, 0, 1 => 22: 2, 1, 2
```




    tensor([[[ 0,  4,  8],
             [12, 16, 20]],
    
            [[ 1,  5,  9],
             [13, 17, 21]],
    
            [[ 2,  6, 10],
             [14, 18, 22]],
    
            [[ 3,  7, 11],
             [15, 19, 23]]])



# tensor 연산 및 주요 함수

## element-wise 연산
- tensor와 상수 연산시, tensor와 tensor간 연산시 원소별로 처리한다.
- 행렬곱 연산을 제외하고 tensor간 연산시 피연산지 tensor간에 shape이 같아야 한다.
    - shape이 다를 경우 조건이 맞으면 broadcasting을 한 뒤에 연산한다. (size가 다른 축의 경우 한개의 피연산자 size가 1일 경우 복사하여 shape을 맞춘다.)
    


```python
import torch
```


```python
# element-wise연산 : shape이 같이야한다
# 행렬곱 2 X 5 @ 5 X 6 = 2 X 6
```


```python
import torch

a = torch.arange(10).reshape(2,5)
b = torch.arange(10,20).reshape(2,5)
c = torch.arange(50, 55)

print(a)
print(b)
print(c)
```

    tensor([[0, 1, 2, 3, 4],
            [5, 6, 7, 8, 9]])
    tensor([[10, 11, 12, 13, 14],
            [15, 16, 17, 18, 19]])
    tensor([50, 51, 52, 53, 54])
    


```python
print(a + 100)
print(a - 100)
print(a < 5)
```

    tensor([[100, 101, 102, 103, 104],
            [105, 106, 107, 108, 109]])
    tensor([[-100,  -99,  -98,  -97,  -96],
            [ -95,  -94,  -93,  -92,  -91]])
    tensor([[ True,  True,  True,  True,  True],
            [False, False, False, False, False]])
    


```python
print(a + b)
print(a == b)
```

    tensor([[10, 12, 14, 16, 18],
            [20, 22, 24, 26, 28]])
    tensor([[False, False, False, False, False],
            [False, False, False, False, False]])
    


```python
# broadcasting
# (2, 5) + (5, ) : [50, 51, 52, 53, 54]
## (2, 5) + (1, 5) : [[50, 51, 52, 53, 54]]
### (2, 5) + (2, 5) : [[50, 51, 52, 53, 54]], [[50, 51, 52, 53, 54]]
print(a + c)
```

    tensor([[50, 52, 54, 56, 58],
            [55, 57, 59, 61, 63]])
    

## 주요 연산함수


```python
import torch
```


```python
torch.e
```




    2.718281828459045




```python
x = torch.arange(-4, 5).reshape(3,3)
print(x)
```

    tensor([[-4, -3, -2],
            [-1,  0,  1],
            [ 2,  3,  4]])
    


```python
print(torch.abs(x)) # 절대값
```

    tensor([[4, 3, 2],
            [1, 0, 1],
            [2, 3, 4]])
    


```python
print(torch.sqrt(torch.abs(x))) # 제곱근
```

    tensor([[2.0000, 1.7321, 1.4142],
            [1.0000, 0.0000, 1.0000],
            [1.4142, 1.7321, 2.0000]])
    


```python
print(torch.exp(x))  # torch.e**x
```

    tensor([[1.8316e-02, 4.9787e-02, 1.3534e-01],
            [3.6788e-01, 1.0000e+00, 2.7183e+00],
            [7.3891e+00, 2.0086e+01, 5.4598e+01]])
    


```python
print(torch.log(torch.abs(x))) # 밑이 e 인 로그 (자연로그)
```

    tensor([[1.3863, 1.0986, 0.6931],
            [0.0000,   -inf, 0.0000],
            [0.6931, 1.0986, 1.3863]])
    


```python
print(torch.log(torch.exp(torch.tensor(1)))) # torch.log() 밑이 e인 로그계산
```

    tensor(1.)
    


```python
print(torch.log10(torch.tensor(10)))         # torch.log10() 밑이 10인 로그계산
```

    tensor(1.)
    


```python
print(torch.log2(torch.tensor(2)))           # torch.log2() 밑이 2인 로그계산
```

    tensor(1.)
    


```python
y = x + torch.randn((3,3))
```


```python
print(y)
```

    tensor([[-3.2212, -4.0375, -1.9533],
            [-1.7530,  0.7289,  1.4327],
            [ 1.8735,  3.5541,  2.1952]])
    


```python
print(torch.ceil(y)) # 올림
```

    tensor([[-3., -4., -1.],
            [-1.,  1.,  2.],
            [ 2.,  4.,  3.]])
    


```python
print(torch.floor(y)) # 내림
```

    tensor([[-4., -5., -2.],
            [-2.,  0.,  1.],
            [ 1.,  3.,  2.]])
    


```python
print(torch.round(y, decimals=2)) # 소수점 둘째자리이하에서 반올림
```

    tensor([[-3.2200, -4.0400, -1.9500],
            [-1.7500,  0.7300,  1.4300],
            [ 1.8700,  3.5500,  2.2000]])
    


```python
print(torch.round(y)) # 반올림 -> 정수
```

    tensor([[-3., -4., -2.],
            [-2.,  1.,  1.],
            [ 2.,  4.,  2.]])
    

### 행렬곱
- `@` 연산자 또는 `torch.matmul(tensor1, tensor2)` 함수 이용


```python
import torch
```


```python
x = torch.FloatTensor([[1, 2],
                       [3, 4],
                       [5, 6]
                      ])

y = torch.FloatTensor([[1, 2],
                       [1, 2],
                      ])
x.size(), y.shape
```




    (torch.Size([3, 2]), torch.Size([2, 2]))




```python
z1 = x @ y
z2 = torch.matmul(x, y)
print(z1.shape, z2.shape)
print(z1) 
print(z2)
```

    torch.Size([3, 2]) torch.Size([3, 2])
    tensor([[ 3.,  6.],
            [ 7., 14.],
            [11., 22.]])
    tensor([[ 3.,  6.],
            [ 7., 14.],
            [11., 22.]])
    


```python
# Batch 행렬곱(Batch matrix muliplication) - bmm()
# x, y가 가지는 3개의 2차원 배열 간에 행렬곱을 처리한다.
# 400 X 200 사진 행렬곱 200 X 500 사진 각각 1000개 => 한번에 계산하게 해줄때 사용
import torch
x = torch.FloatTensor(3,4,2)
y = torch.FloatTensor(3,2,5)
z = torch.bmm(x, y)
z.shape
z
```




    tensor([[[1.3805e+33, 3.1866e-26, 0.0000e+00, 0.0000e+00, 0.0000e+00],
             [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
             [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
             [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],
    
            [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
             [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
             [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
             [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],
    
            [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
             [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
             [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
             [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]])



## torch.nan, torch.inf
- nan: Not a Number, 주로 결측치를 표현한다.
- inf: infinit 무한. 
    - torch.inf: 양의 무한
    - -torch.inf: 음의 무한
- torch.isnan(tensor)
    - 원소별 결측치 확인
- torch.isinf(tensor)    
    - 원소별 inf 확인


```python
import torch
```


```python
print(torch.inf > 10000000000, torch.inf < 10)
print(-torch.inf < 100000000000, -torch.inf > 10)
```

    True False
    True False
    


```python
print(torch.log(torch.tensor(-1))) # nan (계산이 안되는-없는값)
print(torch.isnan(torch.tensor([1,2,torch.nan,3,4])).sum())  # nan 여부 확인, sum(): True 갯수
print(torch.isinf(torch.tensor([1,2,3,4,torch.inf])))  # inf 여부 확인
```

    tensor(nan)
    tensor(1)
    tensor([False, False, False, False,  True])
    

## 기술통계함수


```python
X=torch.randn(3,4)
print(X)
```

    tensor([[-0.5002,  0.8213, -0.3107,  0.5167],
            [ 1.6674,  1.4241, -0.4322, -1.9923],
            [ 1.1839, -1.4941, -0.4244, -1.3173]])
    


```python
print(torch.sum(X), X.sum()) # 전체합계
print(torch.sum(X, dim=0)) # dim/axis 지정: 지정한 axis의 index가 다른 값끼리 계산
print(torch.sum(X, dim=1)) 
print(torch.sum(X, dim=1, keepdims=True)) # keepdims-True: 차원유지
```

    tensor(-0.8577) tensor(-0.8577)
    tensor([ 2.3510,  0.7513, -1.1672, -2.7929])
    tensor([ 0.5270,  0.6670, -2.0518])
    tensor([[ 0.5270],
            [ 0.6670],
            [-2.0518]])
    


```python
print(torch.mean(X))
print(torch.mean(X, dim=1), X.mean(dim=1))
print(torch.mean(X, axis=1, keepdims=True), X.mean(dim=1, keepdims=True))
```

    tensor(-0.0715)
    tensor([ 0.1318,  0.1668, -0.5129]) tensor([ 0.1318,  0.1668, -0.5129])
    tensor([[ 0.1318],
            [ 0.1668],
            [-0.5129]]) tensor([[ 0.1318],
            [ 0.1668],
            [-0.5129]])
    


```python
print(torch.std(X)) # standard deviation 표준 편차
print(torch.var(X)) # variance
print(torch.var(X, dim=0))
print(torch.var(X, dim=0, keepdims=True))
```

    tensor(1.1962)
    tensor(1.4309)
    tensor([1.2948, 2.3733, 0.0046, 1.6857])
    tensor([[1.2948, 2.3733, 0.0046, 1.6857]])
    


```python
# 메소드
print(X.sum(dim=1, keepdims=True))
print(X.mean(dim=1, keepdims=True))
print(X.std())
```

    tensor([[ 0.5270],
            [ 0.6670],
            [-2.0518]])
    tensor([[ 0.1318],
            [ 0.1668],
            [-0.5129]])
    tensor(1.1962)
    


```python
X
```




    tensor([[-0.5002,  0.8213, -0.3107,  0.5167],
            [ 1.6674,  1.4241, -0.4322, -1.9923],
            [ 1.1839, -1.4941, -0.4244, -1.3173]])




```python
print(torch.max(X))
```

    tensor(1.6674)
    


```python
print(torch.max(X, dim=1))
```

    torch.return_types.max(
    values=tensor([0.8213, 1.6674, 1.1839]),
    indices=tensor([1, 0, 0]))
    


```python
print(torch.max(X, dim=0))  # return_types.max 타입객체로 반환. max값과 max값의 index를 묶어서 반환
```

    torch.return_types.max(
    values=tensor([ 1.6674,  1.4241, -0.3107,  0.5167]),
    indices=tensor([1, 1, 0, 0]))
    


```python
print(torch.max(X, dim=1))
```

    torch.return_types.max(
    values=tensor([0.8213, 1.6674, 1.1839]),
    indices=tensor([1, 0, 0]))
    


```python
print(torch.max(X, dim=1).values, torch.max(X, dim=1).indices, sep=" || ")
```

    tensor([0.8213, 1.6674, 1.1839]) || tensor([1, 0, 0])
    


```python
print(torch.max(X, dim=0, keepdims=True))  #keepdims=True : 차원(rank)를 유지
```

    torch.return_types.max(
    values=tensor([[ 1.6674,  1.4241, -0.3107,  0.5167]]),
    indices=tensor([[1, 1, 0, 0]]))
    


```python
print(torch.max(X, dim=1, keepdims=True))
```

    torch.return_types.max(
    values=tensor([[0.8213],
            [1.6674],
            [1.1839]]),
    indices=tensor([[1],
            [0],
            [0]]))
    


```python
print(torch.min(X))
```

    tensor(-1.9923)
    


```python
print(torch.min(X, dim=0))
```

    torch.return_types.min(
    values=tensor([-0.5002, -1.4941, -0.4322, -1.9923]),
    indices=tensor([0, 2, 1, 1]))
    


```python
print(torch.min(X, dim=1))
```

    torch.return_types.min(
    values=tensor([-0.5002, -1.9923, -1.4941]),
    indices=tensor([0, 3, 1]))
    


```python
X
```




    tensor([[-0.5002,  0.8213, -0.3107,  0.5167],
            [ 1.6674,  1.4241, -0.4322, -1.9923],
            [ 1.1839, -1.4941, -0.4244, -1.3173]])




```python
print(torch.argmax(X))
print(torch.argmax(X, dim=0)) # 각 열에서 가장 큰 애가 존재하는 인덱스
print(torch.argmax(X, dim=1)) # 각 행에서 가장 큰 애가 존재하는 인덱스
```

    tensor(4)
    tensor([1, 1, 0, 0])
    tensor([1, 0, 0])
    

# autograd(자동미분)
- 자동 미분을 이용해 gradient를 계산하는 pytorch system.
- 딥러닝 모델에서 weight와 bias tensor들(Parameter)은 backpropagation(역전파)를 이용해 gradient를 구해서 loss가 줄어드는 방향으로 update를 하게된다.
- pytorch는 이런 미분 수행을 자동으로 처리해 준다.
    - gradient(기울기)를 구한다는 것은 미분을 한다는 것을 말한다.
- tensor가 미분 가능하려면 `requires_grad=True` 로 설정되 있어야 한다. (default: False)
    


```python
import torch
```


```python
# 알고리즘 최적화 기법 => 미분(경사하강법)
```


```python
# x = torch.tensor([1.]), requires_grad=True)
x = torch.tensor([1.])
x.requires_grad = True
print(x)
print(x.requires_grad)
```

    tensor([1.], requires_grad=True)
    True
    


```python
y = x ** 2
print(y)
# 계산 결과를 담은 tensor인 y는 계산 결과와 grad_fn에 어떤 계산을 했는지 정보를 담고 있다. (PowBackward0)
# 이는 y 계산에 사용된 x가 requires_grad=True이기 때문이다.
```

    tensor([1.], grad_fn=<PowBackward0>)
    


```python
# 미분 - tensor.backward() 호출
y.backward()   # dy/dx  gradient 계산 후 결과를 x의 grad attribute에 저장한다.
```


```python
x.grad
# 도함수: y` = 2x 이고 x가 1이었으므로 grad는 2
```




    tensor([2.])




```python
# requires_grad가 True로 설정되 있는 않은 경우 --> Exception
x = torch.tensor([1.])
y = x ** 2
# y.backward()
```


```python
x = torch.tensor([1.], requires_grad=True)
y = x ** 2   # 미분: 2x
z = y * 10   # 미분: 10  ===> 2x * 10

print(y, y.requires_grad)
print(z)
z.backward()
print(x.grad)
```

    tensor([1.], grad_fn=<PowBackward0>) True
    tensor([10.], grad_fn=<MulBackward0>)
    tensor([20.])
    


```python
## 편미분
x=torch.tensor([1.],requires_grad=True)
y=torch.tensor([1.],requires_grad=True)
z= 2*x**2 + y**2 # 2X^2 + y^2
print(z)
z.backward() 
print(x.grad)  #dz/dx
print(y.grad)  #dz/dy
```

    tensor([3.], grad_fn=<AddBackward0>)
    tensor([4.])
    tensor([2.])
    


```python
x=torch.tensor([1., 2., 3.] ,requires_grad=True)
y=torch.sum(x**2) # [x1**2 + x2**2 + x3**2]  
y.backward() #[dy/dx1, dy/dx2, dy/dx3] = [2x1, 2x2, 2x3]

print(y)
print(x.grad) # 스칼라를 벡터로 미분
```

    tensor(14., grad_fn=<SumBackward0>)
    tensor([2., 4., 6.])
    

## torch.no_grad() 
- no_grad() 구문에서 연산을 할 경우 requires_grad=True로 설정되었다 하더라도 gradient를 update하지 않는다.
- 딥러닝 모델 학습이 끝나고 평가할 때는 gradient를 계산할 필요가 없기 때문에 no_grad 구문을 사용한다.



```python
x = torch.tensor(1.0, requires_grad = True)
print(x.requires_grad)

y = x**2

print(y.requires_grad)  # 연산이 결과 requires_grad도 True -> 그래야 미분이 가능하므로.
print(x.requires_grad)
print(y)

y.backward()
print(x.grad)

```

    True
    True
    True
    tensor(1., grad_fn=<PowBackward0>)
    tensor(2.)
    


```python
x = torch.tensor(1.0, requires_grad = True)
print(x.requires_grad)

with torch.no_grad():
    print(x.requires_grad)
    y = x**2 # torch.nograd()로 인해서 도함수를 만들지 않는다.
    print(y.requires_grad) # 연산이 적용될 때 requires_grad가 False가 된다.
    print(y)
    
print(x.requires_grad)
# y.backward() # y는 도함수를 만들지 않았기 때문데 Exception발생

```

    True
    True
    False
    tensor(1.)
    True
    

## gradient 값 초기화


```python
x = torch.tensor(1., requires_grad=True)
y = x**2
y.backward()
print("x의 gradient값:", x.grad )
```

    x의 gradient값: tensor(2.)
    


```python
x
```




    tensor(1., requires_grad=True)




```python
z = x **2
z.backward()
print("x의 gradient값:", x.grad )
# 기존 x.gradient 값과 새로운 x.gradient 값과 합친다.
```

    x의 gradient값: tensor(4.)
    


```python
x = torch.tensor(1., requires_grad=True)
y = x**2
y.backward()
x.grad
```




    tensor(2.)




```python
# gradient초기화
x.grad = torch.tensor(0.)   # grad 속성값을 0으로 변경.
z = x**2
z.backward()
x.grad
```




    tensor(2.)



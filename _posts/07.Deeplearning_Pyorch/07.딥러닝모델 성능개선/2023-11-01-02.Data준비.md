---
layout: single
title: 'Data 준비'
typora-root-url: ../
categories: Deeplearning_Pytorch.07.딥러닝모델 성능개선
tag: Pytorch
toc: true
---

# import


```python
import torch
import torch.nn as nn

import torchinfo

from module import train, data

import os
import matplotlib.pyplot as plt

device = 'cuda' if torch.cuda.is_available() else 'cpu'
device
```




    'cpu'



## 하이퍼파라미터, 변수 정의


```python
LR = 0.001
N_EPOCH = 10
BATCH_SIZE = 200

# DATASET, MODEL 저장할 ROOT 디렉토리 경로
DATASET_ROOT_PATH = 'datasets'
MODEL_SAVE_ROOT_PATH = 'models'
```

## Data 준비

### mnist 데이터 로딩


```python
train_loader = data.load_mnist_dataset(DATASET_ROOT_PATH, BATCH_SIZE, True)
test_loader = data.load_mnist_dataset(DATASET_ROOT_PATH, BATCH_SIZE, False)
```

## 모델의 크기 변경에 따른 성능변화


```python
class SmallSizeModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.output = nn.Linear(784, 10) # in: freature - (1*28*28), out: class 개수(10)

    def forward(self, X):
        out = nn.Flatten()(X)
        out = self.output(out)
        return out
```


```python
small_model = SmallSizeModel()
torchinfo.summary(small_model, (BATCH_SIZE, 1, 28, 28))
```




    ==========================================================================================
    Layer (type:depth-idx)                   Output Shape              Param #
    ==========================================================================================
    SmallSizeModel                           [200, 10]                 --
    ├─Linear: 1-1                            [200, 10]                 7,850
    ==========================================================================================
    Total params: 7,850
    Trainable params: 7,850
    Non-trainable params: 0
    Total mult-adds (M): 1.57
    ==========================================================================================
    Input size (MB): 0.63
    Forward/backward pass size (MB): 0.02
    Params size (MB): 0.03
    Estimated Total Size (MB): 0.67
    ==========================================================================================




```python
# loss
loss_fn = nn.CrossEntropyLoss()
# optimizer
optimizer = torch.optim.Adam(small_model.parameters(), lr=LR)
```


```python
# Train -> module.fit()
# train.fit? # train.fit에 대한 파라미터 정보 및 확인

train_loss_list, train_acc_list, valid_loss_list, valid_acc_list = train.fit(train_loader, test_loader, small_model, loss_fn, optimizer, N_EPOCH, 
                                                                        save_best_model = True,
                                                                        save_model_path = os.path.join(MODEL_SAVE_ROOT_PATH, 'small_model.pth'),
                                                                        device = device,
                                                                        mode = 'multi'
                                                                       )
```

    Epoch[1/10] - Train loss: 0.25697 Train Accucracy: 0.92845 || Validation Loss: 0.26678 Validation Accuracy: 0.92490
    ====================================================================================================
    저장: 1 - 이전 : inf, 현재: 0.2667800550162792
    Epoch[2/10] - Train loss: 0.25571 Train Accucracy: 0.92933 || Validation Loss: 0.26564 Validation Accuracy: 0.92600
    ====================================================================================================
    저장: 2 - 이전 : 0.2667800550162792, 현재: 0.2656371323391795
    Epoch[3/10] - Train loss: 0.25386 Train Accucracy: 0.92945 || Validation Loss: 0.26377 Validation Accuracy: 0.92720
    ====================================================================================================
    저장: 3 - 이전 : 0.2656371323391795, 현재: 0.26376935966312887
    Epoch[4/10] - Train loss: 0.25162 Train Accucracy: 0.93032 || Validation Loss: 0.26275 Validation Accuracy: 0.92710
    ====================================================================================================
    저장: 4 - 이전 : 0.26376935966312887, 현재: 0.262746288664639
    Epoch[5/10] - Train loss: 0.25132 Train Accucracy: 0.93035 || Validation Loss: 0.26568 Validation Accuracy: 0.92650
    ====================================================================================================
    Epoch[6/10] - Train loss: 0.24862 Train Accucracy: 0.93142 || Validation Loss: 0.26272 Validation Accuracy: 0.92770
    ====================================================================================================
    저장: 6 - 이전 : 0.262746288664639, 현재: 0.26272219385951756
    Epoch[7/10] - Train loss: 0.24762 Train Accucracy: 0.93178 || Validation Loss: 0.26294 Validation Accuracy: 0.92690
    ====================================================================================================
    Epoch[8/10] - Train loss: 0.24775 Train Accucracy: 0.93152 || Validation Loss: 0.26431 Validation Accuracy: 0.92690
    ====================================================================================================
    Epoch[9/10] - Train loss: 0.24701 Train Accucracy: 0.93122 || Validation Loss: 0.26319 Validation Accuracy: 0.92650
    ====================================================================================================
    Epoch[10/10] - Train loss: 0.24443 Train Accucracy: 0.93243 || Validation Loss: 0.26091 Validation Accuracy: 0.92810
    ====================================================================================================
    저장: 10 - 이전 : 0.26272219385951756, 현재: 0.2609139801561832
    201.4175112247467 초



```python
plt.figure(figsize=(10, 5))
plt.subplot(1, 2 ,1)
plt.plot(train_loss_list, label='trainset')
plt.plot(valid_loss_list, label='validationset')
plt.title('Loss')
plt.legend()

plt.subplot(1, 2 ,2)
plt.plot(train_acc_list, label='trainset')
plt.plot(valid_acc_list, label='validationset')
plt.title('accuracy')
plt.legend()

plt.show()
```


![png](/../../images/2023-11-10-02.Data준비/output_23_0.png)
​    



```python
## nn.Sequential()을 이용해 Layer block 정의하기
### 여러 Layer들을 묶어준다.
layer_block = nn.Sequential(nn.Linear(20, 30), nn.ReLU())

a = torch.ones(1, 20)
r = layer_block(a)
r.shape
r
```




    tensor([[0.4321, 0.0000, 0.0225, 0.0000, 1.0276, 0.3930, 0.0000, 0.0000, 1.0382,
             0.1719, 0.0000, 0.3929, 0.0000, 0.0000, 0.0000, 1.1328, 0.4481, 0.0000,
             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
             0.0000, 0.3957, 0.2792]], grad_fn=<ReluBackward0>)




```python
class BigSizeModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.b1 = nn.Sequential(nn.Linear(784, 2048), nn.ReLU())
        self.b2 = nn.Sequential(nn.Linear(2048, 1024), nn.ReLU())
        self.b3 = nn.Sequential(nn.Linear(1024, 512), nn.ReLU())
        self.b4 = nn.Sequential(nn.Linear(512, 256), nn.ReLU())
        self.b5 = nn.Sequential(nn.Linear(256, 128), nn.ReLU())
        self.b6 = nn.Sequential(nn.Linear(128, 64), nn.ReLU())

        self.output = nn.Linear(64, 10)
    
    def forward(self, X):
        X = nn.Flatten()(X)
        out = self.b1(X)
        out = self.b2(out)
        out = self.b3(out)
        out = self.b4(out)
        out = self.b5(out)
        out = self.b6(out)

        return self.output(out)
```


```python
big_model = BigSizeModel()
torchinfo.summary(big_model, (BATCH_SIZE, 1, 28, 28))
```




    ==========================================================================================
    Layer (type:depth-idx)                   Output Shape              Param #
    ==========================================================================================
    BigSizeModel                             [200, 10]                 --
    ├─Sequential: 1-1                        [200, 2048]               --
    │    └─Linear: 2-1                       [200, 2048]               1,607,680
    │    └─ReLU: 2-2                         [200, 2048]               --
    ├─Sequential: 1-2                        [200, 1024]               --
    │    └─Linear: 2-3                       [200, 1024]               2,098,176
    │    └─ReLU: 2-4                         [200, 1024]               --
    ├─Sequential: 1-3                        [200, 512]                --
    │    └─Linear: 2-5                       [200, 512]                524,800
    │    └─ReLU: 2-6                         [200, 512]                --
    ├─Sequential: 1-4                        [200, 256]                --
    │    └─Linear: 2-7                       [200, 256]                131,328
    │    └─ReLU: 2-8                         [200, 256]                --
    ├─Sequential: 1-5                        [200, 128]                --
    │    └─Linear: 2-9                       [200, 128]                32,896
    │    └─ReLU: 2-10                        [200, 128]                --
    ├─Sequential: 1-6                        [200, 64]                 --
    │    └─Linear: 2-11                      [200, 64]                 8,256
    │    └─ReLU: 2-12                        [200, 64]                 --
    ├─Linear: 1-7                            [200, 10]                 650
    ==========================================================================================
    Total params: 4,403,786
    Trainable params: 4,403,786
    Non-trainable params: 0
    Total mult-adds (M): 880.76
    ==========================================================================================
    Input size (MB): 0.63
    Forward/backward pass size (MB): 6.47
    Params size (MB): 17.62
    Estimated Total Size (MB): 24.71
    ==========================================================================================




```python
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(big_model.parameters(), lr=LR)
```


```python
# 학습
train_loss_list2, train_acc_list2, valid_loss_list2, valid_acc_list2 = train.fit(train_loader, test_loader, big_model, loss_fn, optimizer, N_EPOCH, 
                                                                        save_best_model = True,
                                                                        save_model_path = os.path.join(MODEL_SAVE_ROOT_PATH, 'big_model.pth'),
                                                                        device = device,
                                                                        mode = 'multi'
                                                                       )
```

    Epoch[1/10] - Train loss: 0.12701 Train Accucracy: 0.96365 || Validation Loss: 0.13668 Validation Accuracy: 0.95990
    ====================================================================================================
    저장: 1 - 이전 : inf, 현재: 0.13668256090953945
    Epoch[2/10] - Train loss: 0.08261 Train Accucracy: 0.97612 || Validation Loss: 0.11028 Validation Accuracy: 0.96620
    ====================================================================================================
    저장: 2 - 이전 : 0.13668256090953945, 현재: 0.11027811136096716
    Epoch[3/10] - Train loss: 0.06124 Train Accucracy: 0.98188 || Validation Loss: 0.09488 Validation Accuracy: 0.97310
    ====================================================================================================
    저장: 3 - 이전 : 0.11027811136096716, 현재: 0.09488440541084855
    Epoch[4/10] - Train loss: 0.04865 Train Accucracy: 0.98540 || Validation Loss: 0.10949 Validation Accuracy: 0.97190
    ====================================================================================================
    Epoch[5/10] - Train loss: 0.02825 Train Accucracy: 0.99143 || Validation Loss: 0.07965 Validation Accuracy: 0.97960
    ====================================================================================================
    저장: 5 - 이전 : 0.09488440541084855, 현재: 0.07964686637074919
    Epoch[6/10] - Train loss: 0.02424 Train Accucracy: 0.99297 || Validation Loss: 0.08496 Validation Accuracy: 0.97690
    ====================================================================================================
    Epoch[7/10] - Train loss: 0.02400 Train Accucracy: 0.99278 || Validation Loss: 0.07517 Validation Accuracy: 0.97990
    ====================================================================================================
    저장: 7 - 이전 : 0.07964686637074919, 현재: 0.07516641994036036
    Epoch[8/10] - Train loss: 0.02564 Train Accucracy: 0.99178 || Validation Loss: 0.07979 Validation Accuracy: 0.97880
    ====================================================================================================
    Epoch[9/10] - Train loss: 0.01986 Train Accucracy: 0.99365 || Validation Loss: 0.09614 Validation Accuracy: 0.97800
    ====================================================================================================
    Epoch[10/10] - Train loss: 0.03558 Train Accucracy: 0.98858 || Validation Loss: 0.11282 Validation Accuracy: 0.97350
    ====================================================================================================
    492.53137588500977 초



```python
plt.figure(figsize=(10, 5))
plt.subplot(1, 2 ,1)
plt.plot(train_loss_list2, label='trainset')
plt.plot(valid_loss_list2, label='validationset')
plt.title('Loss')
plt.legend()

plt.subplot(1, 2 ,2)
plt.plot(train_acc_list2, label='trainset')
plt.plot(valid_acc_list2, label='validationset')
plt.title('accuracy')
plt.legend()

plt.show()
```


![png](/../../images/2023-11-10-02.Data준비/output_30_0.png)
​    


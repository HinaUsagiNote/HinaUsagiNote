---
layout: single
title: 'Learning rate Decay'
typora-root-url: ../
categories: Deeplearning_Pytorch.07.딥러닝모델 성능개선
tag: Pytorch
toc: true
---


# Learning rate decay

### Optimizer와 Learning rate scheduler의 속성, 메소드 확인
- 파이토치는 `torch.optim` 모듈에서 다양한 Learning rate 알고리즘을 제공한다.




```python
import torch
import torch.nn as nn

import torchinfo

from module import train, data

import os
import matplotlib.pyplot as plt
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device
```




    'cpu'




```python
optimizer = torch.optim.Adam(bn_model.parameters(), lr=0.001)
```


```python
# 옵티마이저 객체 관련 여러 정보들을 조회 => optimizer.param_groups
print(type(optimizer.param_groups), len(optimizer.param_groups))
print(type(optimizer.param_groups[0]))
```

    <class 'list'> 1
    <class 'dict'>



```python
info_dict = optimizer.param_groups[0]
info_dict.keys()
```




    dict_keys(['params', 'lr', 'betas', 'eps', 'weight_decay', 'amsgrad', 'maximize', 'foreach', 'capturable', 'differentiable', 'fused'])




```python
info_dict['lr'] # 현재 learning rate
```




    0.001




```python
# 학습률의 변화흐름을 시각화 하는 함수
def plot_lr(title, lr_list):
    # title - 그래프 제목
    # lr_list - 에폭별 적용된 learning rate 리스트
    plt.figure(figsize=(10, 5))
    plt.plot(range(len(lr_list)), lr_list)
    
    plt.title(title)
    plt.xticks([x for x in range(len(lr_list)) if x % 5 == 0], rotation=45)
    plt.xlabel('Epochs')
    plt.ylabel('LR')
    plt.grid(True, axis='x', linestyle=':')
    plt.show()
```


```python
# 테스트
plot_lr('My Scheduler', list(range(1, 1000, 10)))
```


![output_51_0](/../../images/2023-11-10-05.Learning rate Decay/output_51_0.png)
    


#### StepLR


```python
optimizer = torch.optim.Adam(bn_model.parameters(), lr=0.001)
# lr_scheduler: optimizer의 lr 는 초기 확습률
step_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,      # 학습률을 변화시킬 Optimizer
                                                 step_size=30,   # 몇 step/epoch 마다 LR를 변경시킬지 간격
                                                 gamma=0.5,      # 변경 비율. new_lr = lr * gamma
                                                )
# optimizer의 lr를 30 에폭/step 마다 0.5 배씩 변경
```


```python
# 현재 LR 조회
optimizer.param_groups[0]['lr'], step_scheduler.get_last_lr()
```




    (0.001, [0.001])




```python
N_EPOCH = 200
STEP_SIZE = 10
steplr_list = [] # epoch별 lr 를 저장할 변수

for epoch in range(N_EPOCH):

    # step
    for stpe in range(STEP_SIZE): # for x, y in dataloader:
        # 학습
        # 모델 예측 - loss 계산 -> 파라미터 업데이트
        optimizer.step()
        # learning rate 변경 요청.
        ## step단위로 변경
        # step_scheduler.step() 
    step_scheduler.step() # epoch 단위로 변경
    steplr_list.append(step_scheduler.get_last_lr()[0]) # 현재 epoch의 lr를 저장.
```


```python
plot_lr('Step LR', steplr_list)
```


![png](/../../images/2023-11-10-05.Learning rate Decay/output_56_0.png)
​    


#### CosineAnnealingLR
cosine 그래프를 그리면서 learning rate를 변경 하는 방식.  
최근에는 learning rate를 단순히 감소시키기 보다는 감소와 증가를 반복하여 진동하는 방식으로 최적점을 찾아가는 알고리즘을 많이 사용한다. 이러한 방법 중 가장 간단하면서도 많이 사용되는 방법이 CosineAnnealingLR이다.


```python
# Annealing: 가열 <-> 냉각
optimizer = torch.optim.Adam(bn_model.parameters(), lr=0.001)
ca_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,
                                                          T_max=10, # 한 cyclr 주기(단위: 에폭, step)
                                                          eta_min=0.00001, # 최소 학습률                                                          
                                                         )
# 범위: 초기 LR ~ eta_min
```


```python
ca_lr_list = []
for epoch in range(N_EPOCH):
    # 한 에폭
    for step in range(STEP_SIZE):
        # 한 스텝
        optimizer.step()
    
    ca_scheduler.step()
    ca_lr_list.append(ca_scheduler.get_last_lr()[0])
```


```python
plot_lr('CosineAnnealingLR', ca_lr_list)
```


![png](/../../images/2023-11-10-05.Learning rate Decay/output_60_0.png)
​    


#### CosineAnnealingWarmRestarts

cosine annealing의 스케쥴링에 cosine 주기의 에폭을 점점 늘리거나 줄일 수 있다. (보통 늘린다.)


```python
optimizer = torch.optim.Adam(bn_model.parameters(), lr=0.001)
caws_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 
                                                                     T_0=10,  # 초기 변화 주기 (cycle)
                                                                     T_mult=2, # 변화주기를 어떤 비율로 변경할지
                                                                     eta_min=1e-5, # 0.00001. 최소학습률
                                                                     verbose=True # 학습률 변경할때마다 로그 출력
                                                                     )
# 변화 범위: 초기 학습률 ~ eat_min
```

    Epoch 00000: adjusting learning rate of group 0 to 1.0000e-03.



```python
caws_lr_list = []

for epoch in range(N_EPOCH):
    # 한 에폭
    for step in range(STEP_SIZE):
        # 한 스텝
        optimizer.step()
    caws_scheduler.step()
    caws_lr_list.append(caws_scheduler.get_last_lr()[0])
```


```python
plot_lr('CosineAnnealingWarmRestart', caws_lr_list)
```


![png](/../../images/2023-11-10-05.Learning rate Decay/output_64_0.png)
​    


---
layout: single
title: 'Batch Normalization'
typora-root-url: ../
categories: Deeplearning_Pytorch.07.딥러닝모델 성능개선
tag: Pytorch
toc: true
---

# Batch Normalization

- Dense => BN => Activation


```python
import torch
import torch.nn as nn

import torchinfo

from module import train, data

import os
import matplotlib.pyplot as plt
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device
```




    'cpu'




```python
a = torch.rand(10, 5) # (10-data수, 5-feature수)
bn = nn.BatchNorm1d(5) # features => 1차원: BatchNorm1d(), 3차원(이미지): BatchNorm2d()
## BatchNorm1d(feature수)
## BatchNorm2d(channel수)
b = bn(a)
```


```python
# featuer별 평균/표준편차
print('a의 평균:', a.mean(axis=0))
print('a의 표준편차', a.std(axis=0))
```

    a의 평균: tensor([0.4685, 0.5811, 0.4800, 0.5975, 0.4469])
    a의 표준편차 tensor([0.2842, 0.2615, 0.2714, 0.2332, 0.2747])



```python
# BatchNorm1d 결과의 featur 별 평균/표준편차
print('b의 평균:', b.mean(axis=0))
print('b의 표준편차', b.std(axis=0))
```

    b의 평균: tensor([3.5763e-08, 8.3447e-08, 5.9605e-08, 1.1921e-07, 1.1921e-08],
           grad_fn=<MeanBackward1>)
    b의 표준편차 tensor([1.0540, 1.0540, 1.0540, 1.0540, 1.0540], grad_fn=<StdBackward0>)



```python
a
```




    tensor([[0.6175, 0.7865, 0.4116, 0.1729, 0.2551],
            [0.2601, 0.8940, 0.5634, 0.5865, 0.6799],
            [0.7627, 0.7826, 0.4996, 0.5271, 0.6931],
            [0.7150, 0.8986, 0.8027, 0.5736, 0.4079],
            [0.1409, 0.5249, 0.1900, 0.6231, 0.9438],
            [0.7002, 0.4319, 0.2278, 0.9505, 0.0989],
            [0.5890, 0.5548, 0.6405, 0.5524, 0.3799],
            [0.1221, 0.1146, 0.0641, 0.7287, 0.6095],
            [0.0679, 0.5399, 0.4732, 0.3502, 0.1469],
            [0.7099, 0.2835, 0.9272, 0.9102, 0.2539]])




```python
b
```




    tensor([[ 0.5523,  0.8276, -0.2657, -1.9193, -0.7357],
            [-0.7729,  1.2611,  0.3241, -0.0497,  0.8940],
            [ 1.0908,  0.8121,  0.0762, -0.3183,  0.9447],
            [ 0.9139,  1.2794,  1.2535, -0.1081, -0.1498],
            [-1.2148, -0.2266, -1.1266,  0.1156,  1.9062],
            [ 0.8590, -0.6014, -0.9798,  1.5954, -1.3350],
            [ 0.4468, -0.1060,  0.6233, -0.2041, -0.2570],
            [-1.2845, -1.8804, -1.6156,  0.5930,  0.6237],
            [-1.4856, -0.1660, -0.0264, -1.1177, -1.1508],
            [ 0.8949, -1.1997,  1.7370,  1.4133, -0.7402]],
           grad_fn=<NativeBatchNormBackward0>)




```python
# Linear -> BatchNorm -> Activation (-> Dropout)
class BNModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.b1 = nn.Sequential(nn.Linear(784, 256), 
                                nn.BatchNorm1d(256),
                                nn.ReLU()
                               )
        self.b2 = nn.Sequential(nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU())
        self.b3 = nn.Sequential(nn.Linear(128, 64), nn.BatchNorm1d(64), nn.ReLU())
        self.output = nn.Linear(64, 10)

    def forward(self, X):
        out = nn.Flatten()(X)
        out = self.b1(out)
        out = self.b2(out)
        out = self.b3(out)
        out = self.output(out)
        return out
```


```python
# 모델 생성
bn_model = BNModel().to(device)
torchinfo.summary(bn_model, (1, 28, 28))
```




    ==========================================================================================
    Layer (type:depth-idx)                   Output Shape              Param #
    ==========================================================================================
    BNModel                                  [1, 10]                   --
    ├─Sequential: 1-1                        [1, 256]                  --
    │    └─Linear: 2-1                       [1, 256]                  200,960
    │    └─BatchNorm1d: 2-2                  [1, 256]                  512
    │    └─ReLU: 2-3                         [1, 256]                  --
    ├─Sequential: 1-2                        [1, 128]                  --
    │    └─Linear: 2-4                       [1, 128]                  32,896
    │    └─BatchNorm1d: 2-5                  [1, 128]                  256
    │    └─ReLU: 2-6                         [1, 128]                  --
    ├─Sequential: 1-3                        [1, 64]                   --
    │    └─Linear: 2-7                       [1, 64]                   8,256
    │    └─BatchNorm1d: 2-8                  [1, 64]                   128
    │    └─ReLU: 2-9                         [1, 64]                   --
    ├─Linear: 1-4                            [1, 10]                   650
    ==========================================================================================
    Total params: 243,658
    Trainable params: 243,658
    Non-trainable params: 0
    Total mult-adds (M): 0.24
    ==========================================================================================
    Input size (MB): 0.00
    Forward/backward pass size (MB): 0.01
    Params size (MB): 0.97
    Estimated Total Size (MB): 0.99
    ==========================================================================================




```python
# train
# loss
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(bn_model.parameters(), lr=LR)

result_bn = train.fit(train_loader, test_loader, bn_model, loss_fn, optimizer, N_EPOCH, save_best_model=False, early_stopping=False, device=device, 
                      mode='multi')
```

    Epoch[1/10] - Train loss: 0.08985 Train Accucracy: 0.97643 || Validation Loss: 0.10999 Validation Accuracy: 0.96890
    ====================================================================================================
    Epoch[2/10] - Train loss: 0.04720 Train Accucracy: 0.98740 || Validation Loss: 0.07929 Validation Accuracy: 0.97690
    ====================================================================================================
    Epoch[3/10] - Train loss: 0.02970 Train Accucracy: 0.99152 || Validation Loss: 0.06659 Validation Accuracy: 0.98020
    ====================================================================================================
    Epoch[4/10] - Train loss: 0.02131 Train Accucracy: 0.99358 || Validation Loss: 0.06555 Validation Accuracy: 0.98060
    ====================================================================================================
    Epoch[5/10] - Train loss: 0.01699 Train Accucracy: 0.99527 || Validation Loss: 0.07176 Validation Accuracy: 0.97890
    ====================================================================================================
    Epoch[6/10] - Train loss: 0.01147 Train Accucracy: 0.99698 || Validation Loss: 0.06305 Validation Accuracy: 0.98120
    ====================================================================================================
    Epoch[7/10] - Train loss: 0.01237 Train Accucracy: 0.99647 || Validation Loss: 0.06570 Validation Accuracy: 0.98230
    ====================================================================================================
    Epoch[8/10] - Train loss: 0.00880 Train Accucracy: 0.99773 || Validation Loss: 0.06468 Validation Accuracy: 0.98100
    ====================================================================================================
    Epoch[9/10] - Train loss: 0.00667 Train Accucracy: 0.99813 || Validation Loss: 0.06635 Validation Accuracy: 0.98140
    ====================================================================================================
    Epoch[10/10] - Train loss: 0.00842 Train Accucracy: 0.99750 || Validation Loss: 0.07005 Validation Accuracy: 0.98080
    ====================================================================================================
    229.6033055782318 초


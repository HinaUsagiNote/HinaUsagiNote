---
layout: single
title: 'Dropout'
typora-root-url: ../
categories: Deeplearning_Pytorch.07.딥러닝모델 성능개선
tag: Pytorch
toc: true
---

# Dropout

- dropout 각 레이어에 적용
    - dropout은 nn.Dropout 객체를 사용
    - 객체 생성시 dropout_rate 설정: 0.2 ~ 0.5
    - Drop시킬 노드를 가진 Layer 뒤에 추가한다.


```python
import torch
import torch.nn as nn

import torchinfo

from module import train, data

import os
import matplotlib.pyplot as plt
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device
```




    'cpu'




```python
LR = 0.001
N_EPOCH = 10
BATCH_SIZE = 200
```


```python
# DATASET, MODEL 저장할 ROOT 디렉토리 경로
DATASET_ROOT_PATH = 'datasets'
MODEL_SAVE_ROOT_PATH = 'models'

train_loader = data.load_mnist_dataset(DATASET_ROOT_PATH, BATCH_SIZE, True)
test_loader = data.load_mnist_dataset(DATASET_ROOT_PATH, BATCH_SIZE, False)
```


```python
class DropoutModel(nn.Module):

    def __init__(self, drop_rate=0.5):
        super().__init__()
        self.b1 = nn.Sequential(nn.Linear(784, 256),
                                nn.ReLU(),
                                nn.Dropout(p=drop_rate)
                               )
        self.b2 = nn.Sequential(nn.Linear(256, 256), nn.ReLU(), nn.Dropout(p=drop_rate))
        self.b3 = nn.Sequential(nn.Linear(256, 128), nn.ReLU(), nn.Dropout(p=drop_rate))
        self.b4 = nn.Sequential(nn.Linear(128, 128), nn.ReLU(), nn.Dropout(p=drop_rate))
        self.output = nn.Sequential(nn.Linear(128, 10), nn.Dropout(p=drop_rate))
    
    def forward(self, X):
        out = nn.Flatten()(X)
        out = self.b1(out)
        out = self.b2(out)
        out = self.b3(out)
        out = self.b4(out)
        out = self.output(out)
        return out
```


```python
d_model = DropoutModel().to(device) # drop_rate=0.5(Default)
torchinfo.summary(d_model, (BATCH_SIZE, 1, 28, 28))
```




    ==========================================================================================
    Layer (type:depth-idx)                   Output Shape              Param #
    ==========================================================================================
    DropoutModel                             [200, 10]                 --
    ├─Sequential: 1-1                        [200, 256]                --
    │    └─Linear: 2-1                       [200, 256]                200,960
    │    └─ReLU: 2-2                         [200, 256]                --
    │    └─Dropout: 2-3                      [200, 256]                --
    ├─Sequential: 1-2                        [200, 256]                --
    │    └─Linear: 2-4                       [200, 256]                65,792
    │    └─ReLU: 2-5                         [200, 256]                --
    │    └─Dropout: 2-6                      [200, 256]                --
    ├─Sequential: 1-3                        [200, 128]                --
    │    └─Linear: 2-7                       [200, 128]                32,896
    │    └─ReLU: 2-8                         [200, 128]                --
    │    └─Dropout: 2-9                      [200, 128]                --
    ├─Sequential: 1-4                        [200, 128]                --
    │    └─Linear: 2-10                      [200, 128]                16,512
    │    └─ReLU: 2-11                        [200, 128]                --
    │    └─Dropout: 2-12                     [200, 128]                --
    ├─Sequential: 1-5                        [200, 10]                 --
    │    └─Linear: 2-13                      [200, 10]                 1,290
    │    └─Dropout: 2-14                     [200, 10]                 --
    ==========================================================================================
    Total params: 317,450
    Trainable params: 317,450
    Non-trainable params: 0
    Total mult-adds (M): 63.49
    ==========================================================================================
    Input size (MB): 0.63
    Forward/backward pass size (MB): 1.24
    Params size (MB): 1.27
    Estimated Total Size (MB): 3.14
    ==========================================================================================




```python
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(d_model.parameters(), lr=LR)

result = train.fit(train_loader, test_loader, d_model, loss_fn, optimizer, N_EPOCH, save_best_model=False, early_stopping=False, device=device, mode='multi')
```

    Epoch[1/10] - Train loss: 0.29718 Train Accucracy: 0.93165 || Validation Loss: 0.29076 Validation Accuracy: 0.93140
    ====================================================================================================
    Epoch[2/10] - Train loss: 0.20175 Train Accucracy: 0.95115 || Validation Loss: 0.20255 Validation Accuracy: 0.94890
    ====================================================================================================
    Epoch[3/10] - Train loss: 0.16657 Train Accucracy: 0.96088 || Validation Loss: 0.17110 Validation Accuracy: 0.95880
    ====================================================================================================
    Epoch[4/10] - Train loss: 0.12829 Train Accucracy: 0.96875 || Validation Loss: 0.14193 Validation Accuracy: 0.96140
    ====================================================================================================
    Epoch[5/10] - Train loss: 0.12563 Train Accucracy: 0.97037 || Validation Loss: 0.13734 Validation Accuracy: 0.96710
    ====================================================================================================
    Epoch[6/10] - Train loss: 0.11393 Train Accucracy: 0.97392 || Validation Loss: 0.12805 Validation Accuracy: 0.97030
    ====================================================================================================
    Epoch[7/10] - Train loss: 0.11319 Train Accucracy: 0.97408 || Validation Loss: 0.13106 Validation Accuracy: 0.96800
    ====================================================================================================
    Epoch[8/10] - Train loss: 0.09571 Train Accucracy: 0.97757 || Validation Loss: 0.11463 Validation Accuracy: 0.97140
    ====================================================================================================
    Epoch[9/10] - Train loss: 0.09665 Train Accucracy: 0.97868 || Validation Loss: 0.11790 Validation Accuracy: 0.97390
    ====================================================================================================
    Epoch[10/10] - Train loss: 0.08790 Train Accucracy: 0.97927 || Validation Loss: 0.11196 Validation Accuracy: 0.97440
    ====================================================================================================
    208.18259477615356 초



```python
# 표준정규
a = torch.randn(5, 3)
ln = nn.Linear(3, 4)
b = ln(a)
do = nn.Dropout(0.5)
c = do(b)
```


```python
a # input
b # linear
```




    tensor([[-0.0559, -0.1409, -0.3585,  0.1669],
            [-0.4429,  1.1053, -0.6790,  0.6994],
            [-0.5131, -0.1918, -0.8159,  0.4282],
            [-0.2911,  0.0528, -0.5525,  0.4402],
            [-0.9957, -0.8803, -1.1406,  1.0986]], grad_fn=<AddmmBackward0>)




```python
c
```




    tensor([[-0.1118, -0.0000, -0.0000,  0.0000],
            [-0.0000,  2.2106, -0.0000,  1.3987],
            [-1.0261, -0.0000, -1.6318,  0.0000],
            [-0.0000,  0.0000, -1.1051,  0.0000],
            [-1.9914, -0.0000, -0.0000,  2.1971]], grad_fn=<MulBackward0>)

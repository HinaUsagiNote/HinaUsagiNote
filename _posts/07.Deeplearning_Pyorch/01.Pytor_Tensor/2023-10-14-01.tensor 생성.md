---
layout: single
title: 'Tensor 생성'
typora-root-url: ../
categories: Deeplearning_Pytorch.01.Pytorch_Tensor
tag: Pytorch
toc: true
---

# Tensor 생성
- 파이토치에서 데이터를 저장하는 자료구조
- ndarray와 성격, 사용법이 유사하다.

```python
import torch

torch.__version__
```




    '2.1.0+cpu'




```python
a = torch.tensor([[1,2],[3,4]], dtype=torch.float32)

print("shape:", a.shape, a.size()) # 전체 shape
print("축별 크기:", a.shape[0], a.size(0)) # 축별 size
print("type:", a.type(), a.dtype)
print('차원크기:', a.dim(), a.ndim)
print('원소개수:', a.numel())
print("device:", a.device) 
```

    shape: torch.Size([2, 2]) torch.Size([2, 2])
    축별 크기: 2 2
    type: torch.FloatTensor torch.float32
    차원크기: 2 2
    원소개수: 4
    device: cpu



```python
print(a)
```

    tensor([[1., 2.],
            [3., 4.]])



```python
torch.tensor(range(10))
```




    tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])




```python
#Float, Double(32, 64bit 실수)/Int, Long(32, 64 bit 정수) type Tensor
b = torch.FloatTensor([1,3,7])  #float32
print(b.dtype)
c = torch.IntTensor([10,20,30]) # int64
print(c.dtype)
d = torch.DoubleTensor([1, 2, 3]) # torch.tensor([], dtype=torch.float64)
print(d.dtype)
e = torch.LongTensor([10, 20, 30, 40]) # torch.tensor([], dtype=torch.int64)
print(e.dtype)
```

    torch.float32
    torch.int32
    torch.float64
    torch.int64


## 특정 값으로 구성된 Tensor 생성
- **torch.zeros(\*size), zeros_like(텐서)**: 0으로 구성된 tensor 생성
- **torch.ones(\*size), ones_like(텐서)**: 1로 구성된 tensor생성
- **torch.full(\*size, fill_value), full_like(텐서, fill_value)**: 지정한 값으로 구성된 tensor생성
  


```python
torch.zeros(3,2,3) # 3 X 2 X 3 3차원, 0으로 구성
torch.ones(2,3) # 2 X 3 2차원, 1으로 구성
torch.full((3,2), fill_value=100) # 3 X 2 2차원. 100으로 구성

```




    tensor([[100, 100],
            [100, 100],
            [100, 100]])




```python
a = torch.tensor([[1, 2],[3,4]]) # 2 X 2
print(a.shape)
b = torch.zeros_like(a) # a와 동일한 shape의 tensor를 생성
b = torch.ones_like(a)
b = torch.full_like(a, 20)
b.shape
```

    torch.Size([2, 2])





    torch.Size([2, 2])



## 동일한 간격으로 떨어진 값들로 구성된 배열생성
- **torch.arange(start=0, end, step=1)** 
- **torch.linspace(start, end, steps,)** : steps - 원소개수


```python
torch.arange(10) # 0 ~ 10-1
torch.arange(0, 1, 0.1) # 0 ~ 1, 증갑: 0.1
torch.arange(10, 1, -1) # 10 ~ 1+1, 증감: -1
```




    tensor([10,  9,  8,  7,  6,  5,  4,  3,  2])




```python
torch.linspace(0, 10, 5) # 0 ~ 10, 등분한값: 5개
torch.linspace(0, 1, 11) # 0 ~ 1, 등분한값: 10개

```




    tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,
            0.9000, 1.0000])



## 빈 tensor 생성
- **torch.empty(\*size)**


```python
torch.empty(3,2) # 의미없는 값들로 채운다
```




    tensor([[0., 0.],
            [0., 0.],
            [0., 0.]])



## 난수를 이용한 생성

- **torch.rand(\*size)**: 0 ~ 1사이 실수로 구성된 배열을 생성. 각 값은 균등분포를 따른다.
- **torch.randn(\*size)**: 표준정규분포(평균:0, 표준편차:1)를 따르는 실수로 구성된 배열 생성
- **torch.randint(low=0, high, size)**: 지정한 범위의 정수로 구성된 배열 생성
- **torch.randperm(n)**: 0 ~ n 사이의 정수를 랜덤하게 섞은 값을 원소로 가지는 배열 생성 (데이터들을 섞을때)
  


```python
torch.manual_seed(1004)  # seed 설정
torch.rand(100) # 0 ~ 1사이 100개 생성
torch.randn(3,3) # 평균 - 표준편차 x 2 ~ 평균 + 표준편차 x 2
torch.randint(1, 10, (3,3))  
torch.randperm(5) # 0, 1, 2, 3, 4
```




    tensor([1, 3, 0, 4, 2])




```python
a = torch.arange(1, 10)
print(a)
# index를 섞어서 데이터를 섞는다.(shuffle)
idx = torch.randperm(9)
a[idx]
```

    tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])





    tensor([3, 6, 4, 8, 2, 7, 5, 1, 9])



## tensor를 상수로 변환
- tensor객체.item()
    - Scalar(상수) tensor를 python 상수로 변환


```python
# tensor.item() -> torch.Tensor객체 -> 파이썬 상수로 변환
a = torch.tensor(10) # 0차원
print(a)
print(a.item()) # 파이썬 정수로 변환
```

    tensor(10)
    10



```python
b = torch.tensor([20]) # 2차원
print(b)
print(b.item()) #원소가 하나인 배열 변환 가능
```

    tensor([20])
    20



```python
c = torch.tensor([1, 10, 100])
print(c)
# print(c.item()) #원소가 여러개일 경우 Exception발생
```

    tensor([  1,  10, 100])



```python
d = torch.tensor([10], device='cpu') # device='cuda' => VRAM(GPU의 RAM)에 저장
# VRAM에 저장된 값도 추출가능
print(d)
print(d.item())
```

    tensor([10])
    10


## ndarray 호환

- ndarray를 tensor로 생성
    - **torch.tensor(ndarray)**
    - **torch.from_numpy(ndarray)**
- tensor를 ndarray로 변환
    - **tensor.numpy()**
    - tensor가 gpu에 있을 경우 cpu로 옮긴 뒤 변환해야 한다.


```python
import numpy as np
import torch
```


```python
torch.tensor([1,2,3])
```




    tensor([1, 2, 3])




```python
# ndarray -> tensor
arr = np.arange(1,10)

torch.tensor(arr)
torch.from_numpy(arr)
```




    tensor([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)




```python
# tensor -> ndarray
t = torch.randn(3,3)
print(t)
b = t.numpy()
print(type(b))
b
```

    tensor([[-0.1761,  2.3487,  1.1318],
            [ 0.1235,  0.7647,  0.5572],
            [ 0.6796, -0.4447, -1.1951]])
    <class 'numpy.ndarray'>





    array([[-0.17609052,  2.3487213 ,  1.1318233 ],
           [ 0.12350701,  0.764688  ,  0.5571773 ],
           [ 0.67956996, -0.44465092, -1.1950685 ]], dtype=float32)




```python
t2 = torch.randn(2,2, device="cpu")
t2
```




    tensor([[-1.6682, -1.7317],
            [-0.5429, -0.5746]])




```python
# VRAM에 있는 Tensor객체는 ndarray로 변환이 안됨
# t2.numpy() 
## VRAM의 Tensor를 RAM(CPU)로 이동시킨뒤에 변환
t2.to("cpu").numpy()
```




    array([[-1.6682048 , -1.7316618 ],
           [-0.5429196 , -0.57457906]], dtype=float32)



## Tensor gpu/cpu 메모리로 옮기기

- pytorch는 데이터셋인 tensor를 cpu메모리와 gpu 메모리로 서로 옮길 수 있다.
    - 데이터에 대한 연산처리를 어디서 하느냐에 따라 메모리를 선택한다.
    - 장치는 문자열로 설정한다.
        - CPU 사용: "cpu"
        - nvida GPU: "cuda"
        - Apple m1: "mps"
- 옮기기
    - tensor 생성시 `device` 파라미터를 이용해 설정
    - `tensor.to(device)`를 이용해 설정
- 현재 실행환경에서 어떤 장비를 사용할 수 있는지 확인
    - nvidia gpu 사용가능확인
        - `torch.cuda.is_available()` - nvida gpu 사용가능 여부
        - `torch.backends.mps.is_available()` - M1 사용가능 여부


```python
# torch.cuda.is_avilable()
# torch.backends.mps.is_available()
```


```python
# device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device
```




    'cpu'




```python
t = torch.tensor([1, 2, 3], dtype=torch.float32, device=device)
print(t.device)
t

t2 = t.to("cpu")
t2
```

    cpu





    tensor([1., 2., 3.])



## 원소 조회및 변경 - indexing/slicing

- 대부분 Numpy 와 동일
    - **slicing에서 step을 <u>음수로 지정할 수 없다.</u>**



```python
import torch
```


```python
t = torch.randint(-10, 10, (100, ))
t.shape
t
```




    tensor([  6,   2,   0,  -4,  -4,  -4,   6,  -3,  -6,   8,  -3,   6,   9, -10,
            -10,   5,   5,   2,   6,   0,  -7,   8,   1,  -6,   8,   2,   7,   4,
             -3,  -1,   2,   2,   8,   6,  -3,  -6,   2,   8,  -8,   3,   9,  -6,
             -1,   6,   2, -10,  -4,  -2,   5,   2,   3,   4,  -5,  -4,  -1,  -9,
             -1,   0,   6,   9,   1,   4,   3,  -9,   4,  -3,  -5,  -2,   3,  -8,
              0,  -7,   2,  -4,   8,  -6,  -8,   9,   4,   2,   8, -10,  -9,   1,
             -2,  -5,  -4,   3,  -3,  -7,  -6,   0,   4,   6,   2,  -1,  -3,  -2,
             -7,  -6])




```python
t[0] # indexing
# t[0].item()
t[[1, 5, -1]] # 여러개 조회 -> fancy Indexing
```




    tensor([ 2, -4, -6])




```python
print(t[:5])
print(t[10:15])
print(t[90:])
print(t[3:30:3])
# t[10:1:-2]  #에러
t[1:10:2].flip(dims=(0,)) # reverse, dims=axis
```

    tensor([ 6,  2,  0, -4, -4])
    tensor([ -3,   6,   9, -10, -10])
    tensor([-6,  0,  4,  6,  2, -1, -3, -2, -7, -6])
    tensor([-4,  6,  8,  9,  5,  6,  8,  8,  4])





    tensor([ 8, -3, -4, -4,  2])




```python
# boolean index
t[t > 0] # 0보다 큰 값들만 조회
```




    tensor([6, 2, 6, 8, 6, 9, 5, 5, 2, 6, 8, 1, 8, 2, 7, 4, 2, 2, 8, 6, 2, 8, 3, 9,
            6, 2, 5, 2, 3, 4, 6, 9, 1, 4, 3, 4, 3, 2, 8, 9, 4, 2, 8, 1, 3, 4, 6, 2])




```python
t
```




    tensor([  6,   2,   0,  -4,  -4,  -4,   6,  -3,  -6,   8,  -3,   6,   9, -10,
            -10,   5,   5,   2,   6,   0,  -7,   8,   1,  -6,   8,   2,   7,   4,
             -3,  -1,   2,   2,   8,   6,  -3,  -6,   2,   8,  -8,   3,   9,  -6,
             -1,   6,   2, -10,  -4,  -2,   5,   2,   3,   4,  -5,  -4,  -1,  -9,
             -1,   0,   6,   9,   1,   4,   3,  -9,   4,  -3,  -5,  -2,   3,  -8,
              0,  -7,   2,  -4,   8,  -6,  -8,   9,   4,   2,   8, -10,  -9,   1,
             -2,  -5,  -4,   3,  -3,  -7,  -6,   0,   4,   6,   2,  -1,  -3,  -2,
             -7,  -6])




```python
# 변경
t[0] = 100
t
```




    tensor([100,   2,   0,  -4,  -4,  -4,   6,  -3,  -6,   8,  -3,   6,   9, -10,
            -10,   5,   5,   2,   6,   0,  -7,   8,   1,  -6,   8,   2,   7,   4,
             -3,  -1,   2,   2,   8,   6,  -3,  -6,   2,   8,  -8,   3,   9,  -6,
             -1,   6,   2, -10,  -4,  -2,   5,   2,   3,   4,  -5,  -4,  -1,  -9,
             -1,   0,   6,   9,   1,   4,   3,  -9,   4,  -3,  -5,  -2,   3,  -8,
              0,  -7,   2,  -4,   8,  -6,  -8,   9,   4,   2,   8, -10,  -9,   1,
             -2,  -5,  -4,   3,  -3,  -7,  -6,   0,   4,   6,   2,  -1,  -3,  -2,
             -7,  -6])




```python
t = torch.arange(1, 10).reshape(3,3) # 2차원: index - 2방향.
t
```




    tensor([[1, 2, 3],
            [4, 5, 6],
            [7, 8, 9]])




```python
# t[ 0축, 1축 ]
print(t[0])
print(t[0, 0])
print(t[1, 1])
```

    tensor([1, 2, 3])
    tensor(1)
    tensor(5)



```python
t[[1,0], [2,2]]  #(1,2), (0,2)
# t[[ 첫번째값, 두번째값 ]-0축,[ 첫번째값, 두번째값 ]-1축]
```




    tensor([6, 3])


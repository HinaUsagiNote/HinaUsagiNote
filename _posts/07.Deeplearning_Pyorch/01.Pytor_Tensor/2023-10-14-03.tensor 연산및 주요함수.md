---
layout: single
title: 'Tensor 연산및 주요함수'
typora-root-url: ../
categories: Deeplearning_Pytorch.01.Pytorch_Tensor
tag: Pytorch
toc: true
---

# tensor 연산 및 주요 함수

## element-wise 연산
- tensor와 상수 연산시, tensor와 tensor간 연산시 원소별로 처리한다.
- 행렬곱 연산을 제외하고 tensor간 연산시 피연산지 tensor간에 shape이 같아야 한다.
    - shape이 다를 경우 조건이 맞으면 broadcasting을 한 뒤에 연산한다. (size가 다른 축의 경우 한개의 피연산자 size가 1일 경우 복사하여 shape을 맞춘다.)
    


```python
import torch
```


```python
# element-wise연산 : shape이 같이야한다
# 행렬곱 2 X 5 @ 5 X 6 = 2 X 6
```


```python
import torch

a = torch.arange(10).reshape(2,5)
b = torch.arange(10,20).reshape(2,5)
c = torch.arange(50, 55)

print(a)
print(b)
print(c)
```

    tensor([[0, 1, 2, 3, 4],
            [5, 6, 7, 8, 9]])
    tensor([[10, 11, 12, 13, 14],
            [15, 16, 17, 18, 19]])
    tensor([50, 51, 52, 53, 54])



```python
print(a + 100)
print(a - 100)
print(a < 5)
```

    tensor([[100, 101, 102, 103, 104],
            [105, 106, 107, 108, 109]])
    tensor([[-100,  -99,  -98,  -97,  -96],
            [ -95,  -94,  -93,  -92,  -91]])
    tensor([[ True,  True,  True,  True,  True],
            [False, False, False, False, False]])



```python
print(a + b)
print(a == b)
```

    tensor([[10, 12, 14, 16, 18],
            [20, 22, 24, 26, 28]])
    tensor([[False, False, False, False, False],
            [False, False, False, False, False]])



```python
# broadcasting
# (2, 5) + (5, ) : [50, 51, 52, 53, 54]
## (2, 5) + (1, 5) : [[50, 51, 52, 53, 54]]
### (2, 5) + (2, 5) : [[50, 51, 52, 53, 54]], [[50, 51, 52, 53, 54]]
print(a + c)
```

    tensor([[50, 52, 54, 56, 58],
            [55, 57, 59, 61, 63]])


## 주요 연산함수


```python
import torch
```


```python
torch.e
```




    2.718281828459045




```python
x = torch.arange(-4, 5).reshape(3,3)
print(x)
```

    tensor([[-4, -3, -2],
            [-1,  0,  1],
            [ 2,  3,  4]])



```python
print(torch.abs(x)) # 절대값
```

    tensor([[4, 3, 2],
            [1, 0, 1],
            [2, 3, 4]])



```python
print(torch.sqrt(torch.abs(x))) # 제곱근
```

    tensor([[2.0000, 1.7321, 1.4142],
            [1.0000, 0.0000, 1.0000],
            [1.4142, 1.7321, 2.0000]])



```python
print(torch.exp(x))  # torch.e**x
```

    tensor([[1.8316e-02, 4.9787e-02, 1.3534e-01],
            [3.6788e-01, 1.0000e+00, 2.7183e+00],
            [7.3891e+00, 2.0086e+01, 5.4598e+01]])



```python
print(torch.log(torch.abs(x))) # 밑이 e 인 로그 (자연로그)
```

    tensor([[1.3863, 1.0986, 0.6931],
            [0.0000,   -inf, 0.0000],
            [0.6931, 1.0986, 1.3863]])



```python
print(torch.log(torch.exp(torch.tensor(1)))) # torch.log() 밑이 e인 로그계산
```

    tensor(1.)



```python
print(torch.log10(torch.tensor(10)))         # torch.log10() 밑이 10인 로그계산
```

    tensor(1.)



```python
print(torch.log2(torch.tensor(2)))           # torch.log2() 밑이 2인 로그계산
```

    tensor(1.)



```python
y = x + torch.randn((3,3))
```


```python
print(y)
```

    tensor([[-3.2212, -4.0375, -1.9533],
            [-1.7530,  0.7289,  1.4327],
            [ 1.8735,  3.5541,  2.1952]])



```python
print(torch.ceil(y)) # 올림
```

    tensor([[-3., -4., -1.],
            [-1.,  1.,  2.],
            [ 2.,  4.,  3.]])



```python
print(torch.floor(y)) # 내림
```

    tensor([[-4., -5., -2.],
            [-2.,  0.,  1.],
            [ 1.,  3.,  2.]])



```python
print(torch.round(y, decimals=2)) # 소수점 둘째자리이하에서 반올림
```

    tensor([[-3.2200, -4.0400, -1.9500],
            [-1.7500,  0.7300,  1.4300],
            [ 1.8700,  3.5500,  2.2000]])



```python
print(torch.round(y)) # 반올림 -> 정수
```

    tensor([[-3., -4., -2.],
            [-2.,  1.,  1.],
            [ 2.,  4.,  2.]])


### 행렬곱
- `@` 연산자 또는 `torch.matmul(tensor1, tensor2)` 함수 이용


```python
import torch
```


```python
x = torch.FloatTensor([[1, 2],
                       [3, 4],
                       [5, 6]
                      ])

y = torch.FloatTensor([[1, 2],
                       [1, 2],
                      ])
x.size(), y.shape
```




    (torch.Size([3, 2]), torch.Size([2, 2]))




```python
z1 = x @ y
z2 = torch.matmul(x, y)
print(z1.shape, z2.shape)
print(z1) 
print(z2)
```

    torch.Size([3, 2]) torch.Size([3, 2])
    tensor([[ 3.,  6.],
            [ 7., 14.],
            [11., 22.]])
    tensor([[ 3.,  6.],
            [ 7., 14.],
            [11., 22.]])



```python
# Batch 행렬곱(Batch matrix muliplication) - bmm()
# x, y가 가지는 3개의 2차원 배열 간에 행렬곱을 처리한다.
# 400 X 200 사진 행렬곱 200 X 500 사진 각각 1000개 => 한번에 계산하게 해줄때 사용
import torch
x = torch.FloatTensor(3,4,2)
y = torch.FloatTensor(3,2,5)
z = torch.bmm(x, y)
z.shape
z
```




    tensor([[[1.3805e+33, 3.1866e-26, 0.0000e+00, 0.0000e+00, 0.0000e+00],
             [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
             [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
             [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],
    
            [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
             [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
             [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
             [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],
    
            [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
             [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
             [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
             [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]])



## torch.nan, torch.inf
- nan: Not a Number, 주로 결측치를 표현한다.
- inf: infinit 무한. 
    - torch.inf: 양의 무한
    - -torch.inf: 음의 무한
- torch.isnan(tensor)
    - 원소별 결측치 확인
- torch.isinf(tensor)    
    - 원소별 inf 확인


```python
import torch
```


```python
print(torch.inf > 10000000000, torch.inf < 10)
print(-torch.inf < 100000000000, -torch.inf > 10)
```

    True False
    True False



```python
print(torch.log(torch.tensor(-1))) # nan (계산이 안되는-없는값)
print(torch.isnan(torch.tensor([1,2,torch.nan,3,4])).sum())  # nan 여부 확인, sum(): True 갯수
print(torch.isinf(torch.tensor([1,2,3,4,torch.inf])))  # inf 여부 확인
```

    tensor(nan)
    tensor(1)
    tensor([False, False, False, False,  True])


## 기술통계함수


```python
X=torch.randn(3,4)
print(X)
```

    tensor([[-0.5002,  0.8213, -0.3107,  0.5167],
            [ 1.6674,  1.4241, -0.4322, -1.9923],
            [ 1.1839, -1.4941, -0.4244, -1.3173]])



```python
print(torch.sum(X), X.sum()) # 전체합계
print(torch.sum(X, dim=0)) # dim/axis 지정: 지정한 axis의 index가 다른 값끼리 계산
print(torch.sum(X, dim=1)) 
print(torch.sum(X, dim=1, keepdims=True)) # keepdims-True: 차원유지
```

    tensor(-0.8577) tensor(-0.8577)
    tensor([ 2.3510,  0.7513, -1.1672, -2.7929])
    tensor([ 0.5270,  0.6670, -2.0518])
    tensor([[ 0.5270],
            [ 0.6670],
            [-2.0518]])



```python
print(torch.mean(X))
print(torch.mean(X, dim=1), X.mean(dim=1))
print(torch.mean(X, axis=1, keepdims=True), X.mean(dim=1, keepdims=True))
```

    tensor(-0.0715)
    tensor([ 0.1318,  0.1668, -0.5129]) tensor([ 0.1318,  0.1668, -0.5129])
    tensor([[ 0.1318],
            [ 0.1668],
            [-0.5129]]) tensor([[ 0.1318],
            [ 0.1668],
            [-0.5129]])



```python
print(torch.std(X)) # standard deviation 표준 편차
print(torch.var(X)) # variance
print(torch.var(X, dim=0))
print(torch.var(X, dim=0, keepdims=True))
```

    tensor(1.1962)
    tensor(1.4309)
    tensor([1.2948, 2.3733, 0.0046, 1.6857])
    tensor([[1.2948, 2.3733, 0.0046, 1.6857]])



```python
# 메소드
print(X.sum(dim=1, keepdims=True))
print(X.mean(dim=1, keepdims=True))
print(X.std())
```

    tensor([[ 0.5270],
            [ 0.6670],
            [-2.0518]])
    tensor([[ 0.1318],
            [ 0.1668],
            [-0.5129]])
    tensor(1.1962)



```python
X
```




    tensor([[-0.5002,  0.8213, -0.3107,  0.5167],
            [ 1.6674,  1.4241, -0.4322, -1.9923],
            [ 1.1839, -1.4941, -0.4244, -1.3173]])




```python
print(torch.max(X))
```

    tensor(1.6674)



```python
print(torch.max(X, dim=1))
```

    torch.return_types.max(
    values=tensor([0.8213, 1.6674, 1.1839]),
    indices=tensor([1, 0, 0]))



```python
print(torch.max(X, dim=0))  # return_types.max 타입객체로 반환. max값과 max값의 index를 묶어서 반환
```

    torch.return_types.max(
    values=tensor([ 1.6674,  1.4241, -0.3107,  0.5167]),
    indices=tensor([1, 1, 0, 0]))



```python
print(torch.max(X, dim=1))
```

    torch.return_types.max(
    values=tensor([0.8213, 1.6674, 1.1839]),
    indices=tensor([1, 0, 0]))



```python
print(torch.max(X, dim=1).values, torch.max(X, dim=1).indices, sep=" || ")
```

    tensor([0.8213, 1.6674, 1.1839]) || tensor([1, 0, 0])



```python
print(torch.max(X, dim=0, keepdims=True))  #keepdims=True : 차원(rank)를 유지
```

    torch.return_types.max(
    values=tensor([[ 1.6674,  1.4241, -0.3107,  0.5167]]),
    indices=tensor([[1, 1, 0, 0]]))



```python
print(torch.max(X, dim=1, keepdims=True))
```

    torch.return_types.max(
    values=tensor([[0.8213],
            [1.6674],
            [1.1839]]),
    indices=tensor([[1],
            [0],
            [0]]))



```python
print(torch.min(X))
```

    tensor(-1.9923)



```python
print(torch.min(X, dim=0))
```

    torch.return_types.min(
    values=tensor([-0.5002, -1.4941, -0.4322, -1.9923]),
    indices=tensor([0, 2, 1, 1]))



```python
print(torch.min(X, dim=1))
```

    torch.return_types.min(
    values=tensor([-0.5002, -1.9923, -1.4941]),
    indices=tensor([0, 3, 1]))



```python
X
```




    tensor([[-0.5002,  0.8213, -0.3107,  0.5167],
            [ 1.6674,  1.4241, -0.4322, -1.9923],
            [ 1.1839, -1.4941, -0.4244, -1.3173]])




```python
print(torch.argmax(X))
print(torch.argmax(X, dim=0)) # 각 열에서 가장 큰 애가 존재하는 인덱스
print(torch.argmax(X, dim=1)) # 각 행에서 가장 큰 애가 존재하는 인덱스
```

    tensor(4)
    tensor([1, 1, 0, 0])
    tensor([1, 0, 0])

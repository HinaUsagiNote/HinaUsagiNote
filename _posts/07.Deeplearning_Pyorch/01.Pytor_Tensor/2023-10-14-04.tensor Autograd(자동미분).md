---
layout: single
title: 'Tensor Autograd(자동미분)'
typora-root-url: ../
categories: Deeplearning_Pytorch.01.Pytorch_Tensor
tag: Pytorch
toc: true
---

# Autograd(자동미분)

- 자동 미분을 이용해 gradient를 계산하는 pytorch system.
- 딥러닝 모델에서 weight와 bias tensor들(Parameter)은 backpropagation(역전파)를 이용해 gradient를 구해서 loss가 줄어드는 방향으로 update를 하게된다.
- pytorch는 이런 미분 수행을 자동으로 처리해 준다.
    - gradient(기울기)를 구한다는 것은 미분을 한다는 것을 말한다.
- tensor가 미분 가능하려면 `requires_grad=True` 로 설정되 있어야 한다. (default: False)
  


```python
import torch
```


```python
# 알고리즘 최적화 기법 => 미분(경사하강법)
```


```python
# x = torch.tensor([1.]), requires_grad=True)
x = torch.tensor([1.])
x.requires_grad = True
print(x)
print(x.requires_grad)
```

    tensor([1.], requires_grad=True)
    True



```python
y = x ** 2
print(y)
# 계산 결과를 담은 tensor인 y는 계산 결과와 grad_fn에 어떤 계산을 했는지 정보를 담고 있다. (PowBackward0)
# 이는 y 계산에 사용된 x가 requires_grad=True이기 때문이다.
```

    tensor([1.], grad_fn=<PowBackward0>)



```python
# 미분 - tensor.backward() 호출
y.backward()   # dy/dx  gradient 계산 후 결과를 x의 grad attribute에 저장한다.
```


```python
x.grad
# 도함수: y` = 2x 이고 x가 1이었으므로 grad는 2
```




    tensor([2.])




```python
# requires_grad가 True로 설정되 있는 않은 경우 --> Exception
x = torch.tensor([1.])
y = x ** 2
# y.backward()
```


```python
x = torch.tensor([1.], requires_grad=True)
y = x ** 2   # 미분: 2x
z = y * 10   # 미분: 10  ===> 2x * 10

print(y, y.requires_grad)
print(z)
z.backward()
print(x.grad)
```

    tensor([1.], grad_fn=<PowBackward0>) True
    tensor([10.], grad_fn=<MulBackward0>)
    tensor([20.])



```python
## 편미분
x=torch.tensor([1.],requires_grad=True)
y=torch.tensor([1.],requires_grad=True)
z= 2*x**2 + y**2 # 2X^2 + y^2
print(z)
z.backward() 
print(x.grad)  #dz/dx
print(y.grad)  #dz/dy
```

    tensor([3.], grad_fn=<AddBackward0>)
    tensor([4.])
    tensor([2.])



```python
x=torch.tensor([1., 2., 3.] ,requires_grad=True)
y=torch.sum(x**2) # [x1**2 + x2**2 + x3**2]  
y.backward() #[dy/dx1, dy/dx2, dy/dx3] = [2x1, 2x2, 2x3]

print(y)
print(x.grad) # 스칼라를 벡터로 미분
```

    tensor(14., grad_fn=<SumBackward0>)
    tensor([2., 4., 6.])


## torch.no_grad() 
- no_grad() 구문에서 연산을 할 경우 requires_grad=True로 설정되었다 하더라도 gradient를 update하지 않는다.
- 딥러닝 모델 학습이 끝나고 평가할 때는 gradient를 계산할 필요가 없기 때문에 no_grad 구문을 사용한다.



```python
x = torch.tensor(1.0, requires_grad = True)
print(x.requires_grad)

y = x**2

print(y.requires_grad)  # 연산이 결과 requires_grad도 True -> 그래야 미분이 가능하므로.
print(x.requires_grad)
print(y)

y.backward()
print(x.grad)

```

    True
    True
    True
    tensor(1., grad_fn=<PowBackward0>)
    tensor(2.)



```python
x = torch.tensor(1.0, requires_grad = True)
print(x.requires_grad)

with torch.no_grad():
    print(x.requires_grad)
    y = x**2 # torch.nograd()로 인해서 도함수를 만들지 않는다.
    print(y.requires_grad) # 연산이 적용될 때 requires_grad가 False가 된다.
    print(y)
    
print(x.requires_grad)
# y.backward() # y는 도함수를 만들지 않았기 때문데 Exception발생

```

    True
    True
    False
    tensor(1.)
    True


## gradient 값 초기화


```python
x = torch.tensor(1., requires_grad=True)
y = x**2
y.backward()
print("x의 gradient값:", x.grad )
```

    x의 gradient값: tensor(2.)



```python
x
```




    tensor(1., requires_grad=True)




```python
z = x **2
z.backward()
print("x의 gradient값:", x.grad )
# 기존 x.gradient 값과 새로운 x.gradient 값과 합친다.
```

    x의 gradient값: tensor(4.)



```python
x = torch.tensor(1., requires_grad=True)
y = x**2
y.backward()
x.grad
```




    tensor(2.)




```python
# gradient초기화
x.grad = torch.tensor(0.)   # grad 속성값을 0으로 변경.
z = x**2
z.backward()
x.grad
```




    tensor(2.)


layout: single
title: 'Dataset을 이용 CSV파일에 저장된 데이터셋 로딩'
typora-root-url: ../
categories: Deeplearning_Pytorch.05.DatasetDataLoader
tag: Pytorch
toc: true


- 공통
    - input layer(첫번째): in_features - 입력데이터의 feature(속성)개수에 맞춰준다.
    - hidden layer -> 알아서(감-art). Linear -> feature수를 줄여나간다.(핵심특성들을 추출해 나가는 과정의 개념)
- 회귀
    - output layer의 출력 unit개수(out_features): 정답의개수
        - 집값: 1, (아파트가격, 단독가격, 빌라가격): 3 => y의 개수
    - 출력의 activation: 기본-None, 값의 범위가 설정되 있고 그 범위의 값을 출력하는 함수가 있을경우 사용.
        - 0 ~ 1: logistic(Sigmoid), -1 ~ 1: hyperbolic tangent(Tanh)
    - loss함수: MSELoss
    - 평가지표: MES, RMSE, R_square(R**2) - 평균으로 예측하는 것대비 얼마나 성능이 좋은지에 대한 수치.
- 다중분류
    - output layer의 출력 unit개수 - 정답 class(고유값)의 개수
    - 출력의 activation 함수: 클래스별 확률을 출력 --> Softmax
    - loss함수: categrocial crossentropy
        - 파이토치 함수
            - CrossEntropyLoss == NLLLoss + LogSoftmax
            - NLLLoss => 정답(y)에 One Hot Encoding 처리후 cross entropy 계산
                - Model의 출력 layer에 Softmax를 적용. 데이터셋의 정답이 one hot encoding 처리가 안되있을 경우 학십시 NLLLoss사용
            - LogSoftmax => 모델 추정결과에 Softmax를 처리한 후 cross entropy 계산
                - 학습데이터셋의 정닶(y)가 one hot encoding되 있다. Model출력값에 Softmax가 적용이 안된 경우 Logsoftmax 사용.
- 이진분류
    - output layer의 출력 unit개수 - 1개 (positive일 확률)
    - 출력의 activation함수: Sigmoid(Logistic) 함수
    - loss함수: Binary crossentropy
        - BCELoss
- 분류의 평가지표
    - 정확도(Accuracy) - 맞은것의개수/전체개수
    - 재현율/민감도(Recall) - Positive중에맞은것의개수/Positive의 개수 (이진분류평가지표)
        - 정답이 Positive인것중 맞은 비율
    - 정밀도(Precision) - 모델이 Positive로 예측한것중맞은것의개수/모델이 Positive로예측한전체개수 - (이진분류평가지표)
        - 모델이 Positive라고 예측한 것중 맞은것의 비율

# 모델 저장

- 학습한 모델을 저장장치에 파일로 저장하고 나중에 불러와 사용(추가 학습, 예측 서비스) 할 수 있도록 한다. 
- 파이토치는 모델의 파라미터만 저장하는 방법과 모델 구조와 파라미터 모두를 저장하는 두가지 방식을 제공한다.
- 저장 함수
    - `torch.save(저장할 객체, 저장경로)`
- 보통 저장파일의 확장자는 `pt`나 `pth` 를 지정한다.

## 모델 전체 저장하기 및 불러오기

- 저장하기
    - `torch.save(model, 저장경로)`
- 불러오기
    - `load_model = torch.load(저장경로)`
- 저장시 **pickle**을 이용해 직렬화하기 때문에 불어오는 실행환경에도 **모델을 저장할 때 사용한 클래스**가 있어야 한다.



## 모델의 파라미터만 저장
- 모델을 구성하는 파라미터만 저장한다.
- 모델의 구조는 저장하지 않기 때문에 불러올 때 **모델을 먼저 생성하고 생성한 모델에 불러온 파라미터를 덮어씌운다.**
- 모델의 파라미터는 **state_dict** 형식으로 저장한다.

### state_dict
- 모델의 파라미터 Tensor들을 레이어 단위별로 나누어 저장한 Ordered Dictionary (OrderedDict)
- `모델객체.state_dict()` 메소드를 이용해 조회한다.
- 모델의 state_dict을 조회 후 저장한다.
    - `torch.save(model.state_dict(), "저장경로")`
- 생성된 모델에 읽어온 state_dict를 덮어씌운다.
    - `new_model.load_state_dict(torch.load("state_dict저장경로"))`


# Checkpoint를 저장 및 불러오기
- 학습이 끝나지 않은 모델을 저장 후 나중에 이어서 학습시킬 경우에는 모델의 구조, 파라미터 뿐만 아니라 optimizer, loss 함수등 학습에 필요한 객체들을 저장해야 한다.
- Dictionary에 필요한 요소들을 key-value 쌍으로 저장후 `torch.save()`를 이용해 저장한다.
```python
# 저장
torch.save({
    'epoch':epoch,
    'model_state_dict':model.state_dict(),
    'optimizer_state_dict':optimizer.state_dict(),
    'loss':train_loss
}, "저장경로")

# 불러오기
model = MyModel()
optimizer = optim.Adam(model.parameter())

checkpoint = torch.load("저장경로")
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']

#### 이어학습
model.train()
#### 추론
model.eval()

```


```python
import torch
import torch.nn as nn

import os 
```


```python
class Network(nn.Module):

    def __init__(self):
        super().__init__()
        self.lr = nn.Linear(784, 64)
        self.out = nn.Linear(64, 10)
        self.relu = nn.ReLU()

    def forward(self, X):
        X = torch.flatten(X, start_dim=1)
        X = self.lr(X)
        X = relu(X)
        X = out(X)
        return X
```


```python
sample_model = Network() # 모델생성
```


```python
# 모델 구조 + 파라미터 저장
## torch.save(sample_model, '경로/sample_model.pth')
```


```python
# 모델의 파라미터만 저장 -> model.state_dict(): 파라미터들만 조회
state_dict = sample_model.state_dict()
print(type(state_dict)) # OrderedDict: 순서를 유지하는 Dictionary
state_dict.keys() # 키값들만 조회
## 레이어객체_변수.weight 변수.bias
```

    <class 'collections.OrderedDict'>
    




    odict_keys(['lr.weight', 'lr.bias', 'out.weight', 'out.bias'])




```python
lr_weight = state_dict['lr.weight']
lr_bias = state_dict['lr.bias']
lr_weight.shape, lr_bias.shape
```




    (torch.Size([64, 784]), torch.Size([64]))




```python
out_weight = state_dict['out.weight']
out_bias = state_dict['out.bias']
out_weight.shape, out_bias.shape
```




    (torch.Size([10, 64]), torch.Size([10]))




```python
out_bias
```




    tensor([ 0.0862, -0.0023, -0.0206,  0.0381, -0.0088,  0.1133,  0.0277,  0.0088,
            -0.0298,  0.1043])




```python
# 파라미터 저장
os.makedirs('models/sample', exist_ok=True)
torch.save(state_dict, 'models/sample/sample_state_dict.pth')
```


```python
# 파리미터 로드
load_state_dict = torch.load('models/sample/sample_state_dict.pth')
load_state_dict.keys()
```




    odict_keys(['lr.weight', 'lr.bias', 'out.weight', 'out.bias'])




```python
load_state_dict['out.bias']
```




    tensor([ 0.0862, -0.0023, -0.0206,  0.0381, -0.0088,  0.1133,  0.0277,  0.0088,
            -0.0298,  0.1043])




```python
# 새로 모델객체를 생성 -> 읽어온 파라미터로 변경
new_model = Network()
new_model.load_state_dict(load_state_dict)
```




    <All keys matched successfully>



# 문제 유형별 MLP 네트워크
- MLP(Multi Layer Perceptron)
    - Fully Connected Layer로 구성된 네트워크

# Regression(회귀)
- 예측할 값이 정해져 있지 않는 경우 => 연속형값(실수)을 추론 

## Boston Housing Dataset
보스턴 주택가격 dataset은 다음과 같은 속성을 바탕으로 해당 타운 주택 가격의 중앙값을 예측하는 문제.
- CRIM: 범죄율
- ZN: 25,000 평방피트당 주거지역 비율
- INDUS: 비소매 상업지구 비율
- CHAS: 찰스강에 인접해 있는지 여부(인접:1, 아니면:0)
- NOX: 일산화질소 농도(단위: 0.1ppm)
- RM: 주택당 방의 수
- AGE: 1940년 이전에 건설된 주택의 비율
- DIS: 5개의 보스턴 직업고용센터와의 거리(가중 평균)
- RAD: 고속도로 접근성
- TAX: 재산세율
- PTRATIO: 학생/교사 비율
- B: 흑인 비율
- LSTAT: 하위 계층 비율
<br><br>
- **Target**
    - MEDV: 타운의 주택가격 중앙값(단위: 1,000달러)

## Import


```python
!pip install scikit-learn
```

    Requirement already satisfied: scikit-learn in c:\users\world\anaconda3\envs\torch\lib\site-packages (1.3.1)
    Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\users\world\anaconda3\envs\torch\lib\site-packages (from scikit-learn) (1.26.0)
    Requirement already satisfied: scipy>=1.5.0 in c:\users\world\anaconda3\envs\torch\lib\site-packages (from scikit-learn) (1.11.3)
    Requirement already satisfied: joblib>=1.1.1 in c:\users\world\anaconda3\envs\torch\lib\site-packages (from scikit-learn) (1.3.2)
    Requirement already satisfied: threadpoolctl>=2.0.0 in c:\users\world\anaconda3\envs\torch\lib\site-packages (from scikit-learn) (3.2.0)
    


```python
import torch
import torch.nn as nn
import torchinfo
from torch.utils.data import DataLoader, Dataset, TensorDataset

from sklearn.model_selection import train_test_split # (x, y) = (x1, x2), (y1, y2) 튜플
from sklearn.preprocessing import StandardScaler

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import time
```


```python
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(device)
```

    cpu
    

## Dataset, DataLoader 생성


```python
# trainset, testset
# train Dataloader, test Dataloader
```


```python
boston = pd.read_csv('data/boston_housing.csv')
print(boston.shape)
print(boston.info())
```

    (506, 14)
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 506 entries, 0 to 505
    Data columns (total 14 columns):
     #   Column   Non-Null Count  Dtype  
    ---  ------   --------------  -----  
     0   CRIM     506 non-null    float64
     1   ZN       506 non-null    float64
     2   INDUS    506 non-null    float64
     3   CHAS     506 non-null    float64
     4   NOX      506 non-null    float64
     5   RM       506 non-null    float64
     6   AGE      506 non-null    float64
     7   DIS      506 non-null    float64
     8   RAD      506 non-null    float64
     9   TAX      506 non-null    float64
     10  PTRATIO  506 non-null    float64
     11  B        506 non-null    float64
     12  LSTAT    506 non-null    float64
     13  MEDV     506 non-null    float64
    dtypes: float64(14)
    memory usage: 55.5 KB
    None
    


```python
boston.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00632</td>
      <td>18.0</td>
      <td>2.31</td>
      <td>0.0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1.0</td>
      <td>296.0</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.02731</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.02729</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.03237</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.06905</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>396.90</td>
      <td>5.33</td>
      <td>36.2</td>
    </tr>
  </tbody>
</table>
</div>




```python
# X(input data, feature), y(output data, target, label)을 분리
## datafram/series.values => ndarray로 변환
X_boston = boston.drop(columns='MEDV').values
y_boston = boston['MEDV'].to_frame().values
X_boston.shape, y_boston.shape
```




    ((506, 13), (506, 1))




```python
# Trainset / Testset 분리
## X_boston, y_boston을 섞은뒤 0.8 : 0.2 비율로 나눔
X_train, X_test, y_train, y_test = train_test_split(X_boston, y_boston, test_size=0.2, random_state=0) # random_state: seed값
```


```python
# 회귀 (정답이 연속형-다 다른값) ==> stratify=y 를 설정하지 않는다 (분류는 필수)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
```

    (404, 13) (102, 13) (404, 1) (102, 1)
    


```python
# Feature scaling => 컬럼들의 scaling(척도-단위)를 맞춰주는 작업.
## 단순 값의 크기에 따라 모델링이 되지 않도록 처리.
# StandarScaler => 모든 컬럼의 척도를 평균-0, 표준편차-1 로 맞춤
# Feature scaling은 Train set의 평균과 표준편차를 이용해 Trainset/Testset의 값들에 적용
X_train_mean = X_train.mean(axis=0) # 컬럼별 평균
X_train_std = X_train.std(axis=0)   # 컬럼별 표준편차
X_train_scaled_raw = (X_train - X_train_mean) / X_train_std # (각원소값 - 평균) / 표준편차 => 표준점수(Z score)
```


```python
X_train_scaled_raw.mean(axis=0)
X_train_scaled_raw.std(axis=0)
```




    array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])




```python
# Sklearn을 이용한 Standard Scaling 처리
scaler = StandardScaler()
scaler.fit(X_train) # 어떻게 변환할지 학습 -> 평균/표준편차 계산
X_train_scaled = scaler.transform(X_train) # 변환.
X_test_scaled = scaler.transform(X_test)   # X_train의 평균/표준편차 기준으로 testset도 변환 => 모델의 좀더 정확하게 평가하기 위해
```


```python
X_train_scaled.mean(axis=0)
X_train_scaled.std(axis=0)
```




    array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])




```python
# train set: 모델을 학습 ==> 이전(과거) 데이터를 대표하는 샘플
# test set: 모델을 평가 ==> 앞으로 예측할(미래) 데이터를 대표하는 샘플
```


```python
# X, y: ndarray => torch.Tensor 변환
X_train_scaled = torch.tensor(X_train_scaled, dtype=torch.float32)
X_test_scaled = torch.tensor(X_test_scaled, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32)
```


```python
# Dataset 생성
boston_trainset = TensorDataset(X_train_scaled, y_train)
boston_testset = TensorDataset(X_test_scaled, y_test)
print('Dataset의 데이터개수:','Trainset', len(boston_trainset), 'Testset:',len(boston_testset))
boston_trainset[0]
```

    Dataset의 데이터개수: Trainset 404 Testset: 102
    




    (tensor([-0.3726, -0.4996, -0.7049,  3.6645, -0.4249,  0.9357,  0.6937, -0.4372,
             -0.1622, -0.5617, -0.4846,  0.3717, -0.4110]),
     tensor([26.7000]))




```python
# Dataloader 생성
```


```python
boston_trainloader = DataLoader(boston_trainset, batch_size=200, shuffle=True, drop_last=True)
boston_testloader = DataLoader(boston_testset, batch_size=len(boston_testset))
```


```python
print('epoch당 step수:', len(boston_trainloader), len(boston_testloader))
```

    epoch당 step수: 2 1
    

# 모델 저장

## 모델 전체 저장 및 불러오기
- 모델구조, 파라미터 저장



```python
X_boston.shape
# 200, 13 -> 13 x 32(lr1 layer)
```




    (506, 13)




```python
y_boston.shape
# Target -> MEDV: 예측값 1
```




    (506, 1)




```python
class BostonModel(nn.Module):

    def __init__(self):
        # nn.Module의 __init__() 실행 => 초기화
        super().__init__()
        # forward propagation(예측) 할때 필요한 Layer들 생성

        # 입력 feature: 13, 출력 feature: 32 => weight: 13(weight수) x 32(unit수)
        self.lr1 = nn.Linear(in_features=13, out_features=32)
        self.lr2 = nn.Linear(32, 16)
        # lr3 -> 출력 layer: out feature = 모델이 출력해야할 값의 개수에 맞춰준다.
        self.lr3 = nn.Linear(16, 1) # 집값(중앙값) 하나를 예측해야 하므로 1로 설정
    
    def forward(self, X):  
        out = self.lr1(X)     # 선형
        out = nn.ReLU()(out)  # 비선형
        
        out = self.lr2(out)     # 선형
        out = nn.ReLU()(out)  # 비선형
        
        out = self.lr3(out)     # 출력 레이어(이 값이 모델의 예측값)
        # 회귀의 출력결과에는 Activation 함수를 적용하지 않는다.
        #  예외: 출력값의 범위가 정해져 잇고 그 범위값을 출력하는 함수가 있을경우에는 적용가능
        #    범위: 0 ~ 1 -> Logistic (nn.Sigmoid())
        #         -1 ~ 1 -> tanh (nn.Tanh())
        return out

```


```python
# 모델 생성
boston_model = BostonModel()
# 모델 구조 확인
print(boston_model) # attribute로 설정된 layer들을 확인
```

    BostonModel(
      (lr1): Linear(in_features=13, out_features=32, bias=True)
      (lr2): Linear(in_features=32, out_features=16, bias=True)
      (lr3): Linear(in_features=16, out_features=1, bias=True)
    )
    


```python
# 모델 구조 확인 - 연산 흐름 + output의 shape 등등
## (모델, 입력데이터 shape(batch_size, features))
torchinfo.summary(boston_model, (100,13))
```




    ==========================================================================================
    Layer (type:depth-idx)                   Output Shape              Param #
    ==========================================================================================
    BostonModel                              [100, 1]                  --
    ├─Linear: 1-1                            [100, 32]                 448
    ├─Linear: 1-2                            [100, 16]                 528
    ├─Linear: 1-3                            [100, 1]                  17
    ==========================================================================================
    Total params: 993
    Trainable params: 993
    Non-trainable params: 0
    Total mult-adds (M): 0.10
    ==========================================================================================
    Input size (MB): 0.01
    Forward/backward pass size (MB): 0.04
    Params size (MB): 0.00
    Estimated Total Size (MB): 0.05
    ==========================================================================================



# 학습(Train)
- 학습 + 검증


```python
import time
```


```python
# 하이퍼파라미터 (우리가 설정하는 파리머터) 정의
N_EPOCH = 100
LR = 0.001

# 모델 준비
boston_model = boston_model.to(device) # 모델: 1. 생성 2. divice를 설정
# loss 함수 정의 - 회귀: mse
loss_fn = nn.MSELoss()
# optimizer 정의
optimizer = torch.optim.RMSprop(boston_model.parameters(), lr=LR)
# torch.optim 모델에 최적화알고리즘들이 정의. (모델의 파리미터, 학습률)
```


```python
# 에폭별 학습 결과를 저장할 리스트
## train loss와 validation loss를 저장
train_losslist = []
valid_losslist = []
```


```python
s = time.time()
# Train  (학습/훈련)
## 두단계 -> Train + validation => step 별로 train -> epoch 별로 검증.
for epoch in range(N_EPOCH):
    # 한 epoch에 대한 train 코드
    #####################################
    # train - 모델을 train mode로 변환
    #####################################
    boston_model.train() # train 모드 변경
    train_loss = 0.0 # 현재 epoch의 train loss를 저장할 변수
    ### batch 단위로 학습 => step
    for X, y in boston_trainloader:
        # 한 STEP에 대한 train 코드
        
        # 1. X, y 를 device 옮긴다. => 모델과 동일한 device에 위치시킨다.
        X, y = X.to(device), y.to(device)
        
        # 2. 모델 추정(예측) => forward propagation
        pred = boston_model(X)

        # 3. loss 계산
        loss = loss_fn(pred, y) # 오차계산 -> grad_fn

        # 파라미터 업데이트
        # 4. 파라미터 초기화
        optimizer.zero_grad()
        # 5. back propagation -> 파라미터들의 weight와 파라미터들의 gradient값들을 계산
        loss.backward() # 모든 weight와 bias에 대한 loss의 gradient들을 구한다. - 각 변수에 grad 속성에 저장
    
        # 6. 파리미터 업데이트
        optimizer.step()

        # 7. 현 step의 loss를 train_loss에 누적
        train_loss += loss.item()
    
    # train_loss의 젠체 평균을 계산 (step별 평균loss의 합계 -> step수로 나눠서 전체 평균으로 계산)
    train_loss /= len(boston_trainloader) # step수로 나눈기

    ############################################
    # validation - 모델을 평가(eval) mode로 변경
    #            - 검증, 평가, 서비스 할때
    #            - validation/test dataset으로 모델을 평가
    ############################################
    boston_model.eval() # 평가 모드 변경.
    # 검증 loss를 저장할 변수
    valid_loss = 0.0
    # 검증은 gradient 계산할 필요없음 forward propagation시 도함수 구할 필요없음
    with torch.no_grad():
        for X_valid, y_valid in boston_testloader:
            # 1. X, y device 이동
            X_valid, y_valid = X_valid.to(device), y_valid.to(device)

            # 2. 모델을 이용해 예측
            pred_valid = boston_model(X_valid)

            # 3. 평가 - MSE
            valid_loss += loss_fn(pred_valid, y_valid).item()
            
        # valid_loss 평균
        valid_loss /= len(boston_testloader) # valid_loss / step 수로 나눔
    
    # 현 epoch 에 대한 학습 결과 로그 출력
    print(f"[{epoch+1}/{N_EPOCH}] Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}")
    train_losslist.append(train_loss)
    valid_losslist.append(valid_loss)

e = time.time()
```

    [1/100] Train Loss: 600.1550, Valid Loss: 573.5142
    [2/100] Train Loss: 593.9623, Valid Loss: 567.0342
    [3/100] Train Loss: 575.2606, Valid Loss: 560.3853
    [4/100] Train Loss: 580.3380, Valid Loss: 553.0983
    [5/100] Train Loss: 573.6417, Valid Loss: 545.2942
    [6/100] Train Loss: 564.1140, Valid Loss: 536.8491
    [7/100] Train Loss: 553.2998, Valid Loss: 527.7004
    [8/100] Train Loss: 541.7245, Valid Loss: 517.7957
    [9/100] Train Loss: 531.5670, Valid Loss: 507.1731
    [10/100] Train Loss: 514.9458, Valid Loss: 496.0745
    [11/100] Train Loss: 507.3479, Valid Loss: 484.3022
    [12/100] Train Loss: 493.2969, Valid Loss: 472.0690
    [13/100] Train Loss: 476.8092, Valid Loss: 459.5027
    [14/100] Train Loss: 465.6772, Valid Loss: 446.5804
    [15/100] Train Loss: 451.4950, Valid Loss: 433.3291
    [16/100] Train Loss: 431.5264, Valid Loss: 420.0411
    [17/100] Train Loss: 421.0817, Valid Loss: 406.4973
    [18/100] Train Loss: 407.7126, Valid Loss: 392.8791
    [19/100] Train Loss: 393.8998, Valid Loss: 379.2253
    [20/100] Train Loss: 371.6994, Valid Loss: 365.6301
    [21/100] Train Loss: 361.5108, Valid Loss: 351.9666
    [22/100] Train Loss: 346.4126, Valid Loss: 338.3172
    [23/100] Train Loss: 331.2187, Valid Loss: 324.7607
    [24/100] Train Loss: 316.5856, Valid Loss: 311.2226
    [25/100] Train Loss: 301.8727, Valid Loss: 297.6621
    [26/100] Train Loss: 286.5567, Valid Loss: 284.0104
    [27/100] Train Loss: 271.9386, Valid Loss: 270.5645
    [28/100] Train Loss: 256.5018, Valid Loss: 257.6515
    [29/100] Train Loss: 241.2394, Valid Loss: 245.3272
    [30/100] Train Loss: 228.8894, Valid Loss: 233.4340
    [31/100] Train Loss: 213.0430, Valid Loss: 222.2762
    [32/100] Train Loss: 203.7046, Valid Loss: 211.6454
    [33/100] Train Loss: 192.2370, Valid Loss: 201.6129
    [34/100] Train Loss: 180.9327, Valid Loss: 192.1906
    [35/100] Train Loss: 171.0233, Valid Loss: 183.3995
    [36/100] Train Loss: 160.3613, Valid Loss: 175.2178
    [37/100] Train Loss: 153.2592, Valid Loss: 167.5822
    [38/100] Train Loss: 143.9723, Valid Loss: 160.5292
    [39/100] Train Loss: 138.0382, Valid Loss: 153.8964
    [40/100] Train Loss: 128.9126, Valid Loss: 147.8072
    [41/100] Train Loss: 122.6429, Valid Loss: 142.1461
    [42/100] Train Loss: 116.6866, Valid Loss: 136.8901
    [43/100] Train Loss: 111.7724, Valid Loss: 131.9608
    [44/100] Train Loss: 106.7711, Valid Loss: 127.3790
    [45/100] Train Loss: 102.5910, Valid Loss: 123.1281
    [46/100] Train Loss: 98.4524, Valid Loss: 119.1372
    [47/100] Train Loss: 93.4276, Valid Loss: 115.4497
    [48/100] Train Loss: 89.1211, Valid Loss: 112.0199
    [49/100] Train Loss: 87.2501, Valid Loss: 108.7361
    [50/100] Train Loss: 83.6961, Valid Loss: 105.6922
    [51/100] Train Loss: 80.2742, Valid Loss: 102.8564
    [52/100] Train Loss: 77.3801, Valid Loss: 100.2052
    [53/100] Train Loss: 75.2953, Valid Loss: 97.6860
    [54/100] Train Loss: 72.1153, Valid Loss: 95.3285
    [55/100] Train Loss: 70.1475, Valid Loss: 93.0888
    [56/100] Train Loss: 67.3530, Valid Loss: 90.9881
    [57/100] Train Loss: 65.4418, Valid Loss: 89.0203
    [58/100] Train Loss: 64.3030, Valid Loss: 87.1254
    [59/100] Train Loss: 62.0888, Valid Loss: 85.3297
    [60/100] Train Loss: 60.0461, Valid Loss: 83.6141
    [61/100] Train Loss: 58.7016, Valid Loss: 82.0145
    [62/100] Train Loss: 56.8409, Valid Loss: 80.4684
    [63/100] Train Loss: 54.9972, Valid Loss: 79.0282
    [64/100] Train Loss: 53.7664, Valid Loss: 77.6330
    [65/100] Train Loss: 52.5459, Valid Loss: 76.3065
    [66/100] Train Loss: 51.4672, Valid Loss: 75.0644
    [67/100] Train Loss: 50.3485, Valid Loss: 73.8415
    [68/100] Train Loss: 49.0947, Valid Loss: 72.6996
    [69/100] Train Loss: 47.5436, Valid Loss: 71.6079
    [70/100] Train Loss: 46.8053, Valid Loss: 70.5447
    [71/100] Train Loss: 45.9745, Valid Loss: 69.5395
    [72/100] Train Loss: 44.3536, Valid Loss: 68.5785
    [73/100] Train Loss: 43.8245, Valid Loss: 67.6423
    [74/100] Train Loss: 40.0232, Valid Loss: 66.8605
    [75/100] Train Loss: 42.0042, Valid Loss: 66.0171
    [76/100] Train Loss: 41.6055, Valid Loss: 65.2004
    [77/100] Train Loss: 40.6308, Valid Loss: 64.3939
    [78/100] Train Loss: 39.9125, Valid Loss: 63.6505
    [79/100] Train Loss: 39.0317, Valid Loss: 62.9366
    [80/100] Train Loss: 37.9607, Valid Loss: 62.2538
    [81/100] Train Loss: 37.7710, Valid Loss: 61.5603
    [82/100] Train Loss: 37.1612, Valid Loss: 60.8965
    [83/100] Train Loss: 36.4835, Valid Loss: 60.2821
    [84/100] Train Loss: 35.9584, Valid Loss: 59.6887
    [85/100] Train Loss: 35.4031, Valid Loss: 59.1032
    [86/100] Train Loss: 34.9103, Valid Loss: 58.5452
    [87/100] Train Loss: 34.3117, Valid Loss: 57.9938
    [88/100] Train Loss: 33.8069, Valid Loss: 57.4671
    [89/100] Train Loss: 32.9687, Valid Loss: 56.9693
    [90/100] Train Loss: 32.7487, Valid Loss: 56.5209
    [91/100] Train Loss: 32.0507, Valid Loss: 56.0449
    [92/100] Train Loss: 31.9455, Valid Loss: 55.5677
    [93/100] Train Loss: 31.3038, Valid Loss: 55.1314
    [94/100] Train Loss: 31.0673, Valid Loss: 54.7330
    [95/100] Train Loss: 30.4065, Valid Loss: 54.3093
    [96/100] Train Loss: 30.2115, Valid Loss: 53.8882
    [97/100] Train Loss: 30.0226, Valid Loss: 53.4680
    [98/100] Train Loss: 29.5780, Valid Loss: 53.0947
    [99/100] Train Loss: 29.2717, Valid Loss: 52.7552
    [100/100] Train Loss: 28.8061, Valid Loss: 52.3957
    


```python
# 회귀 -> loss: mse 평가: mse, rmse (Root Mean squared error)
print('걸린시간:', (e-s), '초')
```

    걸린시간: 0.8266639709472656 초
    


```python
# train loss, valid loss 의 epoch 별 변화의 흐름 시각화.
plt.plot(range(1, N_EPOCH+1), train_losslist, label='Train Loss')
plt.plot(range(1, N_EPOCH+1), valid_losslist, label='Validation Loss')

plt.xlabel('EPOCH')
plt.ylabel('Loss')
plt.legend()
plt.show()
```


    
![png](output_51_0.png)
    



```python
train_losslist[-1] ** (1/2), valid_losslist[-1]**(1/2)
```




    (5.367135437736487, 7.238484939204328)



## state_dict 저장 및 로딩
- 모델 파라미터만 저장


```python
save_path = 'models/boston_model.pth'
torch.save(boston_model, save_path)
```


```python
load_boston_model = torch.load(save_path)
torchinfo.summary(load_boston_model, (200, 13))
```




    ==========================================================================================
    Layer (type:depth-idx)                   Output Shape              Param #
    ==========================================================================================
    BostonModel                              [200, 1]                  --
    ├─Linear: 1-1                            [200, 32]                 448
    ├─Linear: 1-2                            [200, 16]                 528
    ├─Linear: 1-3                            [200, 1]                  17
    ==========================================================================================
    Total params: 993
    Trainable params: 993
    Non-trainable params: 0
    Total mult-adds (M): 0.20
    ==========================================================================================
    Input size (MB): 0.01
    Forward/backward pass size (MB): 0.08
    Params size (MB): 0.00
    Estimated Total Size (MB): 0.09
    ==========================================================================================




```python
# 예측 서비스
new_X = torch.cat([boston_testset[0][0], boston_testset[1][0]])
new_X = new_X.reshape(-1, 13)
new_X.shape # [2: 데이터수, 13: frature수]
```




    torch.Size([2, 13])




```python
boston_testset[0][0] # 첫번째 데이터 => X, y
boston_testset[1][0]
torch.cat([boston_testset[0][0], boston_testset[1][0]]).reshape(-1, 13)
```




    tensor([[-0.4084, -0.4996, -1.1287, -0.2729, -0.8334,  0.0450, -1.8462,  0.6951,
             -0.6246,  0.1591, -0.7127,  0.1855, -0.7361],
            [ 0.7193, -0.4996,  0.9989, -0.2729,  0.6528, -0.1237,  1.1033, -1.2517,
              1.6874,  1.5421,  0.7927,  0.0832, -0.4357]])




```python
# 예측값
pred_new = load_boston_model(new_X)
print(pred_new) 
```

    tensor([[24.8984],
            [17.9753]], grad_fn=<AddmmBackward0>)
    


```python
# 정답
boston_testset[0][1], boston_testset[1][1]
```




    (tensor([22.6000]), tensor([50.]))




```python
# 오차
loss_fn(pred_new[0], boston_testset[0][1])
```




    tensor(5.2826, grad_fn=<MseLossBackward0>)




```python
loss_fn(pred_new[1], boston_testset[1][1])
```




    tensor(1025.5785, grad_fn=<MseLossBackward0>)



# state_dict 저장 및 로딩
- 모델 파라미터만 저장


```python
save_path2 = 'models/boston_model_statedict.pth'
# 모델에서 state_dict 조회
model_sd = boston_model.state_dict()
torch.save(model_sd, save_path2)
```


```python
# type(model_sd), model_sd.keys()
# model_sd['lr1.weight'].shape
# model_sd['lr1.weight'][0]
```


```python
# 불러오기 
## state_dict를 불러오기
load_sd = torch.load(save_path2)
type(load_sd)
```




    collections.OrderedDict




```python
# 새로운 모델을 생성한 뒤에 파리미터를 변경
new_model = BostonModel()
new_model.load_state_dict(load_sd)
```




    <All keys matched successfully>




```python
pred_new2 = new_model(new_X)
pred_new2
```




    tensor([[24.8984],
            [17.9753]], grad_fn=<AddmmBackward0>)



# 분류 (Classification)
- 예측할 값이 정해져 잇는 경우 => 범주형인 경우
- 다중분류
    - 범주값(class)가 여러인 경우
- 이진 분류
    - 범주값: 0/1 => 맞는지 틀린지를 추정문제.
    - 맞는것: Posivitve -> 1
    - 틀린것: Negative -> 0

## Fashion MNIST Dataset - 다중분류(Multi-Class Classification) 문제

10개의 범주(category)와 70,000개의 흑백 이미지로 구성된 [패션 MNIST](https://github.com/zalandoresearch/fashion-mnist) 데이터셋. 
이미지는 해상도(28x28 픽셀)가 낮고 다음처럼 개별 의류 품목을 나타낸다:

<table>
  <tr><td>
    <img src="https://tensorflow.org/images/fashion-mnist-sprite.png"
         alt="Fashion MNIST sprite"  width="600">
  </td></tr>
  <tr><td align="center">
    <b>그림</b> <a href="https://github.com/zalandoresearch/fashion-mnist">패션-MNIST 샘플</a> (Zalando, MIT License).<br/>&nbsp;
  </td></tr>
</table>

이미지는 28x28 크기이며 Gray scale이다. *레이블*(label)은 0에서 9까지의 정수 배열이다. 아래 표는 이미지에 있는 의류의 **클래스**(class)들이다.

<table>
  <tr>
    <th>레이블</th>
    <th>클래스</th>
  </tr>
  <tr>
    <td>0</td>
    <td>T-shirt/top</td>
  </tr>
  <tr>
    <td>1</td>
    <td>Trousers</td>
  </tr>
    <tr>
    <td>2</td>
    <td>Pullover</td>
  </tr>
    <tr>
    <td>3</td>
    <td>Dress</td>
  </tr>
    <tr>
    <td>4</td>
    <td>Coat</td>
  </tr>
    <tr>
    <td>5</td>
    <td>Sandal</td>
  </tr>
    <tr>
    <td>6</td>
    <td>Shirt</td>
  </tr>
    <tr>
    <td>7</td>
    <td>Sneaker</td>
  </tr>
    <tr>
    <td>8</td>
    <td>Bag</td>
  </tr>
    <tr>
    <td>9</td>
    <td>Ankle boot</td>
  </tr>
</table>


```python
import torch
import torch.nn as nn
import torchinfo
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

import numpy as np
import matplotlib.pyplot as plt

import time
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(device)
```

    cpu
    


```python
# Dataset, DataLoader 생성
## Built in Dataset
fmnist_trainset = datasets.FashionMNIST(root='datasets', train=True, download=True, transform=transforms.ToTensor())
fmnist_testset = datasets.FashionMNIST(root='datasets', train=False, download=True, transform=transforms.ToTensor())
```


```python
# 데이터 개수
print('train 개수:', len(fmnist_trainset))
print('test 개수:', len(fmnist_testset))
```

    train 개수: 60000
    test 개수: 10000
    


```python
print(fmnist_trainset)
print()
print(fmnist_testset)
```

    Dataset FashionMNIST
        Number of datapoints: 60000
        Root location: datasets
        Split: Train
        StandardTransform
    Transform: ToTensor()
    
    Dataset FashionMNIST
        Number of datapoints: 10000
        Root location: datasets
        Split: Test
        StandardTransform
    Transform: ToTensor()
    


```python
# index to class
index_to_class = np.array(fmnist_trainset.classes) # fany indexing을 위해서 list->ndarray
index_to_class[[1, 1, 2, 3, 0]]
```




    array(['Trouser', 'Trouser', 'Pullover', 'Dress', 'T-shirt/top'],
          dtype='<U11')




```python
# class to index
class_to_index = fmnist_trainset.class_to_idx
class_to_index
```




    {'T-shirt/top': 0,
     'Trouser': 1,
     'Pullover': 2,
     'Dress': 3,
     'Coat': 4,
     'Sandal': 5,
     'Shirt': 6,
     'Sneaker': 7,
     'Bag': 8,
     'Ankle boot': 9}




```python
# 이미지 확인
idx = 0 
x, y = fmnist_trainset[idx] # Dataset[i]: (X, y)
plt.imshow(x[0], cmap='gray') # Greys: 흑백 반전
plt.title(index_to_class[y])
plt.show()
```


    
![png](output_77_0.png)
    



```python
# x.shape: (channel: 1, height, weidth)
x.shape
```




    torch.Size([1, 28, 28])




```python
y
```




    9




```python
index_to_class.shape
```




    (10,)




```python
# DataLoader
fmnist_trainloader = DataLoader(fmnist_trainset, batch_size=128, shuffle=True, drop_last=True)
fmnist_testloader = DataLoader(fmnist_testset, batch_size=128)
```


```python
# 1 Epoch당 Step 수
f'1 EPOCH당 Step수 Trainset: {len(fmnist_trainloader)}, Testsetset: {len(fmnist_testloader)}'
```




    '1 EPOCH당 Step수 Trainset: 468, Testsetset: 79'




```python
# 모델 정의
class FashionMNISTModel(nn.Module):

    def __init__(self):
        super().__init__()
        # 입력 이미지를 받아서 처리후 리턴
        ## 이미지는 pixcel 크기때문에 처음 특징은 크게 추출 -> 점점 줄여나간다.
        self.lr1 = nn.Linear(28*28, 2048) # 784 -> 2048 특징추출
        self.lr2 = nn.Linear(2048, 1024)  # 2048 -> 1024
        self.lr3 = nn.Linear(1024, 512)   # 1024 -> 512
        self.lr4 = nn.Linear(512, 256)    # 512 -> 256
        self.lr5 = nn.Linear(256, 128)    # 256 -> 128
        self.lr6 = nn.Linear(128, 64)     # 128 -> 64
        # output - out_features: 다중분류: class 개수 (fashion mnist: 10)
        self.lr7 = nn.Linear(64, 10) # 각 클래스별 확률이 출력되도록 한다.
    
    def forward(self, X):
        # X: (bach, channel. height, weidth) ===> (batch, all_features)
        # out = torch.flatten(X, start_dim=1)
        out = nn.Flatten()(X)
        
        out = nn.ReLU()(self.lr1(out))
        out = nn.ReLU()(self.lr2(out))
        out = nn.ReLU()(self.lr3(out))
        out = nn.ReLU()(self.lr4(out))
        out = nn.ReLU()(self.lr5(out))
        out = nn.ReLU()(self.lr6(out))
        # output
        ## CrossEntropyLoss 내부적으로 SoftMax(확률적으로 바꿈)
        out = self.lr7(out)
        return out

```


```python
# 모델 생성
f_model = FashionMNISTModel()
print(f_model)
```

    FashionMNISTModel(
      (lr1): Linear(in_features=784, out_features=2048, bias=True)
      (lr2): Linear(in_features=2048, out_features=1024, bias=True)
      (lr3): Linear(in_features=1024, out_features=512, bias=True)
      (lr4): Linear(in_features=512, out_features=256, bias=True)
      (lr5): Linear(in_features=256, out_features=128, bias=True)
      (lr6): Linear(in_features=128, out_features=64, bias=True)
      (lr7): Linear(in_features=64, out_features=10, bias=True)
    )
    


```python
# (모델, (데이터개수, channel, height, weith))
torchinfo.summary(f_model, (128, 1, 28, 28))
```




    ==========================================================================================
    Layer (type:depth-idx)                   Output Shape              Param #
    ==========================================================================================
    FashionMNISTModel                        [128, 10]                 --
    ├─Linear: 1-1                            [128, 2048]               1,607,680
    ├─Linear: 1-2                            [128, 1024]               2,098,176
    ├─Linear: 1-3                            [128, 512]                524,800
    ├─Linear: 1-4                            [128, 256]                131,328
    ├─Linear: 1-5                            [128, 128]                32,896
    ├─Linear: 1-6                            [128, 64]                 8,256
    ├─Linear: 1-7                            [128, 10]                 650
    ==========================================================================================
    Total params: 4,403,786
    Trainable params: 4,403,786
    Non-trainable params: 0
    Total mult-adds (M): 563.68
    ==========================================================================================
    Input size (MB): 0.40
    Forward/backward pass size (MB): 4.14
    Params size (MB): 17.62
    Estimated Total Size (MB): 22.16
    ==========================================================================================




```python
# 모델 추정결과 형태를 확인
i = torch.ones((2, 1, 28, 28)) # 1 x 28 x 28 이미지 2장
y_hat = f_model(i)
y_hat[0] # 첫번째 이미지에 대한 추론결화
```




    tensor([ 0.0014, -0.0345, -0.0687, -0.0651,  0.1043,  0.0071, -0.0625, -0.0976,
             0.1103,  0.1291], grad_fn=<SelectBackward0>)




```python
y_hat[0].shape
# 정답 class => 예측결과 10중에서 가장 큰값이 있는 index
y_hat.argmax(axis=-1) # 첫번째: 3, 두번째: 3
# index_to_class[y_hat.detach().numpy().argmax(axis=-1)]
# requires_grad=True 인 Tensor를 ndarray를 변환할때는 tensor.detach() 를 한 다음 변환해야한다.
```




    tensor([9, 9])




```python
torch.sum(y_hat.argmax())
```




    tensor(9)




```python
index_to_class
```




    array(['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal',
           'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], dtype='<U11')




```python
# y_hat => 확률값으로 변환 ==> softmax()
y_hat_probabillity = nn.Softmax(dim=-1)(y_hat)
y_hat_probabillity
# 정답의 확률, 정답라벨
y_hat_probabillity.max(dim=-1).values, y_hat_probabillity.argmax(dim=-1)
```




    (tensor([0.1132, 0.1132], grad_fn=<MaxBackward0>), tensor([9, 9]))




```python
a, b = next(iter(fmnist_trainloader))
b
```




    tensor([9, 4, 9, 9, 1, 5, 4, 7, 8, 8, 8, 9, 5, 4, 2, 8, 1, 7, 6, 3, 3, 1, 3, 2,
            9, 0, 5, 7, 1, 3, 2, 4, 1, 0, 5, 5, 9, 0, 3, 8, 8, 2, 7, 3, 1, 7, 7, 9,
            2, 9, 5, 8, 0, 0, 5, 7, 4, 6, 7, 8, 7, 6, 7, 5, 2, 0, 9, 8, 4, 3, 4, 3,
            5, 8, 2, 9, 7, 3, 8, 0, 4, 5, 7, 3, 5, 8, 1, 8, 6, 5, 9, 5, 3, 6, 3, 2,
            3, 9, 6, 9, 1, 5, 7, 9, 4, 9, 5, 8, 4, 9, 6, 7, 4, 8, 3, 4, 2, 1, 6, 2,
            6, 9, 4, 6, 0, 5, 4, 0])




```python
c = f_model(a.to(device))
```


```python
c.shape
```




    torch.Size([128, 10])




```python
d = c.argmax(-1)
```


```python
torch.sum(b == d)/128
```




    tensor(0.1328)




```python
# 학습 (Train)
# 하이퍼파리미터
LR = 0.001
N_EPOCH = 20

# 모델 -> device 옮김
f_model = f_model.to(device)
# loss fn -> 다중분류: nn.CrossEntropyLoss() ==> 다중분류용 Log Loss
loss_fn = nn.CrossEntropyLoss()
# optimizer
optimizer = torch.optim.Adam(f_model.parameters(), lr=LR)
```


```python
# train
import time
# 각 에폭별 학습이 끝나고 모델 평가한 값을 저장.
train_loss_list = []
valid_loss_list = []
valid_acc_list = [] # test set의 정확도 검증 결과 => 전체데이터 중 맞은데이터의 개수
```


```python
s = time.time()
for epoch in range(N_EPOCH):
    ############### train
    f_model.train()
    train_loss = 0.0 # 현재 eopch의 trainset의 loss

s = time.time()
for epoch in range(N_EPOCH):
    ############### train
    f_model.train()
    train_loss = 0.0 # 현재 eopch의 trainset의 loss

    for x, y in fmnist_trainloader:
        x, y = x.to(device), y.to(device) # device 옮기기
        pred = f_model(x) # 예측 - 순전파
        loss = loss_fn(pred, y) # loss 계산 -> (예측값, 정답)
        # 모델 파라미터 업데이트
        optimizer.zero_grad()     # gradient 초기화       
        loss.backward()           # grad 계산 - (오차) 역전파
        optimizer.step()          # 파라미터 업데이트
        train_loss += loss.item() # train loss 누적
    # 1 에폭 학습 종료 => train loss의 평균을 list에 저장
    train_loss /= len(fmnist_trainloader)  # 누적 train loss / step 수
    train_loss_list.append(train_loss)
    
    ############### validation
    f_model.eval()
    valid_loss = 0.0 # 현재 epoch의 대한 검증의 validation loss 저장 변수
    valid_acc = 0.0  # 현재 epoch의 대한 검증의 validation accuracy(정확도) 저장 변수
    
    with torch.no_grad(): # no_grad: 도함수 구할 필요없음
        for x_valid, y_valid in fmnist_testloader:
            x_valid, y_valid = x_valid.to(device), y_valid.to(device)
            #  예측
            pred_valid = f_model(x_valid)          # 라벨별 정답일 가능성 출력 (batch, 10)
            pred_label = pred_valid.argmax(dim=-1) # 정답 class 조회 (pred_valid에서 가장 큰값을 가진 index)
            # 평가 
            ## loss 계산
            loss_valid = loss_fn(pred_valid, y_valid) ## loss_fn() batch만큼 평균을 계산.
            valid_loss += loss_valid
            ## 정확도(accuarcy) 계산
            valid_acc += torch.sum(pred_label == y_valid).item() 
        
        valid_loss /= len(fmnist_testloader)        # step 수로 나눠서 평균 계산
        valid_acc /= len(fmnist_testloader.dataset) # test set의 총 데이터 개수로 나눔
        # 1 eopch에 대한 vudrk dhksfy => valid_loss_list, valid_acc_list 추가
        valid_loss_list.append(valid_loss)
        valid_acc_list.append(valid_acc)
    
    print(f"[{epoch+1}/{N_EPOCH}] Train Loss: {train_loss}, Valid Loss: {valid_loss}, Valid Accuracy: {valid_acc}")
    
e = time.time()
```

    [1/20] Train Loss: 0.6551105150172853, Valid Loss: 0.4565485715866089, Valid Accuracy: 0.8296
    [2/20] Train Loss: 0.3961887560530096, Valid Loss: 0.4160023033618927, Valid Accuracy: 0.8532
    [3/20] Train Loss: 0.3492731497519546, Valid Loss: 0.3938992917537689, Valid Accuracy: 0.8619
    [4/20] Train Loss: 0.3219013679295014, Valid Loss: 0.3694998323917389, Valid Accuracy: 0.8675
    [5/20] Train Loss: 0.2991755286979879, Valid Loss: 0.3386436998844147, Valid Accuracy: 0.8803
    [6/20] Train Loss: 0.28720097568554753, Valid Loss: 0.33928459882736206, Valid Accuracy: 0.8767
    [7/20] Train Loss: 0.2727778639930945, Valid Loss: 0.3285277783870697, Valid Accuracy: 0.8803
    [8/20] Train Loss: 0.25799507870633376, Valid Loss: 0.33903950452804565, Valid Accuracy: 0.8824
    [9/20] Train Loss: 0.24604842200493202, Valid Loss: 0.3337225019931793, Valid Accuracy: 0.8845
    [10/20] Train Loss: 0.2406256765477423, Valid Loss: 0.332624226808548, Valid Accuracy: 0.8871
    [11/20] Train Loss: 0.22591356936300921, Valid Loss: 0.3361697494983673, Valid Accuracy: 0.8854
    [12/20] Train Loss: 0.2181090948641555, Valid Loss: 0.334783673286438, Valid Accuracy: 0.8859
    [13/20] Train Loss: 0.21327709583326793, Valid Loss: 0.3345346450805664, Valid Accuracy: 0.8935
    [14/20] Train Loss: 0.2110095481491751, Valid Loss: 0.3284386992454529, Valid Accuracy: 0.8906
    [15/20] Train Loss: 0.19529994620153537, Valid Loss: 0.3217955529689789, Valid Accuracy: 0.8901
    [16/20] Train Loss: 0.18719645833166745, Valid Loss: 0.3277837634086609, Valid Accuracy: 0.8911
    [17/20] Train Loss: 0.19030732074036047, Valid Loss: 0.3671373426914215, Valid Accuracy: 0.8933
    [18/20] Train Loss: 0.17740353840029138, Valid Loss: 0.3328641355037689, Valid Accuracy: 0.8921
    [19/20] Train Loss: 0.17015816423341504, Valid Loss: 0.33826160430908203, Valid Accuracy: 0.8952
    [20/20] Train Loss: 0.1667192253148836, Valid Loss: 0.3553315997123718, Valid Accuracy: 0.8972
    


```python
valid_loss_list = [v.item() for v in valid_loss_list]
valid_loss_list
```




    [0.4565485715866089,
     0.4160023033618927,
     0.3938992917537689,
     0.3694998323917389,
     0.3386436998844147,
     0.33928459882736206,
     0.3285277783870697,
     0.33903950452804565,
     0.3337225019931793,
     0.332624226808548,
     0.3361697494983673,
     0.334783673286438,
     0.3345346450805664,
     0.3284386992454529,
     0.3217955529689789,
     0.3277837634086609,
     0.3671373426914215,
     0.3328641355037689,
     0.33826160430908203,
     0.3553315997123718]




```python
print(f'1 EPOCH당 걸린시간: {e - s}')
```

    1 EPOCH당 걸린시간: 822.748521566391
    


```python
# 결과 시각화
plt.rcParams['font.family'] = 'Malgun gothic'

plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.plot(train_loss_list, label='train')
plt.plot(valid_loss_list, label='Validation')
plt.title('Epoch별 Loss 변화')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(valid_acc_list)
plt.title('Validetion Accuracy')
plt.tight_layout()
plt.show()
```


    
![png](output_101_0.png)
    



```python
##############################################################################
# 모델 학습이 진행되면 어느 시점부터 성능이 떨어지기 시작
#                    (trainset으로 검증한 결과는 계속 좋아지는데 validation set으로 검증한 결과는 성능이 좋아지다 안좋아지다.)
# 1. 학습도중 성능 개선될 떄마다 저장. (가장 좋은 성능의 모델을 서비스 할수 있게한다.)
# 2. 더이상 성능개선이 안되면 학습을 중지(조기종료)
##############################################################################
```


```python
# 학습 (Train)
# 하이퍼파리미터
LR = 0.001
N_EPOCH = 1000

# 모델 -> device 옮김
f_model = FashionMNISTModel()
f_model = f_model.to(device)
# loss fn -> 다중분류: nn.CrossEntropyLoss() ==> 다중분류용 Log Loss
loss_fn = nn.CrossEntropyLoss()
# optimizer
optimizer = torch.optim.Adam(f_model.parameters(), lr=LR)

######################################
# 조기 종료 + 모델 저장을 위한 변수 추가
######################################
####### 모델 저장을 위한 변수 저장
## 학습 중 가장 좋은 성능 평가지표를 저장. 현 EPOCH의 지표가 이 변수값보다 좋으면 저장
## 평가지표: validation loss
best_score = torch.inf 
save_model_path = 'models/fashion_mnist_best_model.pth'

####### 조기 종료를 위한 변수: 특정 eopch동안 성능 개선이 없으면 학습을 중단
patience = 5 # 성능이 개선 될지를 기달릴 eopch 수. patience 번 만큼 개선이 안되면 중단.(보통 10이상 지정)
trigger_cnt = 0 # 성능 개선을 몇번 째 기다리는 지 정할 변수, patience == trigger_cnt: 중단

s = time.time()
for epoch in range(N_EPOCH):
    ############### train
    f_model.train()
    train_loss = 0.0 # 현재 eopch의 trainset의 loss

    for x, y in fmnist_trainloader:
        x, y = x.to(device), y.to(device) # device 옮기기
        pred = f_model(x) # 예측 - 순전파
        loss = loss_fn(pred, y) # loss 계산 -> (예측값, 정답)
        # 모델 파라미터 업데이트
        optimizer.zero_grad()     # gradient 초기화       
        loss.backward()           # grad 계산 - (오차) 역전파
        optimizer.step()          # 파라미터 업데이트
        train_loss += loss.item() # train loss 누적
    # 1 에폭 학습 종료 => train loss의 평균을 list에 저장
    train_loss /= len(fmnist_trainloader)  # 누적 train loss / step 수
    train_loss_list.append(train_loss)
    
    ############### validation
    f_model.eval()
    valid_loss = 0.0 # 현재 epoch의 대한 검증의 validation loss 저장 변수
    valid_acc = 0.0  # 현재 epoch의 대한 검증의 validation accuracy(정확도) 저장 변수
    
    with torch.no_grad(): # no_grad: 도함수 구할 필요없음
        for x_valid, y_valid in fmnist_testloader:
            x_valid, y_valid = x_valid.to(device), y_valid.to(device)
            #  예측
            pred_valid = f_model(x_valid)          # 라벨별 정답일 가능성 출력 (batch, 10)
            pred_label = pred_valid.argmax(dim=-1) # 정답 class 조회 (pred_valid에서 가장 큰값을 가진 index)
            # 평가 
            ## loss 계산
            loss_valid = loss_fn(pred_valid, y_valid) ## loss_fn() batch만큼 평균을 계산.
            valid_loss += loss_valid
            ## 정확도(accuarcy) 계산
            valid_acc += torch.sum(pred_label == y_valid).item() 
        
        valid_loss /= len(fmnist_testloader)        # step 수로 나눠서 평균 계산
        valid_acc /= len(fmnist_testloader.dataset) # test set의 총 데이터 개수로 나눔
        # 1 eopch에 대한 vudrk dhksfy => valid_loss_list, valid_acc_list 추가
        valid_loss_list.append(valid_loss)
        valid_acc_list.append(valid_acc)
    
    print(f"[{epoch+1}/{N_EPOCH}] Train Loss: {train_loss}, Valid Loss: {valid_loss}, Valid Accuracy: {valid_acc}")
    


    ###############################
    # 조기종료여부, 모델 저장 처리
    #   저장: 현 epoch valid_loss 가 best_score보다 개성도니 경우 저장(작으면 개선)
    ###############################
    if valid_loss < best_score: # 성능이 개선된 경우.
        # 저장로그 출력
        print(f'======> 모델저장: {epoch+1} Epoch - 이전 valid_loss: {best_score}, 현재 valid_loss: {valid_loss}')
        # best_score 교체
        best_score = valid_loss
        # 저장
        torch.save(f_model, save_model_path)
        # trigger_cnt 0으로 초기화
        trigger_cnt = 0
    else: # 성능개선이 안된경우.
        # trigger_cnt 1 증가
        trigger_cnt += 1 
        if patience == trigger_cnt: # patience 만큼 대기 ==> 조기 종료
            # 로그
            print(f'======> {epoch+1} Epoch에서 조기종료-{best_score}에서 개선 안됨')
            break
            
e = time.time()
```

    [1/1000] Train Loss: 0.6529126517538332, Valid Loss: 0.4727691411972046, Valid Accuracy: 0.8294
    ======> 모델저장: 1 Epoch - 이전 valid_loss: inf, 현재 valid_loss: 0.4727691411972046
    [2/1000] Train Loss: 0.40235120112187844, Valid Loss: 0.3901809751987457, Valid Accuracy: 0.8602
    ======> 모델저장: 2 Epoch - 이전 valid_loss: 0.4727691411972046, 현재 valid_loss: 0.3901809751987457
    [3/1000] Train Loss: 0.35432691598295146, Valid Loss: 0.3878174126148224, Valid Accuracy: 0.861
    ======> 모델저장: 3 Epoch - 이전 valid_loss: 0.3901809751987457, 현재 valid_loss: 0.3878174126148224
    [4/1000] Train Loss: 0.32579300019285107, Valid Loss: 0.3973783254623413, Valid Accuracy: 0.8629
    [5/1000] Train Loss: 0.30213577962583965, Valid Loss: 0.362692654132843, Valid Accuracy: 0.869
    ======> 모델저장: 5 Epoch - 이전 valid_loss: 0.3878174126148224, 현재 valid_loss: 0.362692654132843
    [6/1000] Train Loss: 0.2870442514331677, Valid Loss: 0.34652215242385864, Valid Accuracy: 0.8784
    ======> 모델저장: 6 Epoch - 이전 valid_loss: 0.362692654132843, 현재 valid_loss: 0.34652215242385864
    [7/1000] Train Loss: 0.2739411695327005, Valid Loss: 0.3382107615470886, Valid Accuracy: 0.8835
    ======> 모델저장: 7 Epoch - 이전 valid_loss: 0.34652215242385864, 현재 valid_loss: 0.3382107615470886
    [8/1000] Train Loss: 0.2633117822309335, Valid Loss: 0.32794469594955444, Valid Accuracy: 0.882
    ======> 모델저장: 8 Epoch - 이전 valid_loss: 0.3382107615470886, 현재 valid_loss: 0.32794469594955444
    [9/1000] Train Loss: 0.2518994159932829, Valid Loss: 0.3353148102760315, Valid Accuracy: 0.8784
    [10/1000] Train Loss: 0.23975782543739194, Valid Loss: 0.3369169235229492, Valid Accuracy: 0.8855
    [11/1000] Train Loss: 0.22965459651353523, Valid Loss: 0.35423359274864197, Valid Accuracy: 0.8825
    [12/1000] Train Loss: 0.22424998952664882, Valid Loss: 0.32045993208885193, Valid Accuracy: 0.8918
    ======> 모델저장: 12 Epoch - 이전 valid_loss: 0.32794469594955444, 현재 valid_loss: 0.32045993208885193
    [13/1000] Train Loss: 0.21439947546101534, Valid Loss: 0.34093621373176575, Valid Accuracy: 0.8922
    [14/1000] Train Loss: 0.20707031858400402, Valid Loss: 0.3220938444137573, Valid Accuracy: 0.89
    [15/1000] Train Loss: 0.19995766029589707, Valid Loss: 0.3759728670120239, Valid Accuracy: 0.8833
    [16/1000] Train Loss: 0.19504552721404111, Valid Loss: 0.32605788111686707, Valid Accuracy: 0.8944
    [17/1000] Train Loss: 0.18403026546773493, Valid Loss: 0.33446866273880005, Valid Accuracy: 0.8928
    ======> 17 Epoch에서 조기종료-0.32045993208885193에서 개선 안됨
    


```python
print(f'1 Epoch당 걸린 시간 {e - s}')
```

    1 Epoch당 걸린 시간 739.8945105075836
    


```python

```


```python
## 저장된 모델 로딩
best_model = torch.load(save_model_path)
```


```python
### test_dataloader로 평가
best_model = best_model.to(device)
best_model.eval()

valid_loss = 0.0 # 현재 epoch의 대한 검증의 validation loss 저장 변수
valid_acc = 0.0  # 현재 epoch의 대한 검증의 validation accuracy(정확도) 저장 변수
    
with torch.no_grad(): # no_grad: 도함수 구할 필요없음
    for x_valid, y_valid in fmnist_testloader:
        x_valid, y_valid = x_valid.to(device), y_valid.to(device)
        #  예측
        pred_valid = best_model(x_valid)          # 라벨별 정답일 가능성 출력 (batch, 10)
        pred_label = pred_valid.argmax(dim=-1) # 정답 class 조회 (pred_valid에서 가장 큰값을 가진 index)
        # 평가 
        ## loss 계산
        loss_valid = loss_fn(pred_valid, y_valid) ## loss_fn() batch만큼 평균을 계산.
        valid_loss += loss_valid
        ## 정확도(accuarcy) 계산
        valid_acc += torch.sum(pred_label == y_valid).item() 
    
    valid_loss /= len(fmnist_testloader)        # step 수로 나눠서 평균 계산
    valid_acc /= len(fmnist_testloader.dataset) # test set의 총 데이터 개수로 나눔
    # 1 eopch에 대한 vudrk dhksfy => valid_loss_list, valid_acc_list 추가
    valid_loss_list.append(valid_loss)
    valid_acc_list.append(valid_acc)
        
```


```python
valid_loss
```




    tensor(0.3205)




```python
valid_acc
```




    0.8918



## 위스콘신 유방암 데이터셋 - 이진분류(Binary Classification) 문제

- **이진 분류 문제 처리 모델의 두가지 방법**
    1. positive(1)일 확률을 출력하도록 구현
        - output layer: units=1, activation='sigmoid'
        - loss: binary_crossentropy
    2. negative(0)일 확률과 positive(1)일 확률을 출력하도록 구현 => 다중분류 처리 방식으로 해결       
        - output layer: units=2, activation='softmax', y(정답)은 one hot encoding 처리
        - loss: categorical_crossentropy
        
- 위스콘신 대학교에서 제공한 종양의 악성/양성여부 분류를 위한 데이터셋
- Feature
    - 종양에 대한 다양한 측정값들
- Target의 class
    - 0 - malignant(악성종양)
    - 1 - benign(양성종양)


```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

import numpy as np
import matplotlib.pyplot as plt
import time
```


```python
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device
```




    'cpu'



### DataSet, DataLoader 생성


```python
x, y = load_breast_cancer(return_X_y=True)
print(type(x), type(y))
print(x.shape, y.shape)
print(np.unique(y))
```

    <class 'numpy.ndarray'> <class 'numpy.ndarray'>
    (569, 30) (569,)
    [0 1]
    


```python
index_to_class = np.array(['악성', '양성'])
class_to_index = dict(악성=0, 양성=1)
index_to_class, class_to_index
```




    (array(['악성', '양성'], dtype='<U2'), {'악성': 0, '양성': 1})




```python
# y shape을 2차원으로 변경 ==> 모델 출력 shape과 맞춰준다.
# (batch_size, 1)
y = y.reshape(-1, 1)
y.shape
```




    (569, 1)




```python
# train / test set 분리
x_train, x_test, y_train, y_test = train_test_split(x, y, # 나눌 대상 x, y
                                                    test_size = 0.25, # 나눌 비율 => train set: 0.75, test set: 0.25
                                                    stratify=y, # class 별 비율을 맞춰서 나눔
                                                   )
x_train.shape, x_test.shape, y_train.shape, y_test.shape
```




    ((426, 30), (143, 30), (426, 1), (143, 1))




```python
# 전처리 - Feature Scaling (컬럼의 scale(척도, 단위)를 맞춘다.)
## StandarScaler => 평군: 0, 표준편차: 1 을 기준으로 맞춤
scaler = StandardScaler()

x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test) # Trainset으로 fit한 scaler를 이용해 변환
```


```python
# ndarray => Tensor 변환 ==> Dataset 구성 ==> Dataloader 구성
```


```python
# ndarray => Tensor 변환
x_train_tensor = torch.tensor(x_train_scaled, dtype=torch.float32)
x_test_tensor  = torch.tensor(x_test_scaled, dtype=torch.float32)

y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
y_test_tensor  = torch.tensor(y_test, dtype=torch.float32)
```


```python
# Dataset 생성 => TensorDataset
trainset = TensorDataset(x_train_tensor, y_train_tensor)
testset = TensorDataset(x_test_tensor, y_test_tensor)
print(f"DataSet의 데이터개수 TrainSet: {len(trainset)}, TestSet: {len(testset)}")
```

    DataSet의 데이터개수 TrainSet: 426, TestSet: 143
    


```python
# DataLoader 생성
trainloader = DataLoader(trainset, batch_size=200, shuffle=True, drop_last=True)
testloader = DataLoader(testset, batch_size=len(testset))
print(f"Epoch당 Step 수 TrainLoader: {len(trainloader)}, TestLoader: {len(testloader)}")
```

    Epoch당 Step 수 TrainLoader: 2, TestLoader: 1
    

### Model 클래스 정의


```python
x_train.shape
```




    (426, 30)




```python
y_test.shape
```




    (143, 1)




```python
class BCModel(nn.Module):

    def __init__(self):
        super().__init__() # nn.Module __init__() 초기화

        self.lr1 = nn.Linear(30, 32)
        self.lr2 = nn.Linear(32, 8)
        self.lr3 = nn.Linear(8, 1) # 출력 Layer: 이진분류 - positive의 확률 값 한개를 출력: out_features=1
        
    def forward(self, x):
        # x (입력) shape: (batchsize, 30)
        out = nn.ReLU()(self.lr1(x))
        out = nn.ReLU()(self.lr2(out))
        # 이진분류 출력값 처리 -> Linear()는 한개의 값을 출력 => 확률값으로 변경 ==> Sigmoid/Logistic 함수를 Activation함수 사용
        out = self.lr3(out) 
        out = nn.Sigmoid()(out) # nn.Sigmoid/Logistic(): 확률값으로 변경. Activation 함수사용
        
        return out
```


```python
model = BCModel()
tmp_x = torch.ones(5, 30)
print(tmp_x.shape)
tmp_y = model(tmp_x)
tmp_y
# 0.4990 -> 1(양성-positive)일 확률
# Tensor객체.tpye(타입을지정) ==> Tensor 데이터타입 변환
# bool -> int: False: 0, True: 1
(tmp_y > 0.5).type(torch.int32) == 0, 1 # label 뽑기
```

    torch.Size([5, 30])
    




    (tensor([[True],
             [True],
             [True],
             [True],
             [True]]),
     1)




```python
torchinfo.summary(model, (1, 30))
```




    ==========================================================================================
    Layer (type:depth-idx)                   Output Shape              Param #
    ==========================================================================================
    BCModel                                  [1, 1]                    --
    ├─Linear: 1-1                            [1, 32]                   992
    ├─Linear: 1-2                            [1, 8]                    264
    ├─Linear: 1-3                            [1, 1]                    9
    ==========================================================================================
    Total params: 1,265
    Trainable params: 1,265
    Non-trainable params: 0
    Total mult-adds (M): 0.00
    ==========================================================================================
    Input size (MB): 0.00
    Forward/backward pass size (MB): 0.00
    Params size (MB): 0.01
    Estimated Total Size (MB): 0.01
    ==========================================================================================




```python
trainset[0]
```




    (tensor([ 0.4094,  0.1785,  0.4586,  0.2805,  1.4519,  1.1197,  1.3839,  1.1532,
              0.4889,  0.7177,  0.9248,  0.2452,  0.9596,  0.6623,  0.1024,  0.2789,
              0.6563,  0.7683, -0.2065,  0.1975,  1.0319,  0.6142,  1.0585,  0.8590,
              1.6372,  0.9781,  1.4728,  1.5402,  0.5459,  1.0845]),
     tensor([0.]))



### Train(학습/훈련)


```python
# 하이퍼파라미터
LR = 0.001
N_EPOCH = 1000

# 조기 종료
## 모델 저장 변수 
best_score = torch.inf
save_model_path = 'models/bc_best_model.pth'
## 특정 epoch동안 성능개선 없으면 학습중단
patience = 20   # 성능이 개선 될떄 까지 몇 에폭 기다릴 것인지
trigger_cnt = 0 # 성능이 개선 될때 까지 현재 몇번째 기달렸는지

# 모델 생성
model = BCModel().to(device)
# loss 함수
loss_fn = nn.BCELoss() # Binary Cross Entropy Loss
# Optimizer 정의
optimizer = torch.optim.Adam(model.parameters(), lr=LR)
```


```python
##########################################################################
# Epoch 별 검즘 -> Train Loss, Validation Loss, Validation Acccuray
# 조기종료(Enarly Stopping) - 성능 개선이 안되면 학습을 중단
# 가장 좋은 성능을 내는 에폭의 모델을 저장.
# 조기종료/모델 저장 ==> Validation loss 기준
##########################################################################

train_loss_list = []
valid_loss_list = []
valid_acc_list  = []

s = time.time()

for epoch in range(N_EPOCH):
    ######################
    #      Train
    ######################
    model.to(device) 
    train_loss = 0.0

    for x, y in trainloader:
        x, y = x.to(device), y.to(device)

        pred = model(x) # 예측 - 순전파
        loss = loss_fn(pred, y) # loss 계산 -> (예측값, 정답)
        optimizer.zero_grad() # gradient 초기화
        loss.backward() # grad 계산 - 오차 역전파
        optimizer.step() # 파라미터 업데이트
        train_loss += loss.item() # train loss 누적

    train_loss /= len(trainloader)
    train_loss_list.append(train_loss)

    ######################
    #     Validation
    ######################
    model.eval()
    valid_loss = 0.0
    valid_acc  = 0.0

    with torch.no_grad():
        for x_valid, y_valid in testloader:
            x_valid, y_valid = x_valid.to(device), y_valid.to(device)

            # 예측
            pred_valid = model(x_valid)
            pred_label = (pred_valid > 0.5).type(torch.int32)

            # 평가
            ## Loss 계산
            loss_valid = loss_fn(pred_valid, y_valid)
            valid_loss += loss_valid.item()

            ## Accuracy 계산
            valid_acc += torch.sum(pred_label == y_valid).item()

        valid_loss /= len(testloader)
        valid_acc /= len(testloader.dataset)

    valid_loss_list.append(valid_loss)
    valid_acc_list.append(valid_acc)

    print(f"[{epoch+1}/{N_EPOCH}] Train Loss: {train_loss}, Validation Loss: {valid_loss}, Validation Accuracy: {valid_acc}")

    ##############################
    # 조기종료, 모델 저장처리
    #     저장: 현 EPOCH valide Loss가 best_score보다 개선된 경우 저장
    ##############################
    if valid_loss < best_score:
        print(f"====> 모델저장: {epoch+1}, Epoch - 이전 valid_loss: {best_score}, 현재 valid_loss: {valid_loss}")
        best_score = valid_loss
        torch.save(model, save_model_path)
        trigger_cnt = 0
    else:
        trigger_cnt += 1
        if patience == trigger_cnt:
            print(f"====> {epoch+1} Epoch에서 조기종료-{best_score}에서 개선 안됨")
            break

e = time.time()
print(f"학습시간: {e - s}")
```

    [1/1000] Train Loss: 0.6741175055503845, Validation Loss: 0.6708158850669861, Validation Accuracy: 0.6293706293706294
    ====> 모델저장: 1, Epoch - 이전 valid_loss: inf, 현재 valid_loss: 0.6708158850669861
    [2/1000] Train Loss: 0.6678630113601685, Validation Loss: 0.6614448428153992, Validation Accuracy: 0.6363636363636364
    ====> 모델저장: 2, Epoch - 이전 valid_loss: 0.6708158850669861, 현재 valid_loss: 0.6614448428153992
    [3/1000] Train Loss: 0.6614936292171478, Validation Loss: 0.6521831750869751, Validation Accuracy: 0.6433566433566433
    ====> 모델저장: 3, Epoch - 이전 valid_loss: 0.6614448428153992, 현재 valid_loss: 0.6521831750869751
    [4/1000] Train Loss: 0.6519761383533478, Validation Loss: 0.6430516839027405, Validation Accuracy: 0.6573426573426573
    ====> 모델저장: 4, Epoch - 이전 valid_loss: 0.6521831750869751, 현재 valid_loss: 0.6430516839027405
    [5/1000] Train Loss: 0.6439206302165985, Validation Loss: 0.6342710852622986, Validation Accuracy: 0.6643356643356644
    ====> 모델저장: 5, Epoch - 이전 valid_loss: 0.6430516839027405, 현재 valid_loss: 0.6342710852622986
    [6/1000] Train Loss: 0.6359294950962067, Validation Loss: 0.6255669593811035, Validation Accuracy: 0.6713286713286714
    ====> 모델저장: 6, Epoch - 이전 valid_loss: 0.6342710852622986, 현재 valid_loss: 0.6255669593811035
    [7/1000] Train Loss: 0.6280315816402435, Validation Loss: 0.6170052886009216, Validation Accuracy: 0.7202797202797203
    ====> 모델저장: 7, Epoch - 이전 valid_loss: 0.6255669593811035, 현재 valid_loss: 0.6170052886009216
    [8/1000] Train Loss: 0.6209920048713684, Validation Loss: 0.6082406044006348, Validation Accuracy: 0.7552447552447552
    ====> 모델저장: 8, Epoch - 이전 valid_loss: 0.6170052886009216, 현재 valid_loss: 0.6082406044006348
    [9/1000] Train Loss: 0.6122882068157196, Validation Loss: 0.5992779731750488, Validation Accuracy: 0.7762237762237763
    ====> 모델저장: 9, Epoch - 이전 valid_loss: 0.6082406044006348, 현재 valid_loss: 0.5992779731750488
    [10/1000] Train Loss: 0.6038097143173218, Validation Loss: 0.5899872779846191, Validation Accuracy: 0.7972027972027972
    ====> 모델저장: 10, Epoch - 이전 valid_loss: 0.5992779731750488, 현재 valid_loss: 0.5899872779846191
    [11/1000] Train Loss: 0.5941876471042633, Validation Loss: 0.5802217721939087, Validation Accuracy: 0.8321678321678322
    ====> 모델저장: 11, Epoch - 이전 valid_loss: 0.5899872779846191, 현재 valid_loss: 0.5802217721939087
    [12/1000] Train Loss: 0.5875888466835022, Validation Loss: 0.5697325468063354, Validation Accuracy: 0.8461538461538461
    ====> 모델저장: 12, Epoch - 이전 valid_loss: 0.5802217721939087, 현재 valid_loss: 0.5697325468063354
    [13/1000] Train Loss: 0.5770463645458221, Validation Loss: 0.5581168532371521, Validation Accuracy: 0.8601398601398601
    ====> 모델저장: 13, Epoch - 이전 valid_loss: 0.5697325468063354, 현재 valid_loss: 0.5581168532371521
    [14/1000] Train Loss: 0.5681036114692688, Validation Loss: 0.5456867814064026, Validation Accuracy: 0.8741258741258742
    ====> 모델저장: 14, Epoch - 이전 valid_loss: 0.5581168532371521, 현재 valid_loss: 0.5456867814064026
    [15/1000] Train Loss: 0.557398647069931, Validation Loss: 0.5325140953063965, Validation Accuracy: 0.8951048951048951
    ====> 모델저장: 15, Epoch - 이전 valid_loss: 0.5456867814064026, 현재 valid_loss: 0.5325140953063965
    [16/1000] Train Loss: 0.5435732305049896, Validation Loss: 0.5188500881195068, Validation Accuracy: 0.9090909090909091
    ====> 모델저장: 16, Epoch - 이전 valid_loss: 0.5325140953063965, 현재 valid_loss: 0.5188500881195068
    [17/1000] Train Loss: 0.5307561755180359, Validation Loss: 0.5047910213470459, Validation Accuracy: 0.9230769230769231
    ====> 모델저장: 17, Epoch - 이전 valid_loss: 0.5188500881195068, 현재 valid_loss: 0.5047910213470459
    [18/1000] Train Loss: 0.517554372549057, Validation Loss: 0.49033787846565247, Validation Accuracy: 0.9370629370629371
    ====> 모델저장: 18, Epoch - 이전 valid_loss: 0.5047910213470459, 현재 valid_loss: 0.49033787846565247
    [19/1000] Train Loss: 0.5020811855792999, Validation Loss: 0.4755590558052063, Validation Accuracy: 0.9300699300699301
    ====> 모델저장: 19, Epoch - 이전 valid_loss: 0.49033787846565247, 현재 valid_loss: 0.4755590558052063
    [20/1000] Train Loss: 0.4903675466775894, Validation Loss: 0.4606221914291382, Validation Accuracy: 0.9300699300699301
    ====> 모델저장: 20, Epoch - 이전 valid_loss: 0.4755590558052063, 현재 valid_loss: 0.4606221914291382
    [21/1000] Train Loss: 0.47660303115844727, Validation Loss: 0.4456084668636322, Validation Accuracy: 0.9370629370629371
    ====> 모델저장: 21, Epoch - 이전 valid_loss: 0.4606221914291382, 현재 valid_loss: 0.4456084668636322
    [22/1000] Train Loss: 0.45938338339328766, Validation Loss: 0.43059617280960083, Validation Accuracy: 0.951048951048951
    ====> 모델저장: 22, Epoch - 이전 valid_loss: 0.4456084668636322, 현재 valid_loss: 0.43059617280960083
    [23/1000] Train Loss: 0.44604532420635223, Validation Loss: 0.4156356453895569, Validation Accuracy: 0.951048951048951
    ====> 모델저장: 23, Epoch - 이전 valid_loss: 0.43059617280960083, 현재 valid_loss: 0.4156356453895569
    [24/1000] Train Loss: 0.435753658413887, Validation Loss: 0.40084460377693176, Validation Accuracy: 0.951048951048951
    ====> 모델저장: 24, Epoch - 이전 valid_loss: 0.4156356453895569, 현재 valid_loss: 0.40084460377693176
    [25/1000] Train Loss: 0.41685838997364044, Validation Loss: 0.38615837693214417, Validation Accuracy: 0.951048951048951
    ====> 모델저장: 25, Epoch - 이전 valid_loss: 0.40084460377693176, 현재 valid_loss: 0.38615837693214417
    [26/1000] Train Loss: 0.40195851027965546, Validation Loss: 0.37165987491607666, Validation Accuracy: 0.9440559440559441
    ====> 모델저장: 26, Epoch - 이전 valid_loss: 0.38615837693214417, 현재 valid_loss: 0.37165987491607666
    [27/1000] Train Loss: 0.38718321919441223, Validation Loss: 0.35732731223106384, Validation Accuracy: 0.9440559440559441
    ====> 모델저장: 27, Epoch - 이전 valid_loss: 0.37165987491607666, 현재 valid_loss: 0.35732731223106384
    [28/1000] Train Loss: 0.3756188303232193, Validation Loss: 0.34318527579307556, Validation Accuracy: 0.9440559440559441
    ====> 모델저장: 28, Epoch - 이전 valid_loss: 0.35732731223106384, 현재 valid_loss: 0.34318527579307556
    [29/1000] Train Loss: 0.36218634247779846, Validation Loss: 0.3293122947216034, Validation Accuracy: 0.9440559440559441
    ====> 모델저장: 29, Epoch - 이전 valid_loss: 0.34318527579307556, 현재 valid_loss: 0.3293122947216034
    [30/1000] Train Loss: 0.34152472019195557, Validation Loss: 0.3156229257583618, Validation Accuracy: 0.9440559440559441
    ====> 모델저장: 30, Epoch - 이전 valid_loss: 0.3293122947216034, 현재 valid_loss: 0.3156229257583618
    [31/1000] Train Loss: 0.33133769035339355, Validation Loss: 0.302140474319458, Validation Accuracy: 0.951048951048951
    ====> 모델저장: 31, Epoch - 이전 valid_loss: 0.3156229257583618, 현재 valid_loss: 0.302140474319458
    [32/1000] Train Loss: 0.3215331882238388, Validation Loss: 0.2890358567237854, Validation Accuracy: 0.958041958041958
    ====> 모델저장: 32, Epoch - 이전 valid_loss: 0.302140474319458, 현재 valid_loss: 0.2890358567237854
    [33/1000] Train Loss: 0.29992568492889404, Validation Loss: 0.2762816250324249, Validation Accuracy: 0.958041958041958
    ====> 모델저장: 33, Epoch - 이전 valid_loss: 0.2890358567237854, 현재 valid_loss: 0.2762816250324249
    [34/1000] Train Loss: 0.2927548587322235, Validation Loss: 0.26399102807044983, Validation Accuracy: 0.958041958041958
    ====> 모델저장: 34, Epoch - 이전 valid_loss: 0.2762816250324249, 현재 valid_loss: 0.26399102807044983
    [35/1000] Train Loss: 0.2798386812210083, Validation Loss: 0.2521766126155853, Validation Accuracy: 0.958041958041958
    ====> 모델저장: 35, Epoch - 이전 valid_loss: 0.26399102807044983, 현재 valid_loss: 0.2521766126155853
    [36/1000] Train Loss: 0.2743297666311264, Validation Loss: 0.240827277302742, Validation Accuracy: 0.958041958041958
    ====> 모델저장: 36, Epoch - 이전 valid_loss: 0.2521766126155853, 현재 valid_loss: 0.240827277302742
    [37/1000] Train Loss: 0.26038219034671783, Validation Loss: 0.22999005019664764, Validation Accuracy: 0.958041958041958
    ====> 모델저장: 37, Epoch - 이전 valid_loss: 0.240827277302742, 현재 valid_loss: 0.22999005019664764
    [38/1000] Train Loss: 0.24617457389831543, Validation Loss: 0.21968670189380646, Validation Accuracy: 0.958041958041958
    ====> 모델저장: 38, Epoch - 이전 valid_loss: 0.22999005019664764, 현재 valid_loss: 0.21968670189380646
    [39/1000] Train Loss: 0.23627957701683044, Validation Loss: 0.20990131795406342, Validation Accuracy: 0.965034965034965
    ====> 모델저장: 39, Epoch - 이전 valid_loss: 0.21968670189380646, 현재 valid_loss: 0.20990131795406342
    [40/1000] Train Loss: 0.22506674379110336, Validation Loss: 0.20059525966644287, Validation Accuracy: 0.965034965034965
    ====> 모델저장: 40, Epoch - 이전 valid_loss: 0.20990131795406342, 현재 valid_loss: 0.20059525966644287
    [41/1000] Train Loss: 0.2195119708776474, Validation Loss: 0.19180943071842194, Validation Accuracy: 0.965034965034965
    ====> 모델저장: 41, Epoch - 이전 valid_loss: 0.20059525966644287, 현재 valid_loss: 0.19180943071842194
    [42/1000] Train Loss: 0.21259357035160065, Validation Loss: 0.18349049985408783, Validation Accuracy: 0.965034965034965
    ====> 모델저장: 42, Epoch - 이전 valid_loss: 0.19180943071842194, 현재 valid_loss: 0.18349049985408783
    [43/1000] Train Loss: 0.2008635625243187, Validation Loss: 0.17563655972480774, Validation Accuracy: 0.965034965034965
    ====> 모델저장: 43, Epoch - 이전 valid_loss: 0.18349049985408783, 현재 valid_loss: 0.17563655972480774
    [44/1000] Train Loss: 0.19014614075422287, Validation Loss: 0.16817213594913483, Validation Accuracy: 0.965034965034965
    ====> 모델저장: 44, Epoch - 이전 valid_loss: 0.17563655972480774, 현재 valid_loss: 0.16817213594913483
    [45/1000] Train Loss: 0.18756898492574692, Validation Loss: 0.16113337874412537, Validation Accuracy: 0.965034965034965
    ====> 모델저장: 45, Epoch - 이전 valid_loss: 0.16817213594913483, 현재 valid_loss: 0.16113337874412537
    [46/1000] Train Loss: 0.18165917694568634, Validation Loss: 0.15450641512870789, Validation Accuracy: 0.972027972027972
    ====> 모델저장: 46, Epoch - 이전 valid_loss: 0.16113337874412537, 현재 valid_loss: 0.15450641512870789
    [47/1000] Train Loss: 0.17236723005771637, Validation Loss: 0.14822958409786224, Validation Accuracy: 0.972027972027972
    ====> 모델저장: 47, Epoch - 이전 valid_loss: 0.15450641512870789, 현재 valid_loss: 0.14822958409786224
    [48/1000] Train Loss: 0.16983720660209656, Validation Loss: 0.14233353734016418, Validation Accuracy: 0.9790209790209791
    ====> 모델저장: 48, Epoch - 이전 valid_loss: 0.14822958409786224, 현재 valid_loss: 0.14233353734016418
    [49/1000] Train Loss: 0.16643912345170975, Validation Loss: 0.1367732137441635, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 49, Epoch - 이전 valid_loss: 0.14233353734016418, 현재 valid_loss: 0.1367732137441635
    [50/1000] Train Loss: 0.15189404785633087, Validation Loss: 0.1315813809633255, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 50, Epoch - 이전 valid_loss: 0.1367732137441635, 현재 valid_loss: 0.1315813809633255
    [51/1000] Train Loss: 0.1564871370792389, Validation Loss: 0.12665173411369324, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 51, Epoch - 이전 valid_loss: 0.1315813809633255, 현재 valid_loss: 0.12665173411369324
    [52/1000] Train Loss: 0.14721008390188217, Validation Loss: 0.12198711931705475, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 52, Epoch - 이전 valid_loss: 0.12665173411369324, 현재 valid_loss: 0.12198711931705475
    [53/1000] Train Loss: 0.14368975162506104, Validation Loss: 0.11760080605745316, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 53, Epoch - 이전 valid_loss: 0.12198711931705475, 현재 valid_loss: 0.11760080605745316
    [54/1000] Train Loss: 0.13746348023414612, Validation Loss: 0.11348062753677368, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 54, Epoch - 이전 valid_loss: 0.11760080605745316, 현재 valid_loss: 0.11348062753677368
    [55/1000] Train Loss: 0.13617891818284988, Validation Loss: 0.10956874489784241, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 55, Epoch - 이전 valid_loss: 0.11348062753677368, 현재 valid_loss: 0.10956874489784241
    [56/1000] Train Loss: 0.1283860094845295, Validation Loss: 0.10589202493429184, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 56, Epoch - 이전 valid_loss: 0.10956874489784241, 현재 valid_loss: 0.10589202493429184
    [57/1000] Train Loss: 0.12722620368003845, Validation Loss: 0.10247378796339035, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 57, Epoch - 이전 valid_loss: 0.10589202493429184, 현재 valid_loss: 0.10247378796339035
    [58/1000] Train Loss: 0.1211077980697155, Validation Loss: 0.09928875416517258, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 58, Epoch - 이전 valid_loss: 0.10247378796339035, 현재 valid_loss: 0.09928875416517258
    [59/1000] Train Loss: 0.1249229796230793, Validation Loss: 0.09629734605550766, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 59, Epoch - 이전 valid_loss: 0.09928875416517258, 현재 valid_loss: 0.09629734605550766
    [60/1000] Train Loss: 0.12163551524281502, Validation Loss: 0.09351880103349686, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 60, Epoch - 이전 valid_loss: 0.09629734605550766, 현재 valid_loss: 0.09351880103349686
    [61/1000] Train Loss: 0.11694461107254028, Validation Loss: 0.09095652401447296, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 61, Epoch - 이전 valid_loss: 0.09351880103349686, 현재 valid_loss: 0.09095652401447296
    [62/1000] Train Loss: 0.11012277007102966, Validation Loss: 0.08852499723434448, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 62, Epoch - 이전 valid_loss: 0.09095652401447296, 현재 valid_loss: 0.08852499723434448
    [63/1000] Train Loss: 0.1156475841999054, Validation Loss: 0.08626413345336914, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 63, Epoch - 이전 valid_loss: 0.08852499723434448, 현재 valid_loss: 0.08626413345336914
    [64/1000] Train Loss: 0.11127766594290733, Validation Loss: 0.08410678058862686, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 64, Epoch - 이전 valid_loss: 0.08626413345336914, 현재 valid_loss: 0.08410678058862686
    [65/1000] Train Loss: 0.10898396372795105, Validation Loss: 0.08211714029312134, Validation Accuracy: 0.9790209790209791
    ====> 모델저장: 65, Epoch - 이전 valid_loss: 0.08410678058862686, 현재 valid_loss: 0.08211714029312134
    [66/1000] Train Loss: 0.10059763118624687, Validation Loss: 0.0802692398428917, Validation Accuracy: 0.9790209790209791
    ====> 모델저장: 66, Epoch - 이전 valid_loss: 0.08211714029312134, 현재 valid_loss: 0.0802692398428917
    [67/1000] Train Loss: 0.10426374897360802, Validation Loss: 0.0785137191414833, Validation Accuracy: 0.9790209790209791
    ====> 모델저장: 67, Epoch - 이전 valid_loss: 0.0802692398428917, 현재 valid_loss: 0.0785137191414833
    [68/1000] Train Loss: 0.10317957401275635, Validation Loss: 0.07690529525279999, Validation Accuracy: 0.9790209790209791
    ====> 모델저장: 68, Epoch - 이전 valid_loss: 0.0785137191414833, 현재 valid_loss: 0.07690529525279999
    [69/1000] Train Loss: 0.09973451495170593, Validation Loss: 0.07538805156946182, Validation Accuracy: 0.9790209790209791
    ====> 모델저장: 69, Epoch - 이전 valid_loss: 0.07690529525279999, 현재 valid_loss: 0.07538805156946182
    [70/1000] Train Loss: 0.09990551695227623, Validation Loss: 0.07396455109119415, Validation Accuracy: 0.9790209790209791
    ====> 모델저장: 70, Epoch - 이전 valid_loss: 0.07538805156946182, 현재 valid_loss: 0.07396455109119415
    [71/1000] Train Loss: 0.09642726182937622, Validation Loss: 0.072609543800354, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 71, Epoch - 이전 valid_loss: 0.07396455109119415, 현재 valid_loss: 0.072609543800354
    [72/1000] Train Loss: 0.09503374993801117, Validation Loss: 0.07134933024644852, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 72, Epoch - 이전 valid_loss: 0.072609543800354, 현재 valid_loss: 0.07134933024644852
    [73/1000] Train Loss: 0.09588029608130455, Validation Loss: 0.07014207541942596, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 73, Epoch - 이전 valid_loss: 0.07134933024644852, 현재 valid_loss: 0.07014207541942596
    [74/1000] Train Loss: 0.08599577471613884, Validation Loss: 0.06898954510688782, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 74, Epoch - 이전 valid_loss: 0.07014207541942596, 현재 valid_loss: 0.06898954510688782
    [75/1000] Train Loss: 0.08852285519242287, Validation Loss: 0.06786073744297028, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 75, Epoch - 이전 valid_loss: 0.06898954510688782, 현재 valid_loss: 0.06786073744297028
    [76/1000] Train Loss: 0.08933375030755997, Validation Loss: 0.06680620461702347, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 76, Epoch - 이전 valid_loss: 0.06786073744297028, 현재 valid_loss: 0.06680620461702347
    [77/1000] Train Loss: 0.08811774477362633, Validation Loss: 0.06574998795986176, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 77, Epoch - 이전 valid_loss: 0.06680620461702347, 현재 valid_loss: 0.06574998795986176
    [78/1000] Train Loss: 0.0884055569767952, Validation Loss: 0.06482389569282532, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 78, Epoch - 이전 valid_loss: 0.06574998795986176, 현재 valid_loss: 0.06482389569282532
    [79/1000] Train Loss: 0.08322933316230774, Validation Loss: 0.06395028531551361, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 79, Epoch - 이전 valid_loss: 0.06482389569282532, 현재 valid_loss: 0.06395028531551361
    [80/1000] Train Loss: 0.08357486128807068, Validation Loss: 0.06313171982765198, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 80, Epoch - 이전 valid_loss: 0.06395028531551361, 현재 valid_loss: 0.06313171982765198
    [81/1000] Train Loss: 0.08680471405386925, Validation Loss: 0.06233074516057968, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 81, Epoch - 이전 valid_loss: 0.06313171982765198, 현재 valid_loss: 0.06233074516057968
    [82/1000] Train Loss: 0.08348387107253075, Validation Loss: 0.06161367520689964, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 82, Epoch - 이전 valid_loss: 0.06233074516057968, 현재 valid_loss: 0.06161367520689964
    [83/1000] Train Loss: 0.08275306969881058, Validation Loss: 0.06093236058950424, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 83, Epoch - 이전 valid_loss: 0.06161367520689964, 현재 valid_loss: 0.06093236058950424
    [84/1000] Train Loss: 0.08187570050358772, Validation Loss: 0.06028825044631958, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 84, Epoch - 이전 valid_loss: 0.06093236058950424, 현재 valid_loss: 0.06028825044631958
    [85/1000] Train Loss: 0.081184271723032, Validation Loss: 0.059705086052417755, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 85, Epoch - 이전 valid_loss: 0.06028825044631958, 현재 valid_loss: 0.059705086052417755
    [86/1000] Train Loss: 0.07916170358657837, Validation Loss: 0.0591803714632988, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 86, Epoch - 이전 valid_loss: 0.059705086052417755, 현재 valid_loss: 0.0591803714632988
    [87/1000] Train Loss: 0.0782342441380024, Validation Loss: 0.058634284883737564, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 87, Epoch - 이전 valid_loss: 0.0591803714632988, 현재 valid_loss: 0.058634284883737564
    [88/1000] Train Loss: 0.07785648852586746, Validation Loss: 0.05814041569828987, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 88, Epoch - 이전 valid_loss: 0.058634284883737564, 현재 valid_loss: 0.05814041569828987
    [89/1000] Train Loss: 0.07750289514660835, Validation Loss: 0.057722967118024826, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 89, Epoch - 이전 valid_loss: 0.05814041569828987, 현재 valid_loss: 0.057722967118024826
    [90/1000] Train Loss: 0.07606060802936554, Validation Loss: 0.05734610930085182, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 90, Epoch - 이전 valid_loss: 0.057722967118024826, 현재 valid_loss: 0.05734610930085182
    [91/1000] Train Loss: 0.07399865612387657, Validation Loss: 0.056982867419719696, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 91, Epoch - 이전 valid_loss: 0.05734610930085182, 현재 valid_loss: 0.056982867419719696
    [92/1000] Train Loss: 0.06503233499825001, Validation Loss: 0.05662105977535248, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 92, Epoch - 이전 valid_loss: 0.056982867419719696, 현재 valid_loss: 0.05662105977535248
    [93/1000] Train Loss: 0.0720178410410881, Validation Loss: 0.05633613467216492, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 93, Epoch - 이전 valid_loss: 0.05662105977535248, 현재 valid_loss: 0.05633613467216492
    [94/1000] Train Loss: 0.07242501527070999, Validation Loss: 0.05592881143093109, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 94, Epoch - 이전 valid_loss: 0.05633613467216492, 현재 valid_loss: 0.05592881143093109
    [95/1000] Train Loss: 0.0733112096786499, Validation Loss: 0.055534541606903076, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 95, Epoch - 이전 valid_loss: 0.05592881143093109, 현재 valid_loss: 0.055534541606903076
    [96/1000] Train Loss: 0.06985883601009846, Validation Loss: 0.05524216964840889, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 96, Epoch - 이전 valid_loss: 0.055534541606903076, 현재 valid_loss: 0.05524216964840889
    [97/1000] Train Loss: 0.07127485796809196, Validation Loss: 0.05495191738009453, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 97, Epoch - 이전 valid_loss: 0.05524216964840889, 현재 valid_loss: 0.05495191738009453
    [98/1000] Train Loss: 0.06984798982739449, Validation Loss: 0.05468336492776871, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 98, Epoch - 이전 valid_loss: 0.05495191738009453, 현재 valid_loss: 0.05468336492776871
    [99/1000] Train Loss: 0.06973106041550636, Validation Loss: 0.054332684725522995, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 99, Epoch - 이전 valid_loss: 0.05468336492776871, 현재 valid_loss: 0.054332684725522995
    [100/1000] Train Loss: 0.0691126137971878, Validation Loss: 0.05408832058310509, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 100, Epoch - 이전 valid_loss: 0.054332684725522995, 현재 valid_loss: 0.05408832058310509
    [101/1000] Train Loss: 0.06760479509830475, Validation Loss: 0.05388537794351578, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 101, Epoch - 이전 valid_loss: 0.05408832058310509, 현재 valid_loss: 0.05388537794351578
    [102/1000] Train Loss: 0.06681676208972931, Validation Loss: 0.053635451942682266, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 102, Epoch - 이전 valid_loss: 0.05388537794351578, 현재 valid_loss: 0.053635451942682266
    [103/1000] Train Loss: 0.06774579733610153, Validation Loss: 0.053449273109436035, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 103, Epoch - 이전 valid_loss: 0.053635451942682266, 현재 valid_loss: 0.053449273109436035
    [104/1000] Train Loss: 0.06614266708493233, Validation Loss: 0.053271982818841934, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 104, Epoch - 이전 valid_loss: 0.053449273109436035, 현재 valid_loss: 0.053271982818841934
    [105/1000] Train Loss: 0.0630611702799797, Validation Loss: 0.053148940205574036, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 105, Epoch - 이전 valid_loss: 0.053271982818841934, 현재 valid_loss: 0.053148940205574036
    [106/1000] Train Loss: 0.06608235277235508, Validation Loss: 0.05302254110574722, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 106, Epoch - 이전 valid_loss: 0.053148940205574036, 현재 valid_loss: 0.05302254110574722
    [107/1000] Train Loss: 0.06524216756224632, Validation Loss: 0.0529605858027935, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 107, Epoch - 이전 valid_loss: 0.05302254110574722, 현재 valid_loss: 0.0529605858027935
    [108/1000] Train Loss: 0.06572580896317959, Validation Loss: 0.05289063602685928, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 108, Epoch - 이전 valid_loss: 0.0529605858027935, 현재 valid_loss: 0.05289063602685928
    [109/1000] Train Loss: 0.06497154012322426, Validation Loss: 0.05277571454644203, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 109, Epoch - 이전 valid_loss: 0.05289063602685928, 현재 valid_loss: 0.05277571454644203
    [110/1000] Train Loss: 0.05475056543946266, Validation Loss: 0.052610550075769424, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 110, Epoch - 이전 valid_loss: 0.05277571454644203, 현재 valid_loss: 0.052610550075769424
    [111/1000] Train Loss: 0.05959320813417435, Validation Loss: 0.05228496342897415, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 111, Epoch - 이전 valid_loss: 0.052610550075769424, 현재 valid_loss: 0.05228496342897415
    [112/1000] Train Loss: 0.05971934646368027, Validation Loss: 0.05203291028738022, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 112, Epoch - 이전 valid_loss: 0.05228496342897415, 현재 valid_loss: 0.05203291028738022
    [113/1000] Train Loss: 0.061487214639782906, Validation Loss: 0.05177511274814606, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 113, Epoch - 이전 valid_loss: 0.05203291028738022, 현재 valid_loss: 0.05177511274814606
    [114/1000] Train Loss: 0.06093236990272999, Validation Loss: 0.051537156105041504, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 114, Epoch - 이전 valid_loss: 0.05177511274814606, 현재 valid_loss: 0.051537156105041504
    [115/1000] Train Loss: 0.061283178627491, Validation Loss: 0.05133724957704544, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 115, Epoch - 이전 valid_loss: 0.051537156105041504, 현재 valid_loss: 0.05133724957704544
    [116/1000] Train Loss: 0.0606058593839407, Validation Loss: 0.05121348053216934, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 116, Epoch - 이전 valid_loss: 0.05133724957704544, 현재 valid_loss: 0.05121348053216934
    [117/1000] Train Loss: 0.06068597920238972, Validation Loss: 0.051164660602808, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 117, Epoch - 이전 valid_loss: 0.05121348053216934, 현재 valid_loss: 0.051164660602808
    [118/1000] Train Loss: 0.05308140069246292, Validation Loss: 0.05114025995135307, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 118, Epoch - 이전 valid_loss: 0.051164660602808, 현재 valid_loss: 0.05114025995135307
    [119/1000] Train Loss: 0.058262865990400314, Validation Loss: 0.051101572811603546, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 119, Epoch - 이전 valid_loss: 0.05114025995135307, 현재 valid_loss: 0.051101572811603546
    [120/1000] Train Loss: 0.05805416777729988, Validation Loss: 0.051069919019937515, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 120, Epoch - 이전 valid_loss: 0.051101572811603546, 현재 valid_loss: 0.051069919019937515
    [121/1000] Train Loss: 0.059048961848020554, Validation Loss: 0.05096794664859772, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 121, Epoch - 이전 valid_loss: 0.051069919019937515, 현재 valid_loss: 0.05096794664859772
    [122/1000] Train Loss: 0.059061404317617416, Validation Loss: 0.05090993642807007, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 122, Epoch - 이전 valid_loss: 0.05096794664859772, 현재 valid_loss: 0.05090993642807007
    [123/1000] Train Loss: 0.05537532642483711, Validation Loss: 0.0508354976773262, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 123, Epoch - 이전 valid_loss: 0.05090993642807007, 현재 valid_loss: 0.0508354976773262
    [124/1000] Train Loss: 0.05099874176084995, Validation Loss: 0.05065483599901199, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 124, Epoch - 이전 valid_loss: 0.0508354976773262, 현재 valid_loss: 0.05065483599901199
    [125/1000] Train Loss: 0.05679813772439957, Validation Loss: 0.05050450935959816, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 125, Epoch - 이전 valid_loss: 0.05065483599901199, 현재 valid_loss: 0.05050450935959816
    [126/1000] Train Loss: 0.05569666996598244, Validation Loss: 0.05038188770413399, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 126, Epoch - 이전 valid_loss: 0.05050450935959816, 현재 valid_loss: 0.05038188770413399
    [127/1000] Train Loss: 0.05543568730354309, Validation Loss: 0.050310976803302765, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 127, Epoch - 이전 valid_loss: 0.05038188770413399, 현재 valid_loss: 0.050310976803302765
    [128/1000] Train Loss: 0.05482260324060917, Validation Loss: 0.05028534308075905, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 128, Epoch - 이전 valid_loss: 0.050310976803302765, 현재 valid_loss: 0.05028534308075905
    [129/1000] Train Loss: 0.052823975682258606, Validation Loss: 0.050181303173303604, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 129, Epoch - 이전 valid_loss: 0.05028534308075905, 현재 valid_loss: 0.050181303173303604
    [130/1000] Train Loss: 0.04687358997762203, Validation Loss: 0.049921583384275436, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 130, Epoch - 이전 valid_loss: 0.050181303173303604, 현재 valid_loss: 0.049921583384275436
    [131/1000] Train Loss: 0.05468953400850296, Validation Loss: 0.049734003841876984, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 131, Epoch - 이전 valid_loss: 0.049921583384275436, 현재 valid_loss: 0.049734003841876984
    [132/1000] Train Loss: 0.05397849716246128, Validation Loss: 0.04961743950843811, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 132, Epoch - 이전 valid_loss: 0.049734003841876984, 현재 valid_loss: 0.04961743950843811
    [133/1000] Train Loss: 0.05403509736061096, Validation Loss: 0.049540527164936066, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 133, Epoch - 이전 valid_loss: 0.04961743950843811, 현재 valid_loss: 0.049540527164936066
    [134/1000] Train Loss: 0.053347526118159294, Validation Loss: 0.04952680319547653, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 134, Epoch - 이전 valid_loss: 0.049540527164936066, 현재 valid_loss: 0.04952680319547653
    [135/1000] Train Loss: 0.052017830312252045, Validation Loss: 0.0495179146528244, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 135, Epoch - 이전 valid_loss: 0.04952680319547653, 현재 valid_loss: 0.0495179146528244
    [136/1000] Train Loss: 0.05281858518719673, Validation Loss: 0.04953191429376602, Validation Accuracy: 0.986013986013986
    [137/1000] Train Loss: 0.05282251164317131, Validation Loss: 0.04953896254301071, Validation Accuracy: 0.986013986013986
    [138/1000] Train Loss: 0.05204973928630352, Validation Loss: 0.04957572743296623, Validation Accuracy: 0.986013986013986
    [139/1000] Train Loss: 0.05195312015712261, Validation Loss: 0.04960506036877632, Validation Accuracy: 0.986013986013986
    [140/1000] Train Loss: 0.05121877044439316, Validation Loss: 0.04977471008896828, Validation Accuracy: 0.986013986013986
    [141/1000] Train Loss: 0.04983697831630707, Validation Loss: 0.0498589351773262, Validation Accuracy: 0.986013986013986
    [142/1000] Train Loss: 0.050653666257858276, Validation Loss: 0.050016943365335464, Validation Accuracy: 0.986013986013986
    [143/1000] Train Loss: 0.04653195757418871, Validation Loss: 0.0500168651342392, Validation Accuracy: 0.986013986013986
    [144/1000] Train Loss: 0.05047500878572464, Validation Loss: 0.05001133307814598, Validation Accuracy: 0.986013986013986
    [145/1000] Train Loss: 0.04914079234004021, Validation Loss: 0.049958087503910065, Validation Accuracy: 0.986013986013986
    [146/1000] Train Loss: 0.04404107294976711, Validation Loss: 0.04984227940440178, Validation Accuracy: 0.986013986013986
    [147/1000] Train Loss: 0.04802554100751877, Validation Loss: 0.0498347170650959, Validation Accuracy: 0.986013986013986
    [148/1000] Train Loss: 0.048466918990015984, Validation Loss: 0.049879979342222214, Validation Accuracy: 0.986013986013986
    [149/1000] Train Loss: 0.04388879518955946, Validation Loss: 0.04980580508708954, Validation Accuracy: 0.986013986013986
    [150/1000] Train Loss: 0.048354990780353546, Validation Loss: 0.04971960559487343, Validation Accuracy: 0.986013986013986
    [151/1000] Train Loss: 0.045813657343387604, Validation Loss: 0.049756355583667755, Validation Accuracy: 0.986013986013986
    [152/1000] Train Loss: 0.03726680763065815, Validation Loss: 0.049874868243932724, Validation Accuracy: 0.986013986013986
    [153/1000] Train Loss: 0.047502001747488976, Validation Loss: 0.0499417781829834, Validation Accuracy: 0.986013986013986
    [154/1000] Train Loss: 0.04689234681427479, Validation Loss: 0.04999479278922081, Validation Accuracy: 0.986013986013986
    [155/1000] Train Loss: 0.0438510999083519, Validation Loss: 0.04990578070282936, Validation Accuracy: 0.9790209790209791
    ====> 155 Epoch에서 조기종료-0.0495179146528244에서 개선 안됨
    학습시간: 2.3831734657287598
    


```python
# 결과 시각화
plt.rcParams['font.family'] = 'Malgun gothic'

plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.plot(train_loss_list, label='Train')
plt.plot(valid_loss_list, label='Valid')
plt.title('Epoch별 Loss 변화')
plt.ylim(0, 0.6)
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(valid_acc_list)
plt.title('Validation Accuracy')
plt.tight_layout()
plt.show()
```


    
![png](output_133_0.png)
    


### 모델 Train 결과 확인/평가


```python
# 저장된 모델 로딩
best_model = torch.load(save_model_path)
```


```python
# 모델이 학습한 데이터는 전처리 된것 (Standard Scaling)
## 예측(추론) 핳 데이터도 같은 전처리를 해야한다.
```


```python
x_test_tensor.shape
```




    torch.Size([143, 30])




```python
pred_new = best_model(x_test_tensor)
pred_new.shape
```




    torch.Size([143, 1])




```python
pred_new[:10] # positive(1)일 확률
```




    tensor([[9.9620e-01],
            [1.8350e-05],
            [9.9765e-01],
            [9.9989e-01],
            [9.9965e-01],
            [1.5421e-01],
            [4.7193e-04],
            [9.9993e-01],
            [9.9931e-01],
            [9.7072e-05]], grad_fn=<SliceBackward0>)




```python
# 확률 -> class index
pred_new_label = (pred_new > 0.5).type(torch.int32)
pred_new_label[:10]
```




    tensor([[1],
            [0],
            [1],
            [1],
            [1],
            [0],
            [0],
            [1],
            [1],
            [0]], dtype=torch.int32)




```python
y_test_tensor[:10]
```




    tensor([[1.],
            [0.],
            [1.],
            [1.],
            [1.],
            [1.],
            [0.],
            [1.],
            [1.],
            [0.]])




```python
# test_dataloader 평가
best_model = best_model.to(device)
best_model.eval()

valid_loss = 0.0
valid_acc = 0.0

with torch.no_grad():
        for x_valid, y_valid in testloader:
            x_valid, y_valid = x_valid.to(device), y_valid.to(device)

            # 예측
            pred_valid = model(x_valid)
            pred_label = (pred_valid > 0.5).type(torch.int32)

            # 평가
            ## Loss 계산
            loss_valid = loss_fn(pred_valid, y_valid)
            valid_loss += loss_valid

            ## Accuracy 계산
            valid_acc += torch.sum(pred_label == y_valid).item()

        valid_loss /= len(testloader)
        valid_acc /= len(testloader.dataset)

        valid_loss_list.append(valid_loss)
        valid_acc_list.append(valid_acc)
```


```python
valid_loss
```




    tensor(0.0499)




```python
valid_acc
```




    0.9790209790209791



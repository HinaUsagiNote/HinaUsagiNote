---
layout: single
title: '모델 다중 입력/출력'
typora-root-url: ../
categories: Deeplearning_Pytorch.02.모델 다중 입력/출력
tag: Pytorch
toc: true
---

# 다중 입력, 다중 출력

- 다중입력: Feature가 여러개인 경우
- 다중출력: Output 결과가 여러개인 경우

가상 데이터를 이용해 사과와 오렌지 수확량을 예측하는 선형회귀 모델 정의


|온도(F)|강수량(mm)|습도(%)|사과생산량(ton)|오렌지생산량|
|-|-|-|-:|-:|
|73|67|43|56|70|
|91|88|64|81|101|
|87|134|58|119|133|
|102|43|37|22|37|
|69|96|70|103|119|

```
사과수확량  = w11 * 온도 + w12 * 강수량 + w13 * 습도 + b1
오렌지수확량 = w21 * 온도 + w22 * 강수량 + w23 *습도 + b2
```

- `온도`, `강수량`, `습도` 값이 사과와, 오렌지 수확량에 어느정도 영향을 주는지 가중치를 찾는다.
    - 모델은 사과의 수확량, 오렌지의 수확량 **두개의 예측결과를 출력**해야 한다.
    - 사과에 대해 예측하기 위한 weight 3개와 오렌지에 대해 예측하기 위한 weight 3개 이렇게 두 묶음, 총 6개의 weight를 정의하고 학습을 통해 가장 적당한 값을 찾는다.

## Training Data
- Train data는 feature와 target를 각각 따로 2개의 행렬로 구성한다.
- Feature의 행은 관측치(개별 데이터)를 열을 Feature(특성, 변수)를 표현한다.


```python
import torch
```


```python
# Input (temp, rainfall, humidity) : (5, 3)
inputs = torch.tensor([[73, 67, 43], 
                   [91, 88, 64], 
                   [87, 134, 58], 
                   [102, 43, 37], 
                   [69, 96, 70]], dtype=torch.float32)
```


```python
# Targets: 생산량 - (apples, oranges) - (5, 2)
targets = torch.tensor([[56, 70], 
                    [81, 101], 
                    [119, 133], 
                    [22, 37], 
                    [103, 119]], dtype=torch.float32)
```

## Linear Regression Model (from scratch)

### weight와 bias
- weight 는 각 feature에 곱해지는 가중치로 target 값에 얼마나 영향을 주는지를 나타낸다. 직선의 방정식에서 기울기이다.
- bias는 각 feature와 weight간의 가중합에 더해주는 값으로 모든 feature가 0일 경우 target의 값을 나타낸다. 직선의 방정식에서 절편이다.
- weight의 shape: (2, 3)
- bias의 shape: (2, ) 


```python
import torch
torch.manual_seed(0)
```




    <torch._C.Generator at 0x24fe1eaf490>




```python
### weight와 bias를 생성
weights = torch.randn(3, 2, requires_grad=True) # 파라미터 -> 학습을 통해 찾아야 하는 변수 -> requires_grad=True
# [3, 2] => [Input Feature개수, Output 개수] => [[온도W, 강수량W, 습도W], [사과, 오렌지]]
bias = torch.randn(2, requires_grad=True) # [2] => [Output 개수=> 사과, 오렌지]
```


```python
print(weights.shape, bias.shape)
print(weights)
print(bias)
```

    torch.Size([3, 2]) torch.Size([2])
    tensor([[ 1.5410, -0.2934],
            [-2.1788,  0.5684],
            [-1.0845, -1.3986]], requires_grad=True)
    tensor([0.4033, 0.8380], requires_grad=True)


### Linear Regression model
모델은 weights `w`와 inputs `x`의 내적(dot product)한 값에 bias `b`를 더하는 간단한 함수이다. 

$$
\hspace{2.5cm} X \hspace{1.1cm} \cdot \hspace{1.2cm} W^T \hspace{1.2cm}  + \hspace{1cm} b \hspace{2cm}
$$

$$
\left[ \begin{array}{cc}
73 & 67 & 43 \\
91 & 88 & 64 \\
\vdots & \vdots & \vdots \\
69 & 96 & 70
\end{array} \right]
%
\cdot
%
\left[ \begin{array}{cc}
w_{11} & w_{21} \\
w_{12} & w_{22} \\
w_{13} & w_{23}
\end{array} \right]
%
+
%
\left[ \begin{array}{cc}
b_{1} & b_{2} \\
b_{1} & b_{2} \\
\vdots & \vdots \\
b_{1} & b_{2} \\
\end{array} \right]
$$


```python
print(weights.shape, bias.shape)
print(weights)
print(bias)
```


```python
def model(X):
    return X @ weights + bias
```


```python
targets
```




    tensor([[ 56.,  70.],
            [ 81., 101.],
            [119., 133.],
            [ 22.,  37.],
            [103., 119.]])




```python
# inputs.shape
pred = model(inputs)
```


```python
print(pred.shape)
pred
```

    torch.Size([5, 2])





    tensor([[ -79.7173,  -42.6370],
            [-120.5089,  -65.3522],
            [-220.3900,  -29.6390],
            [  23.7697,  -56.3972],
            [-178.3483,  -62.7408]], grad_fn=<AddBackward0>)



### Loss Function
모델이 예측한 값과 정답간의 차이를 비교하는 메소드. 


```python
def mse_loss_fn(preads, gargets):
    squared_error = (pred - targets)**2
    return torch.mean(squared_error)
```


```python
loss = mse_loss_fn(pred, targets) # (2, ) => 0: 사과생산량 오차, 1: 오렌지생산량 오차
loss
```




    tensor(36193.4961, grad_fn=<MeanBackward0>)



### Gradients 계산
loss에 대한 weight와 bias의 gradients (미분계수)를 계산한다. **Pytorch의 자동미분**을 이용한다. (**graident를 구하려는 tensor는 requires_grad=True로 설정한다.**)


```python
loss.backward()
```


```python
print(weights)
print(weights.grad)
```

    tensor([[ 1.5410, -0.2934],
            [-2.1788,  0.5684],
            [-1.0845, -1.3986]], requires_grad=True)
    tensor([[-15400.8271, -11915.3555],
            [-19847.4902, -13088.5010],
            [-11609.1875,  -8220.1104]])



```python
new_weight_0_0 = weights[0, 0] - weights.grad[0, 0] * 0.001 # Lr
new_weight_0_0
```




    tensor(16.9418, grad_fn=<SubBackward0>)




```python
print(bias)
print(bias.grad)
```

    tensor([0.4033, 0.8380], requires_grad=True)
    tensor([-191.2390, -143.3533])


### 모델 최적화
gradient decent 알고리즘을 이용해 loss를 줄여 모델의 추론 성능을 높인다. 이를 위해 좋은 성능을 낼 수 있도록 **경사하강법(gradient decent)** 을 이용해 weight와 bias를 update한다. 

1. 추론하기
2. loss 계산하기
3. weight와 bias에 대한 gradient계산하기
4. 계산된 gradient에 비례한 값을 학습률을 곱해 작게 만든 뒤 wegith에서 빼서 조정한다.
5. gradient를 0으로 초기화


```python
# step(파라미터 업데이트)수 지정
STEPS = 100

# 학습률 (Learning Rate)
LR = 0.0001 # 1e-4

# Optimizer 생성
optimizer = torch.optim.SGD([weights, bias], lr=LR) # 경사하강법처리. gradient값 초기화
```


```python
for i in range(STEPS):
    # 추론 
    pred = model(inputs)
    # 오차계산
    loss = mse_loss_fn(pred, targets)
    # gradient 계산
    loss.backward()
    # 최적화 -> 파라미터(weights, bias) 값들 업데이트
    optimizer.step()
    # 계산된 gradient 값 초기화
    optimizer.zero_grad()
    # 10 step당 loss 출력
    if i % 10 == 0 or i == (STEPS-1):
        print(f'Step: {i} - loss:{loss.item()}') #  tensor객체.item(): scalar나 원소가 1개인 tensor의 값을 추출(파이썬 타입 값)
```

    Step: 0 - loss:5.865939140319824
    Step: 10 - loss:4.954825401306152
    Step: 20 - loss:4.199545860290527
    Step: 30 - loss:3.57344126701355
    Step: 40 - loss:3.054426670074463
    Step: 50 - loss:2.6241672039031982
    Step: 60 - loss:2.2674918174743652
    Step: 70 - loss:1.971828818321228
    Step: 80 - loss:1.7267286777496338
    Step: 90 - loss:1.5235397815704346
    Step: 99 - loss:1.3705735206604004



```python
print('학습전 파라미터')
print(weights)
print(bias)
# 온도, 강수량, 습도
```

    학습전 파라미터
    tensor([[ 1.5410, -0.2934],
            [-2.1788,  0.5684],
            [-1.0845, -1.3986]], requires_grad=True)
    tensor([0.4033, 0.8380], requires_grad=True)



```python
print('학습후 300번 파라미터')
print(weights)
print(bias)
# 온도, 강수량, 습도
```

    학습후 300번 파라미터
    tensor([[-0.3910, -0.2828],
            [ 0.8682,  0.8374],
            [ 0.6338,  0.7962]], requires_grad=True)
    tensor([0.3987, 0.8516], requires_grad=True)



```python
pred_value = model(inputs)
```


```python
# 예측 정답 - 300번 학습
pred_value
```




    tensor([[ 57.2775,  70.5489],
            [ 81.7814,  99.7641],
            [119.4813, 134.6364],
            [ 21.2970,  37.4738],
            [101.1330, 117.4618]], grad_fn=<AddBackward0>)




```python
# 정답
targets
```




    tensor([[ 56.,  70.],
            [ 81., 101.],
            [119., 133.],
            [ 22.,  37.],
            [103., 119.]])



# pytorch built-in 모델을 사용해 Linear Regression 구현


```python
import torch
import torch.nn as nn # 모델들을 제공하는 모듈
```


```python
inputs = torch.tensor([[73, 67, 43], 
                   [91, 88, 64], 
                   [87, 134, 58], 
                   [102, 43, 37], 
                   [69, 96, 70]], dtype=torch.float32)
```


```python
targets = torch.tensor([[56, 70], 
                    [81, 101], 
                    [119, 133], 
                    [22, 37], 
                    [103, 119]], dtype=torch.float32)
```

## nn.Linear
Pytorch는 nn.Linear 클래스를 통해 Linear Regression 모델을 제공한다.  
nn.Linear에 입력 feature의 개수와 출력 값의 개수를 지정하면 random 값으로 초기화한 weight와 bias들을 생성


```python
# Linear(input feature개수, output개수) 모델 정의
model = nn.Linear(3, 2)
```


```python
## weight와 bias 조회
print(model.weight)
print(model.bias)
```

    Parameter containing:
    tensor([[-0.2020, -0.4732, -0.1228],
            [ 0.1234, -0.3761, -0.0296]], requires_grad=True)
    Parameter containing:
    tensor([ 0.4133, -0.0594], requires_grad=True)


## Optimizer와 Loss 함수 정의
- torch.optim 모듈에 다양한 Optimizer 클래스가 구현되있다. 그 중에서 Adam를 사용한다.
- torch.nn 또는 torch.nn.functional 모듈에 다양한 Loss 함수가 제공된다. 이중 mse_loss() 를 사용한다.


```python
# LOSS 함수 정의
# torch.nn.functional.mse_Loss()
loss_fc = nn.MSELoss()
```


```python
# Optimizer
## ([최적화대상-모델의 파라미터], lr=학습률)
optimizer = torch.optim.SGD(model.parameters(), lr=0.0001) 
```


```python
### 학습전에 추론 -> 오차 계산
## 추론
pred = model(inputs)
pred
```




    tensor([[-51.3168, -17.5246],
            [-67.4688, -23.8240],
            [-87.6917, -41.4416],
            [-45.0803,  -4.7409],
            [-67.5479, -29.7258]], grad_fn=<AddmmBackward0>)




```python
targets
```




    tensor([[ 56.,  70.],
            [ 81., 101.],
            [119., 133.],
            [ 22.,  37.],
            [103., 119.]])




```python
## 오차 계산
loss = loss_fc(pred, targets) # (모델예측결과, 정답)
loss
```




    tensor(18740.0840, grad_fn=<MseLossBackward0>)



## Model Train
주어진 epoch 만큼 학습하는 `fit`  함수를 정의한다.


```python
def fit(num_epochs:'몇번 반복할지', model:'학습시킬모델', loss_fc:'오차함수', optim:'옵티마이저', inputs:'입력데이터', targets:'출력데이터'):
    for epoch in range(num_epochs):
        # 추론
        pred = model(inputs)
        # 오차 계산
        loss = loss_fc(pred, targets)
        # gradient 계산
        loss.backward()
        # 파라미터 update
        optim.step()
        # 계산된 gradient값 초기화
        optim.zero_grad()
        ### loss(결과) 출력
        if epoch % 10 == 0 or epoch == (num_epochs-1):
            print(f'Step: {epoch}/{num_epochs}: train loss:{loss.item():.5f}')
```


```python
fit(200, model, loss_fc, optimizer, inputs, targets)
```

    Step: 0/200: train loss:18740.08398
    Step: 10/200: train loss:393.48126
    Step: 20/200: train loss:71.74210
    Step: 30/200: train loss:26.01782
    Step: 40/200: train loss:12.91758
    Step: 50/200: train loss:8.43894
    Step: 60/200: train loss:6.44737
    Step: 70/200: train loss:5.26142
    Step: 80/200: train loss:4.40389
    Step: 90/200: train loss:3.72701
    Step: 100/200: train loss:3.17505
    Step: 110/200: train loss:2.71996
    Step: 120/200: train loss:2.34337
    Step: 130/200: train loss:2.03138
    Step: 140/200: train loss:1.77280
    Step: 150/200: train loss:1.55844
    Step: 160/200: train loss:1.38076
    Step: 170/200: train loss:1.23347
    Step: 180/200: train loss:1.11136
    Step: 190/200: train loss:1.01015
    Step: 199/200: train loss:0.93394



```python
# 학습후 모델 추론 결과 확인
p = model(inputs)
p
```




    tensor([[ 57.2727,  70.3651],
            [ 81.8452, 100.1246],
            [119.3441, 134.1090],
            [ 21.2584,  37.3385],
            [101.2700, 117.9887]], grad_fn=<AddmmBackward0>)




```python
l = loss_fc(p, targets)
l.item()
```




    0.9262372255325317




```python
targets
```




    tensor([[ 56.,  70.],
            [ 81., 101.],
            [119., 133.],
            [ 22.,  37.],
            [103., 119.]])


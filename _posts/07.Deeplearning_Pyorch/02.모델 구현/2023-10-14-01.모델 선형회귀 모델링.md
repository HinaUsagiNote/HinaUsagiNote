---
layout: single
title: '모델 선형회귀 모델링'
typora-root-url: ../
categories: Deeplearning_Pytorch.02.모델 선형회귀 모델
tag: Pytorch
toc: true
---

# 데이터 확인

- 입력데이터: 공부시간
- 출력데이터: 성적

|공부시간|점수|
|-|-|
|1|20|
|2|40|
|3|60|

우리가 수집한 공부시간과 점수 데이터를 바탕으로 둘 간의 관계를 식으로 정의 할 수 있으면 **내가 몇시간 공부하면 점수를 얼마 받을 수 있는지 예측할 수 있게 된다.**  



## 학습(훈련) 데이터셋 만들기
- 모델을 학습시키기 위한 데이터셋을 구성한다.
- 입력데이터와 출력데이터을 따로 행렬로 구성한다.
- 같은 데이터 포인트의 입력, 출력 데이터를 같은 index에 정의한다.

# 모델링

## 모델 정의

- Feature와 Target간의 관계를 수식으로 정의한다.
- 여기서는 공부시간(Feature)와 점수(Target)간의 관계를 정의하는데 **선형회귀(Linear Regression) 모델** 을 가설로 세우고 모델링을 한다.  

### 선형회귀 (Linear Regression)
- Feature들의 가중합을 이용해 Target을 추정한다.
- Feature에 곱해지는 가중치(weight)들은 각 Feature가 Target 얼마나 영향을 주는지 영향도가 된다.
    - 음수일 경우는 target값을 줄이고, 양수일 경우는 target값을 늘린다.
    - 가중치가 0에 가까울 수록 target에 영향을 주지 않는 feature
    - 0에서 멀수록 target에 많은 영향을 주는 target이 된다.
          

$$
\hat{y} = Wx + b
$$
<center>$\hat{y}$: 모델추정값<br>W: 가중치<br>x: Feature<br>b: bias(편향)</center>




```python
import torch
```


```python
# 훈련(train)데이터 정의
## 입력데이터 변수명: X, 출력데이터 변수명: y
X_train = torch.tensor([[1],[2],[3]], dtype=torch.float32)
y_train = torch.tensor([[20],[40],[60]], dtype=torch.float32)
```


```python
print(X_train.shape, y_train.shape)
# [3, 1] -> [데이터개수, 개별데이터의 shape] => 3개의 데이터. 각데이터는 1개의 값을 구성된 1차원 배열
```

    torch.Size([3, 1]) torch.Size([3, 1])



```python
#### 모델정의
torch.manual_seed(0)

## weight와 bias를 정의
weight = torch.rand(1, 1, requires_grad=True) # (1: 입력 값의개수, 1: 출력 값의개수)
bias = torch.rand(1, requires_grad=True)

print('초기 파라미터')
print('weight:', weight)
print('bias:', bias)

def linear_model(X):
    pred = X @ weight + bias
    return pred
```

    초기 파라미터
    weight: tensor([[0.4963]], requires_grad=True)
    bias: tensor([0.7682], requires_grad=True)



```python
### 예측
pred_train = linear_model(X_train)
pred_train
```




    tensor([[1.2645],
            [1.7607],
            [2.2570]], grad_fn=<AddBackward0>)




```python
# 실제 점수
y_train
```




    tensor([[20.],
            [40.],
            [60.]])




```python
### 오차함수 정의 -> 모델이 추론한 값과 정답사이의 차이를 계산하는 함수
# 평균 제곱 오차 (Mean Squared Error : MSE)
def mse_loss_fn(pred:'예측값', y:'정답'):
    return torch.mean((pred - y)**2)
```


```python
loss = mse_loss_fn(pred_train, y_train)
print('오차:', loss)
```

    오차: tensor(1715.8387, grad_fn=<MeanBackward0>)



```python
### weight와 bias에 대한 gradient 계산
loss.backward()
```


```python
print('weight의 grad:', weight.grad)
print('bias의 grad', bias.grad)
```

    weight의 grad: tensor([[-178.9621]])
    bias의 grad tensor([-76.4785])



```python
### 최적화 -> 경사하강법 ==> 파이토치의 함수를 사용
optimizer = torch.optim.SGD([weight, bias], # 최적화할 대상 => requires_grad=True
                            lr = 0.001,     # 학습률                           
                           )
### 경사하강법 연산.
print('Udate전 weight:', weight)
optimizer.step()
print('Udate후 weight:', weight)
optimizer.zero_grad()
```

    Udate전 weight: tensor([[0.4963]], requires_grad=True)
    Udate후 weight: tensor([[0.6752]], requires_grad=True)



```python
#### 업데이트후 로스계산
## 추론
pred = linear_model(X_train)
## 오차계산
loss2 = mse_loss_fn(pred, y_train)
print('이전', loss)
print('업데이트후', loss2)
```

    이전 tensor(1715.8387, grad_fn=<MeanBackward0>)
    업데이트후 tensor(1678.1724, grad_fn=<MeanBackward0>)


### 학습
1. 모델을 이용해 추정한다.
   - pred = model(input)
1. loss를 계산한다.
   - loss = loss_fn(pred, target)
1. 계산된 loss를 파라미터에 대해 미분하여 계산한 gradient 값을 각 파라미터에 저장한다.
   - loss.backward()
1. optimizer를 이용해 파라미터를 update한다.
   - optimizer.step()  
1. 파라미터의 gradient(미분값)을 0으로 초기화한다.
   - optimizer.zero_grad()
- 위의 단계를 반복한다.   


```python
import torch
```


```python
# 돌려봐야지 알아야한다 -> 많이 돌려도 효율이 나쁠수도 있다.
STEPS = 100 # 파라미터(weight, bias)를 업데이트를 할 횟수
for _ in range(STEPS):
    # 1. 추론(예측)
    pred = linear_model(X_train)
    # 2. 오차계산
    loss = mse_loss_fn(pred, y_train)
    # 3. weight, bias 대한 gradient 계산
    loss.backward()
    # 4. 최적화 -> optimizer를 이용. ==> 경사하강법(알고리즘)
    optimizer.step()
    # 5. 계산된 gradient 값 초기화
    optimizer.zero_grad()
```


```python
### weight, bias 값
print('weight:', weight)
print('bias:', bias)
```

    weight: tensor([[16.2360]], requires_grad=True)
    bias: tensor([7.0396], requires_grad=True)



```python
### 오차 계산
pred3 = linear_model(X_train)
loss3 = mse_loss_fn(pred3, y_train)
```


```python
# torch.sqrt: 제곱근 구하기
print(loss3, torch.sqrt(loss3))
```

    tensor(9.6839, grad_fn=<MeanBackward0>) tensor(3.1119, grad_fn=<SqrtBackward0>)



```python
loss**(1/2)
```




    tensor(3.1200, grad_fn=<PowBackward0>)

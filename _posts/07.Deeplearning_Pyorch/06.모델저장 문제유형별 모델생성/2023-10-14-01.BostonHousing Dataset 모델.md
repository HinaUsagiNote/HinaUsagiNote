---
layout: single
title: 'Boston Housing Dataset 모델'
typora-root-url: ../
categories: Deeplearning_Pytorch.06.모델저장 문제유형별 모델생성
tag: Pytorch
toc: true
---

# Boston Housing Dataset

보스턴 주택가격 dataset은 다음과 같은 속성을 바탕으로 해당 타운 주택 가격의 중앙값을 예측하는 문제.
- CRIM: 범죄율
- ZN: 25,000 평방피트당 주거지역 비율
- INDUS: 비소매 상업지구 비율
- CHAS: 찰스강에 인접해 있는지 여부(인접:1, 아니면:0)
- NOX: 일산화질소 농도(단위: 0.1ppm)
- RM: 주택당 방의 수
- AGE: 1940년 이전에 건설된 주택의 비율
- DIS: 5개의 보스턴 직업고용센터와의 거리(가중 평균)
- RAD: 고속도로 접근성
- TAX: 재산세율
- PTRATIO: 학생/교사 비율
- B: 흑인 비율
- LSTAT: 하위 계층 비율
<br><br>
- **Target**
    - MEDV: 타운의 주택가격 중앙값(단위: 1,000달러)

## Import


```python
!pip install scikit-learn
```

    Requirement already satisfied: scikit-learn in c:\users\world\anaconda3\envs\torch\lib\site-packages (1.3.1)
    Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\users\world\anaconda3\envs\torch\lib\site-packages (from scikit-learn) (1.26.0)
    Requirement already satisfied: scipy>=1.5.0 in c:\users\world\anaconda3\envs\torch\lib\site-packages (from scikit-learn) (1.11.3)
    Requirement already satisfied: joblib>=1.1.1 in c:\users\world\anaconda3\envs\torch\lib\site-packages (from scikit-learn) (1.3.2)
    Requirement already satisfied: threadpoolctl>=2.0.0 in c:\users\world\anaconda3\envs\torch\lib\site-packages (from scikit-learn) (3.2.0)



```python
import torch
import torch.nn as nn
import torchinfo
from torch.utils.data import DataLoader, Dataset, TensorDataset

from sklearn.model_selection import train_test_split # (x, y) = (x1, x2), (y1, y2) 튜플
from sklearn.preprocessing import StandardScaler

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import time
```


```python
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(device)
```

    cpu


## Dataset, DataLoader 생성


```python
# trainset, testset
# train Dataloader, test Dataloader
```


```python
boston = pd.read_csv('data/boston_housing.csv')
print(boston.shape)
print(boston.info())
```

    (506, 14)
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 506 entries, 0 to 505
    Data columns (total 14 columns):
     #   Column   Non-Null Count  Dtype  
    ---  ------   --------------  -----  
     0   CRIM     506 non-null    float64
     1   ZN       506 non-null    float64
     2   INDUS    506 non-null    float64
     3   CHAS     506 non-null    float64
     4   NOX      506 non-null    float64
     5   RM       506 non-null    float64
     6   AGE      506 non-null    float64
     7   DIS      506 non-null    float64
     8   RAD      506 non-null    float64
     9   TAX      506 non-null    float64
     10  PTRATIO  506 non-null    float64
     11  B        506 non-null    float64
     12  LSTAT    506 non-null    float64
     13  MEDV     506 non-null    float64
    dtypes: float64(14)
    memory usage: 55.5 KB
    None



```python
boston.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00632</td>
      <td>18.0</td>
      <td>2.31</td>
      <td>0.0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1.0</td>
      <td>296.0</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.02731</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.02729</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.03237</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.06905</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>396.90</td>
      <td>5.33</td>
      <td>36.2</td>
    </tr>
  </tbody>
</table>
</div>




```python
# X(input data, feature), y(output data, target, label)을 분리
## datafram/series.values => ndarray로 변환
X_boston = boston.drop(columns='MEDV').values
y_boston = boston['MEDV'].to_frame().values
X_boston.shape, y_boston.shape
```




    ((506, 13), (506, 1))




```python
# Trainset / Testset 분리
## X_boston, y_boston을 섞은뒤 0.8 : 0.2 비율로 나눔
X_train, X_test, y_train, y_test = train_test_split(X_boston, y_boston, test_size=0.2, random_state=0) # random_state: seed값
```


```python
# 회귀 (정답이 연속형-다 다른값) ==> stratify=y 를 설정하지 않는다 (분류는 필수)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
```

    (404, 13) (102, 13) (404, 1) (102, 1)



```python
# Feature scaling => 컬럼들의 scaling(척도-단위)를 맞춰주는 작업.
## 단순 값의 크기에 따라 모델링이 되지 않도록 처리.
# StandarScaler => 모든 컬럼의 척도를 평균-0, 표준편차-1 로 맞춤
# Feature scaling은 Train set의 평균과 표준편차를 이용해 Trainset/Testset의 값들에 적용
X_train_mean = X_train.mean(axis=0) # 컬럼별 평균
X_train_std = X_train.std(axis=0)   # 컬럼별 표준편차
X_train_scaled_raw = (X_train - X_train_mean) / X_train_std # (각원소값 - 평균) / 표준편차 => 표준점수(Z score)
```


```python
X_train_scaled_raw.mean(axis=0)
X_train_scaled_raw.std(axis=0)
```




    array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])




```python
# Sklearn을 이용한 Standard Scaling 처리
scaler = StandardScaler()
scaler.fit(X_train) # 어떻게 변환할지 학습 -> 평균/표준편차 계산
X_train_scaled = scaler.transform(X_train) # 변환.
X_test_scaled = scaler.transform(X_test)   # X_train의 평균/표준편차 기준으로 testset도 변환 => 모델의 좀더 정확하게 평가하기 위해
```


```python
X_train_scaled.mean(axis=0)
X_train_scaled.std(axis=0)
```




    array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])




```python
# train set: 모델을 학습 ==> 이전(과거) 데이터를 대표하는 샘플
# test set: 모델을 평가 ==> 앞으로 예측할(미래) 데이터를 대표하는 샘플
```


```python
# X, y: ndarray => torch.Tensor 변환
X_train_scaled = torch.tensor(X_train_scaled, dtype=torch.float32)
X_test_scaled = torch.tensor(X_test_scaled, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32)
```


```python
# Dataset 생성
boston_trainset = TensorDataset(X_train_scaled, y_train)
boston_testset = TensorDataset(X_test_scaled, y_test)
print('Dataset의 데이터개수:','Trainset', len(boston_trainset), 'Testset:',len(boston_testset))
boston_trainset[0]
```

    Dataset의 데이터개수: Trainset 404 Testset: 102





    (tensor([-0.3726, -0.4996, -0.7049,  3.6645, -0.4249,  0.9357,  0.6937, -0.4372,
             -0.1622, -0.5617, -0.4846,  0.3717, -0.4110]),
     tensor([26.7000]))




```python
# Dataloader 생성
```


```python
boston_trainloader = DataLoader(boston_trainset, batch_size=200, shuffle=True, drop_last=True)
boston_testloader = DataLoader(boston_testset, batch_size=len(boston_testset))
```


```python
print('epoch당 step수:', len(boston_trainloader), len(boston_testloader))
```

    epoch당 step수: 2 1


# 모델 저장

## 모델 전체 저장 및 불러오기
- 모델구조, 파라미터 저장



```python
X_boston.shape
# 200, 13 -> 13 x 32(lr1 layer)
```




    (506, 13)




```python
y_boston.shape
# Target -> MEDV: 예측값 1
```




    (506, 1)




```python
class BostonModel(nn.Module):

    def __init__(self):
        # nn.Module의 __init__() 실행 => 초기화
        super().__init__()
        # forward propagation(예측) 할때 필요한 Layer들 생성

        # 입력 feature: 13, 출력 feature: 32 => weight: 13(weight수) x 32(unit수)
        self.lr1 = nn.Linear(in_features=13, out_features=32)
        self.lr2 = nn.Linear(32, 16)
        # lr3 -> 출력 layer: out feature = 모델이 출력해야할 값의 개수에 맞춰준다.
        self.lr3 = nn.Linear(16, 1) # 집값(중앙값) 하나를 예측해야 하므로 1로 설정
    
    def forward(self, X):  
        out = self.lr1(X)     # 선형
        out = nn.ReLU()(out)  # 비선형
        
        out = self.lr2(out)     # 선형
        out = nn.ReLU()(out)  # 비선형
        
        out = self.lr3(out)     # 출력 레이어(이 값이 모델의 예측값)
        # 회귀의 출력결과에는 Activation 함수를 적용하지 않는다.
        #  예외: 출력값의 범위가 정해져 잇고 그 범위값을 출력하는 함수가 있을경우에는 적용가능
        #    범위: 0 ~ 1 -> Logistic (nn.Sigmoid())
        #         -1 ~ 1 -> tanh (nn.Tanh())
        return out

```


```python
# 모델 생성
boston_model = BostonModel()
# 모델 구조 확인
print(boston_model) # attribute로 설정된 layer들을 확인
```

    BostonModel(
      (lr1): Linear(in_features=13, out_features=32, bias=True)
      (lr2): Linear(in_features=32, out_features=16, bias=True)
      (lr3): Linear(in_features=16, out_features=1, bias=True)
    )



```python
# 모델 구조 확인 - 연산 흐름 + output의 shape 등등
## (모델, 입력데이터 shape(batch_size, features))
torchinfo.summary(boston_model, (100,13))
```




    ==========================================================================================
    Layer (type:depth-idx)                   Output Shape              Param #
    ==========================================================================================
    BostonModel                              [100, 1]                  --
    ├─Linear: 1-1                            [100, 32]                 448
    ├─Linear: 1-2                            [100, 16]                 528
    ├─Linear: 1-3                            [100, 1]                  17
    ==========================================================================================
    Total params: 993
    Trainable params: 993
    Non-trainable params: 0
    Total mult-adds (M): 0.10
    ==========================================================================================
    Input size (MB): 0.01
    Forward/backward pass size (MB): 0.04
    Params size (MB): 0.00
    Estimated Total Size (MB): 0.05
    ==========================================================================================



# 학습(Train)
- 학습 + 검증


```python
import time
```


```python
# 하이퍼파라미터 (우리가 설정하는 파리머터) 정의
N_EPOCH = 100
LR = 0.001

# 모델 준비
boston_model = boston_model.to(device) # 모델: 1. 생성 2. divice를 설정
# loss 함수 정의 - 회귀: mse
loss_fn = nn.MSELoss()
# optimizer 정의
optimizer = torch.optim.RMSprop(boston_model.parameters(), lr=LR)
# torch.optim 모델에 최적화알고리즘들이 정의. (모델의 파리미터, 학습률)
```


```python
# 에폭별 학습 결과를 저장할 리스트
## train loss와 validation loss를 저장
train_losslist = []
valid_losslist = []
```


```python
s = time.time()
# Train  (학습/훈련)
## 두단계 -> Train + validation => step 별로 train -> epoch 별로 검증.
for epoch in range(N_EPOCH):
    # 한 epoch에 대한 train 코드
    #####################################
    # train - 모델을 train mode로 변환
    #####################################
    boston_model.train() # train 모드 변경
    train_loss = 0.0 # 현재 epoch의 train loss를 저장할 변수
    ### batch 단위로 학습 => step
    for X, y in boston_trainloader:
        # 한 STEP에 대한 train 코드
        
        # 1. X, y 를 device 옮긴다. => 모델과 동일한 device에 위치시킨다.
        X, y = X.to(device), y.to(device)
        
        # 2. 모델 추정(예측) => forward propagation
        pred = boston_model(X)

        # 3. loss 계산
        loss = loss_fn(pred, y) # 오차계산 -> grad_fn

        # 파라미터 업데이트
        # 4. 파라미터 초기화
        optimizer.zero_grad()
        # 5. back propagation -> 파라미터들의 weight와 파라미터들의 gradient값들을 계산
        loss.backward() # 모든 weight와 bias에 대한 loss의 gradient들을 구한다. - 각 변수에 grad 속성에 저장
    
        # 6. 파리미터 업데이트
        optimizer.step()

        # 7. 현 step의 loss를 train_loss에 누적
        train_loss += loss.item()
    
    # train_loss의 젠체 평균을 계산 (step별 평균loss의 합계 -> step수로 나눠서 전체 평균으로 계산)
    train_loss /= len(boston_trainloader) # step수로 나눈기

    ############################################
    # validation - 모델을 평가(eval) mode로 변경
    #            - 검증, 평가, 서비스 할때
    #            - validation/test dataset으로 모델을 평가
    ############################################
    boston_model.eval() # 평가 모드 변경.
    # 검증 loss를 저장할 변수
    valid_loss = 0.0
    # 검증은 gradient 계산할 필요없음 forward propagation시 도함수 구할 필요없음
    with torch.no_grad():
        for X_valid, y_valid in boston_testloader:
            # 1. X, y device 이동
            X_valid, y_valid = X_valid.to(device), y_valid.to(device)

            # 2. 모델을 이용해 예측
            pred_valid = boston_model(X_valid)

            # 3. 평가 - MSE
            valid_loss += loss_fn(pred_valid, y_valid).item()
            
        # valid_loss 평균
        valid_loss /= len(boston_testloader) # valid_loss / step 수로 나눔
    
    # 현 epoch 에 대한 학습 결과 로그 출력
    print(f"[{epoch+1}/{N_EPOCH}] Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}")
    train_losslist.append(train_loss)
    valid_losslist.append(valid_loss)

e = time.time()
```

    [1/100] Train Loss: 600.1550, Valid Loss: 573.5142
    [2/100] Train Loss: 593.9623, Valid Loss: 567.0342
    [3/100] Train Loss: 575.2606, Valid Loss: 560.3853
    [4/100] Train Loss: 580.3380, Valid Loss: 553.0983
    [5/100] Train Loss: 573.6417, Valid Loss: 545.2942
    [6/100] Train Loss: 564.1140, Valid Loss: 536.8491
    [7/100] Train Loss: 553.2998, Valid Loss: 527.7004
    [8/100] Train Loss: 541.7245, Valid Loss: 517.7957
    [9/100] Train Loss: 531.5670, Valid Loss: 507.1731
    [10/100] Train Loss: 514.9458, Valid Loss: 496.0745
    [11/100] Train Loss: 507.3479, Valid Loss: 484.3022
    [12/100] Train Loss: 493.2969, Valid Loss: 472.0690
    [13/100] Train Loss: 476.8092, Valid Loss: 459.5027
    [14/100] Train Loss: 465.6772, Valid Loss: 446.5804
    [15/100] Train Loss: 451.4950, Valid Loss: 433.3291
    [16/100] Train Loss: 431.5264, Valid Loss: 420.0411
    [17/100] Train Loss: 421.0817, Valid Loss: 406.4973
    [18/100] Train Loss: 407.7126, Valid Loss: 392.8791
    [19/100] Train Loss: 393.8998, Valid Loss: 379.2253
    [20/100] Train Loss: 371.6994, Valid Loss: 365.6301
    [21/100] Train Loss: 361.5108, Valid Loss: 351.9666
    [22/100] Train Loss: 346.4126, Valid Loss: 338.3172
    [23/100] Train Loss: 331.2187, Valid Loss: 324.7607
    [24/100] Train Loss: 316.5856, Valid Loss: 311.2226
    [25/100] Train Loss: 301.8727, Valid Loss: 297.6621
    [26/100] Train Loss: 286.5567, Valid Loss: 284.0104
    [27/100] Train Loss: 271.9386, Valid Loss: 270.5645
    [28/100] Train Loss: 256.5018, Valid Loss: 257.6515
    [29/100] Train Loss: 241.2394, Valid Loss: 245.3272
    [30/100] Train Loss: 228.8894, Valid Loss: 233.4340
    [31/100] Train Loss: 213.0430, Valid Loss: 222.2762
    [32/100] Train Loss: 203.7046, Valid Loss: 211.6454
    [33/100] Train Loss: 192.2370, Valid Loss: 201.6129
    [34/100] Train Loss: 180.9327, Valid Loss: 192.1906
    [35/100] Train Loss: 171.0233, Valid Loss: 183.3995
    [36/100] Train Loss: 160.3613, Valid Loss: 175.2178
    [37/100] Train Loss: 153.2592, Valid Loss: 167.5822
    [38/100] Train Loss: 143.9723, Valid Loss: 160.5292
    [39/100] Train Loss: 138.0382, Valid Loss: 153.8964
    [40/100] Train Loss: 128.9126, Valid Loss: 147.8072
    [41/100] Train Loss: 122.6429, Valid Loss: 142.1461
    [42/100] Train Loss: 116.6866, Valid Loss: 136.8901
    [43/100] Train Loss: 111.7724, Valid Loss: 131.9608
    [44/100] Train Loss: 106.7711, Valid Loss: 127.3790
    [45/100] Train Loss: 102.5910, Valid Loss: 123.1281
    [46/100] Train Loss: 98.4524, Valid Loss: 119.1372
    [47/100] Train Loss: 93.4276, Valid Loss: 115.4497
    [48/100] Train Loss: 89.1211, Valid Loss: 112.0199
    [49/100] Train Loss: 87.2501, Valid Loss: 108.7361
    [50/100] Train Loss: 83.6961, Valid Loss: 105.6922
    [51/100] Train Loss: 80.2742, Valid Loss: 102.8564
    [52/100] Train Loss: 77.3801, Valid Loss: 100.2052
    [53/100] Train Loss: 75.2953, Valid Loss: 97.6860
    [54/100] Train Loss: 72.1153, Valid Loss: 95.3285
    [55/100] Train Loss: 70.1475, Valid Loss: 93.0888
    [56/100] Train Loss: 67.3530, Valid Loss: 90.9881
    [57/100] Train Loss: 65.4418, Valid Loss: 89.0203
    [58/100] Train Loss: 64.3030, Valid Loss: 87.1254
    [59/100] Train Loss: 62.0888, Valid Loss: 85.3297
    [60/100] Train Loss: 60.0461, Valid Loss: 83.6141
    [61/100] Train Loss: 58.7016, Valid Loss: 82.0145
    [62/100] Train Loss: 56.8409, Valid Loss: 80.4684
    [63/100] Train Loss: 54.9972, Valid Loss: 79.0282
    [64/100] Train Loss: 53.7664, Valid Loss: 77.6330
    [65/100] Train Loss: 52.5459, Valid Loss: 76.3065
    [66/100] Train Loss: 51.4672, Valid Loss: 75.0644
    [67/100] Train Loss: 50.3485, Valid Loss: 73.8415
    [68/100] Train Loss: 49.0947, Valid Loss: 72.6996
    [69/100] Train Loss: 47.5436, Valid Loss: 71.6079
    [70/100] Train Loss: 46.8053, Valid Loss: 70.5447
    [71/100] Train Loss: 45.9745, Valid Loss: 69.5395
    [72/100] Train Loss: 44.3536, Valid Loss: 68.5785
    [73/100] Train Loss: 43.8245, Valid Loss: 67.6423
    [74/100] Train Loss: 40.0232, Valid Loss: 66.8605
    [75/100] Train Loss: 42.0042, Valid Loss: 66.0171
    [76/100] Train Loss: 41.6055, Valid Loss: 65.2004
    [77/100] Train Loss: 40.6308, Valid Loss: 64.3939
    [78/100] Train Loss: 39.9125, Valid Loss: 63.6505
    [79/100] Train Loss: 39.0317, Valid Loss: 62.9366
    [80/100] Train Loss: 37.9607, Valid Loss: 62.2538
    [81/100] Train Loss: 37.7710, Valid Loss: 61.5603
    [82/100] Train Loss: 37.1612, Valid Loss: 60.8965
    [83/100] Train Loss: 36.4835, Valid Loss: 60.2821
    [84/100] Train Loss: 35.9584, Valid Loss: 59.6887
    [85/100] Train Loss: 35.4031, Valid Loss: 59.1032
    [86/100] Train Loss: 34.9103, Valid Loss: 58.5452
    [87/100] Train Loss: 34.3117, Valid Loss: 57.9938
    [88/100] Train Loss: 33.8069, Valid Loss: 57.4671
    [89/100] Train Loss: 32.9687, Valid Loss: 56.9693
    [90/100] Train Loss: 32.7487, Valid Loss: 56.5209
    [91/100] Train Loss: 32.0507, Valid Loss: 56.0449
    [92/100] Train Loss: 31.9455, Valid Loss: 55.5677
    [93/100] Train Loss: 31.3038, Valid Loss: 55.1314
    [94/100] Train Loss: 31.0673, Valid Loss: 54.7330
    [95/100] Train Loss: 30.4065, Valid Loss: 54.3093
    [96/100] Train Loss: 30.2115, Valid Loss: 53.8882
    [97/100] Train Loss: 30.0226, Valid Loss: 53.4680
    [98/100] Train Loss: 29.5780, Valid Loss: 53.0947
    [99/100] Train Loss: 29.2717, Valid Loss: 52.7552
    [100/100] Train Loss: 28.8061, Valid Loss: 52.3957



```python
# 회귀 -> loss: mse 평가: mse, rmse (Root Mean squared error)
print('걸린시간:', (e-s), '초')
```

    걸린시간: 0.8266639709472656 초



```python
# train loss, valid loss 의 epoch 별 변화의 흐름 시각화.
plt.plot(range(1, N_EPOCH+1), train_losslist, label='Train Loss')
plt.plot(range(1, N_EPOCH+1), valid_losslist, label='Validation Loss')

plt.xlabel('EPOCH')
plt.ylabel('Loss')
plt.legend()
plt.show()
```


![png](/../../images/2023-10-14-01.모델저장 문제유형별 모델생성/output_51_0.png)
    



```python
train_losslist[-1] ** (1/2), valid_losslist[-1]**(1/2)
```




    (5.367135437736487, 7.238484939204328)



## state_dict 저장 및 로딩
- 모델 파라미터만 저장


```python
save_path = 'models/boston_model.pth'
torch.save(boston_model, save_path)
```


```python
load_boston_model = torch.load(save_path)
torchinfo.summary(load_boston_model, (200, 13))
```




    ==========================================================================================
    Layer (type:depth-idx)                   Output Shape              Param #
    ==========================================================================================
    BostonModel                              [200, 1]                  --
    ├─Linear: 1-1                            [200, 32]                 448
    ├─Linear: 1-2                            [200, 16]                 528
    ├─Linear: 1-3                            [200, 1]                  17
    ==========================================================================================
    Total params: 993
    Trainable params: 993
    Non-trainable params: 0
    Total mult-adds (M): 0.20
    ==========================================================================================
    Input size (MB): 0.01
    Forward/backward pass size (MB): 0.08
    Params size (MB): 0.00
    Estimated Total Size (MB): 0.09
    ==========================================================================================




```python
# 예측 서비스
new_X = torch.cat([boston_testset[0][0], boston_testset[1][0]])
new_X = new_X.reshape(-1, 13)
new_X.shape # [2: 데이터수, 13: frature수]
```




    torch.Size([2, 13])




```python
boston_testset[0][0] # 첫번째 데이터 => X, y
boston_testset[1][0]
torch.cat([boston_testset[0][0], boston_testset[1][0]]).reshape(-1, 13)
```




    tensor([[-0.4084, -0.4996, -1.1287, -0.2729, -0.8334,  0.0450, -1.8462,  0.6951,
             -0.6246,  0.1591, -0.7127,  0.1855, -0.7361],
            [ 0.7193, -0.4996,  0.9989, -0.2729,  0.6528, -0.1237,  1.1033, -1.2517,
              1.6874,  1.5421,  0.7927,  0.0832, -0.4357]])




```python
# 예측값
pred_new = load_boston_model(new_X)
print(pred_new) 
```

    tensor([[24.8984],
            [17.9753]], grad_fn=<AddmmBackward0>)



```python
# 정답
boston_testset[0][1], boston_testset[1][1]
```




    (tensor([22.6000]), tensor([50.]))




```python
# 오차
loss_fn(pred_new[0], boston_testset[0][1])
```




    tensor(5.2826, grad_fn=<MseLossBackward0>)




```python
loss_fn(pred_new[1], boston_testset[1][1])
```




    tensor(1025.5785, grad_fn=<MseLossBackward0>)



# state_dict 저장 및 로딩
- 모델 파라미터만 저장


```python
save_path2 = 'models/boston_model_statedict.pth'
# 모델에서 state_dict 조회
model_sd = boston_model.state_dict()
torch.save(model_sd, save_path2)
```


```python
# type(model_sd), model_sd.keys()
# model_sd['lr1.weight'].shape
# model_sd['lr1.weight'][0]
```


```python
# 불러오기 
## state_dict를 불러오기
load_sd = torch.load(save_path2)
type(load_sd)
```




    collections.OrderedDict




```python
# 새로운 모델을 생성한 뒤에 파리미터를 변경
new_model = BostonModel()
new_model.load_state_dict(load_sd)
```




    <All keys matched successfully>




```python
pred_new2 = new_model(new_X)
pred_new2
```




    tensor([[24.8984],
            [17.9753]], grad_fn=<AddmmBackward0>)

---
layout: single
title: 'breast_cancer 모델'
typora-root-url: ../
categories: Deeplearning_Pytorch.05.DatasetDataLoader
tag: Pytorch
toc: true
---

# 분류 (Classification)
- 예측할 값이 정해져 잇는 경우 => 범주형인 경우
- 다중분류
    - 범주값(class)가 여러인 경우
- 이진 분류
    - 범주값: 0/1 => 맞는지 틀린지를 추정문제.
    - 맞는것: Posivitve -> 1
    - 틀린것: Negative -> 0

## Fashion MNIST Dataset - 다중분류(Multi-Class Classification) 문제

10개의 범주(category)와 70,000개의 흑백 이미지로 구성된 [패션 MNIST](https://github.com/zalandoresearch/fashion-mnist) 데이터셋. 
이미지는 해상도(28x28 픽셀)가 낮고 다음처럼 개별 의류 품목을 나타낸다:

<table>
  <tr><td>
    <img src="https://tensorflow.org/images/fashion-mnist-sprite.png"
         alt="Fashion MNIST sprite"  width="600">
  </td></tr>
  <tr><td align="center">
    <b>그림</b> <a href="https://github.com/zalandoresearch/fashion-mnist">패션-MNIST 샘플</a> (Zalando, MIT License).<br/>&nbsp;
  </td></tr>
</table>

이미지는 28x28 크기이며 Gray scale이다. *레이블*(label)은 0에서 9까지의 정수 배열이다. 아래 표는 이미지에 있는 의류의 **클래스**(class)들이다.

<table>
  <tr>
    <th>레이블</th>
    <th>클래스</th>
  </tr>
  <tr>
    <td>0</td>
    <td>T-shirt/top</td>
  </tr>
  <tr>
    <td>1</td>
    <td>Trousers</td>
  </tr>
    <tr>
    <td>2</td>
    <td>Pullover</td>
  </tr>
    <tr>
    <td>3</td>
    <td>Dress</td>
  </tr>
    <tr>
    <td>4</td>
    <td>Coat</td>
  </tr>
    <tr>
    <td>5</td>
    <td>Sandal</td>
  </tr>
    <tr>
    <td>6</td>
    <td>Shirt</td>
  </tr>
    <tr>
    <td>7</td>
    <td>Sneaker</td>
  </tr>
    <tr>
    <td>8</td>
    <td>Bag</td>
  </tr>
    <tr>
    <td>9</td>
    <td>Ankle boot</td>
  </tr>
</table>


```python
import torch
import torch.nn as nn
import torchinfo
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

import numpy as np
import matplotlib.pyplot as plt

import time
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(device)
```

    cpu



```python
# Dataset, DataLoader 생성
## Built in Dataset
fmnist_trainset = datasets.FashionMNIST(root='datasets', train=True, download=True, transform=transforms.ToTensor())
fmnist_testset = datasets.FashionMNIST(root='datasets', train=False, download=True, transform=transforms.ToTensor())
```


```python
# 데이터 개수
print('train 개수:', len(fmnist_trainset))
print('test 개수:', len(fmnist_testset))
```

    train 개수: 60000
    test 개수: 10000



```python
print(fmnist_trainset)
print()
print(fmnist_testset)
```

    Dataset FashionMNIST
        Number of datapoints: 60000
        Root location: datasets
        Split: Train
        StandardTransform
    Transform: ToTensor()
    
    Dataset FashionMNIST
        Number of datapoints: 10000
        Root location: datasets
        Split: Test
        StandardTransform
    Transform: ToTensor()



```python
# index to class
index_to_class = np.array(fmnist_trainset.classes) # fany indexing을 위해서 list->ndarray
index_to_class[[1, 1, 2, 3, 0]]
```




    array(['Trouser', 'Trouser', 'Pullover', 'Dress', 'T-shirt/top'],
          dtype='<U11')




```python
# class to index
class_to_index = fmnist_trainset.class_to_idx
class_to_index
```




    {'T-shirt/top': 0,
     'Trouser': 1,
     'Pullover': 2,
     'Dress': 3,
     'Coat': 4,
     'Sandal': 5,
     'Shirt': 6,
     'Sneaker': 7,
     'Bag': 8,
     'Ankle boot': 9}




```python
# 이미지 확인
idx = 0 
x, y = fmnist_trainset[idx] # Dataset[i]: (X, y)
plt.imshow(x[0], cmap='gray') # Greys: 흑백 반전
plt.title(index_to_class[y])
plt.show()
```


![png](/../../images/2023-10-14-01.모델저장 문제유형별 모델생성/output_77_0.png)
    



```python
# x.shape: (channel: 1, height, weidth)
x.shape
```




    torch.Size([1, 28, 28])




```python
y
```




    9




```python
index_to_class.shape
```




    (10,)




```python
# DataLoader
fmnist_trainloader = DataLoader(fmnist_trainset, batch_size=128, shuffle=True, drop_last=True)
fmnist_testloader = DataLoader(fmnist_testset, batch_size=128)
```


```python
# 1 Epoch당 Step 수
f'1 EPOCH당 Step수 Trainset: {len(fmnist_trainloader)}, Testsetset: {len(fmnist_testloader)}'
```




    '1 EPOCH당 Step수 Trainset: 468, Testsetset: 79'




```python
# 모델 정의
class FashionMNISTModel(nn.Module):

    def __init__(self):
        super().__init__()
        # 입력 이미지를 받아서 처리후 리턴
        ## 이미지는 pixcel 크기때문에 처음 특징은 크게 추출 -> 점점 줄여나간다.
        self.lr1 = nn.Linear(28*28, 2048) # 784 -> 2048 특징추출
        self.lr2 = nn.Linear(2048, 1024)  # 2048 -> 1024
        self.lr3 = nn.Linear(1024, 512)   # 1024 -> 512
        self.lr4 = nn.Linear(512, 256)    # 512 -> 256
        self.lr5 = nn.Linear(256, 128)    # 256 -> 128
        self.lr6 = nn.Linear(128, 64)     # 128 -> 64
        # output - out_features: 다중분류: class 개수 (fashion mnist: 10)
        self.lr7 = nn.Linear(64, 10) # 각 클래스별 확률이 출력되도록 한다.
    
    def forward(self, X):
        # X: (bach, channel. height, weidth) ===> (batch, all_features)
        # out = torch.flatten(X, start_dim=1)
        out = nn.Flatten()(X)
        
        out = nn.ReLU()(self.lr1(out))
        out = nn.ReLU()(self.lr2(out))
        out = nn.ReLU()(self.lr3(out))
        out = nn.ReLU()(self.lr4(out))
        out = nn.ReLU()(self.lr5(out))
        out = nn.ReLU()(self.lr6(out))
        # output
        ## CrossEntropyLoss 내부적으로 SoftMax(확률적으로 바꿈)
        out = self.lr7(out)
        return out

```


```python
# 모델 생성
f_model = FashionMNISTModel()
print(f_model)
```

    FashionMNISTModel(
      (lr1): Linear(in_features=784, out_features=2048, bias=True)
      (lr2): Linear(in_features=2048, out_features=1024, bias=True)
      (lr3): Linear(in_features=1024, out_features=512, bias=True)
      (lr4): Linear(in_features=512, out_features=256, bias=True)
      (lr5): Linear(in_features=256, out_features=128, bias=True)
      (lr6): Linear(in_features=128, out_features=64, bias=True)
      (lr7): Linear(in_features=64, out_features=10, bias=True)
    )



```python
# (모델, (데이터개수, channel, height, weith))
torchinfo.summary(f_model, (128, 1, 28, 28))
```




    ==========================================================================================
    Layer (type:depth-idx)                   Output Shape              Param #
    ==========================================================================================
    FashionMNISTModel                        [128, 10]                 --
    ├─Linear: 1-1                            [128, 2048]               1,607,680
    ├─Linear: 1-2                            [128, 1024]               2,098,176
    ├─Linear: 1-3                            [128, 512]                524,800
    ├─Linear: 1-4                            [128, 256]                131,328
    ├─Linear: 1-5                            [128, 128]                32,896
    ├─Linear: 1-6                            [128, 64]                 8,256
    ├─Linear: 1-7                            [128, 10]                 650
    ==========================================================================================
    Total params: 4,403,786
    Trainable params: 4,403,786
    Non-trainable params: 0
    Total mult-adds (M): 563.68
    ==========================================================================================
    Input size (MB): 0.40
    Forward/backward pass size (MB): 4.14
    Params size (MB): 17.62
    Estimated Total Size (MB): 22.16
    ==========================================================================================




```python
# 모델 추정결과 형태를 확인
i = torch.ones((2, 1, 28, 28)) # 1 x 28 x 28 이미지 2장
y_hat = f_model(i)
y_hat[0] # 첫번째 이미지에 대한 추론결화
```




    tensor([ 0.0014, -0.0345, -0.0687, -0.0651,  0.1043,  0.0071, -0.0625, -0.0976,
             0.1103,  0.1291], grad_fn=<SelectBackward0>)




```python
y_hat[0].shape
# 정답 class => 예측결과 10중에서 가장 큰값이 있는 index
y_hat.argmax(axis=-1) # 첫번째: 3, 두번째: 3
# index_to_class[y_hat.detach().numpy().argmax(axis=-1)]
# requires_grad=True 인 Tensor를 ndarray를 변환할때는 tensor.detach() 를 한 다음 변환해야한다.
```




    tensor([9, 9])




```python
torch.sum(y_hat.argmax())
```




    tensor(9)




```python
index_to_class
```




    array(['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal',
           'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], dtype='<U11')




```python
# y_hat => 확률값으로 변환 ==> softmax()
y_hat_probabillity = nn.Softmax(dim=-1)(y_hat)
y_hat_probabillity
# 정답의 확률, 정답라벨
y_hat_probabillity.max(dim=-1).values, y_hat_probabillity.argmax(dim=-1)
```




    (tensor([0.1132, 0.1132], grad_fn=<MaxBackward0>), tensor([9, 9]))




```python
a, b = next(iter(fmnist_trainloader))
b
```




    tensor([9, 4, 9, 9, 1, 5, 4, 7, 8, 8, 8, 9, 5, 4, 2, 8, 1, 7, 6, 3, 3, 1, 3, 2,
            9, 0, 5, 7, 1, 3, 2, 4, 1, 0, 5, 5, 9, 0, 3, 8, 8, 2, 7, 3, 1, 7, 7, 9,
            2, 9, 5, 8, 0, 0, 5, 7, 4, 6, 7, 8, 7, 6, 7, 5, 2, 0, 9, 8, 4, 3, 4, 3,
            5, 8, 2, 9, 7, 3, 8, 0, 4, 5, 7, 3, 5, 8, 1, 8, 6, 5, 9, 5, 3, 6, 3, 2,
            3, 9, 6, 9, 1, 5, 7, 9, 4, 9, 5, 8, 4, 9, 6, 7, 4, 8, 3, 4, 2, 1, 6, 2,
            6, 9, 4, 6, 0, 5, 4, 0])




```python
c = f_model(a.to(device))
```


```python
c.shape
```




    torch.Size([128, 10])




```python
d = c.argmax(-1)
```


```python
torch.sum(b == d)/128
```




    tensor(0.1328)




```python
# 학습 (Train)
# 하이퍼파리미터
LR = 0.001
N_EPOCH = 20

# 모델 -> device 옮김
f_model = f_model.to(device)
# loss fn -> 다중분류: nn.CrossEntropyLoss() ==> 다중분류용 Log Loss
loss_fn = nn.CrossEntropyLoss()
# optimizer
optimizer = torch.optim.Adam(f_model.parameters(), lr=LR)
```


```python
# train
import time
# 각 에폭별 학습이 끝나고 모델 평가한 값을 저장.
train_loss_list = []
valid_loss_list = []
valid_acc_list = [] # test set의 정확도 검증 결과 => 전체데이터 중 맞은데이터의 개수
```


```python
s = time.time()
for epoch in range(N_EPOCH):
    ############### train
    f_model.train()
    train_loss = 0.0 # 현재 eopch의 trainset의 loss

s = time.time()
for epoch in range(N_EPOCH):
    ############### train
    f_model.train()
    train_loss = 0.0 # 현재 eopch의 trainset의 loss

    for x, y in fmnist_trainloader:
        x, y = x.to(device), y.to(device) # device 옮기기
        pred = f_model(x) # 예측 - 순전파
        loss = loss_fn(pred, y) # loss 계산 -> (예측값, 정답)
        # 모델 파라미터 업데이트
        optimizer.zero_grad()     # gradient 초기화       
        loss.backward()           # grad 계산 - (오차) 역전파
        optimizer.step()          # 파라미터 업데이트
        train_loss += loss.item() # train loss 누적
    # 1 에폭 학습 종료 => train loss의 평균을 list에 저장
    train_loss /= len(fmnist_trainloader)  # 누적 train loss / step 수
    train_loss_list.append(train_loss)
    
    ############### validation
    f_model.eval()
    valid_loss = 0.0 # 현재 epoch의 대한 검증의 validation loss 저장 변수
    valid_acc = 0.0  # 현재 epoch의 대한 검증의 validation accuracy(정확도) 저장 변수
    
    with torch.no_grad(): # no_grad: 도함수 구할 필요없음
        for x_valid, y_valid in fmnist_testloader:
            x_valid, y_valid = x_valid.to(device), y_valid.to(device)
            #  예측
            pred_valid = f_model(x_valid)          # 라벨별 정답일 가능성 출력 (batch, 10)
            pred_label = pred_valid.argmax(dim=-1) # 정답 class 조회 (pred_valid에서 가장 큰값을 가진 index)
            # 평가 
            ## loss 계산
            loss_valid = loss_fn(pred_valid, y_valid) ## loss_fn() batch만큼 평균을 계산.
            valid_loss += loss_valid
            ## 정확도(accuarcy) 계산
            valid_acc += torch.sum(pred_label == y_valid).item() 
        
        valid_loss /= len(fmnist_testloader)        # step 수로 나눠서 평균 계산
        valid_acc /= len(fmnist_testloader.dataset) # test set의 총 데이터 개수로 나눔
        # 1 eopch에 대한 vudrk dhksfy => valid_loss_list, valid_acc_list 추가
        valid_loss_list.append(valid_loss)
        valid_acc_list.append(valid_acc)
    
    print(f"[{epoch+1}/{N_EPOCH}] Train Loss: {train_loss}, Valid Loss: {valid_loss}, Valid Accuracy: {valid_acc}")
    
e = time.time()
```

    [1/20] Train Loss: 0.6551105150172853, Valid Loss: 0.4565485715866089, Valid Accuracy: 0.8296
    [2/20] Train Loss: 0.3961887560530096, Valid Loss: 0.4160023033618927, Valid Accuracy: 0.8532
    [3/20] Train Loss: 0.3492731497519546, Valid Loss: 0.3938992917537689, Valid Accuracy: 0.8619
    [4/20] Train Loss: 0.3219013679295014, Valid Loss: 0.3694998323917389, Valid Accuracy: 0.8675
    [5/20] Train Loss: 0.2991755286979879, Valid Loss: 0.3386436998844147, Valid Accuracy: 0.8803
    [6/20] Train Loss: 0.28720097568554753, Valid Loss: 0.33928459882736206, Valid Accuracy: 0.8767
    [7/20] Train Loss: 0.2727778639930945, Valid Loss: 0.3285277783870697, Valid Accuracy: 0.8803
    [8/20] Train Loss: 0.25799507870633376, Valid Loss: 0.33903950452804565, Valid Accuracy: 0.8824
    [9/20] Train Loss: 0.24604842200493202, Valid Loss: 0.3337225019931793, Valid Accuracy: 0.8845
    [10/20] Train Loss: 0.2406256765477423, Valid Loss: 0.332624226808548, Valid Accuracy: 0.8871
    [11/20] Train Loss: 0.22591356936300921, Valid Loss: 0.3361697494983673, Valid Accuracy: 0.8854
    [12/20] Train Loss: 0.2181090948641555, Valid Loss: 0.334783673286438, Valid Accuracy: 0.8859
    [13/20] Train Loss: 0.21327709583326793, Valid Loss: 0.3345346450805664, Valid Accuracy: 0.8935
    [14/20] Train Loss: 0.2110095481491751, Valid Loss: 0.3284386992454529, Valid Accuracy: 0.8906
    [15/20] Train Loss: 0.19529994620153537, Valid Loss: 0.3217955529689789, Valid Accuracy: 0.8901
    [16/20] Train Loss: 0.18719645833166745, Valid Loss: 0.3277837634086609, Valid Accuracy: 0.8911
    [17/20] Train Loss: 0.19030732074036047, Valid Loss: 0.3671373426914215, Valid Accuracy: 0.8933
    [18/20] Train Loss: 0.17740353840029138, Valid Loss: 0.3328641355037689, Valid Accuracy: 0.8921
    [19/20] Train Loss: 0.17015816423341504, Valid Loss: 0.33826160430908203, Valid Accuracy: 0.8952
    [20/20] Train Loss: 0.1667192253148836, Valid Loss: 0.3553315997123718, Valid Accuracy: 0.8972



```python
valid_loss_list = [v.item() for v in valid_loss_list]
valid_loss_list
```




    [0.4565485715866089,
     0.4160023033618927,
     0.3938992917537689,
     0.3694998323917389,
     0.3386436998844147,
     0.33928459882736206,
     0.3285277783870697,
     0.33903950452804565,
     0.3337225019931793,
     0.332624226808548,
     0.3361697494983673,
     0.334783673286438,
     0.3345346450805664,
     0.3284386992454529,
     0.3217955529689789,
     0.3277837634086609,
     0.3671373426914215,
     0.3328641355037689,
     0.33826160430908203,
     0.3553315997123718]




```python
print(f'1 EPOCH당 걸린시간: {e - s}')
```

    1 EPOCH당 걸린시간: 822.748521566391



```python
# 결과 시각화
plt.rcParams['font.family'] = 'Malgun gothic'

plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.plot(train_loss_list, label='train')
plt.plot(valid_loss_list, label='Validation')
plt.title('Epoch별 Loss 변화')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(valid_acc_list)
plt.title('Validetion Accuracy')
plt.tight_layout()
plt.show()
```


![output_101_0](/../../images/2023-10-14-01.모델저장 문제유형별 모델생성/output_101_0.png)![png](output_101_0.png)
    



```python
##############################################################################
# 모델 학습이 진행되면 어느 시점부터 성능이 떨어지기 시작
#                    (trainset으로 검증한 결과는 계속 좋아지는데 validation set으로 검증한 결과는 성능이 좋아지다 안좋아지다.)
# 1. 학습도중 성능 개선될 떄마다 저장. (가장 좋은 성능의 모델을 서비스 할수 있게한다.)
# 2. 더이상 성능개선이 안되면 학습을 중지(조기종료)
##############################################################################
```


```python
# 학습 (Train)
# 하이퍼파리미터
LR = 0.001
N_EPOCH = 1000

# 모델 -> device 옮김
f_model = FashionMNISTModel()
f_model = f_model.to(device)
# loss fn -> 다중분류: nn.CrossEntropyLoss() ==> 다중분류용 Log Loss
loss_fn = nn.CrossEntropyLoss()
# optimizer
optimizer = torch.optim.Adam(f_model.parameters(), lr=LR)

######################################
# 조기 종료 + 모델 저장을 위한 변수 추가
######################################
####### 모델 저장을 위한 변수 저장
## 학습 중 가장 좋은 성능 평가지표를 저장. 현 EPOCH의 지표가 이 변수값보다 좋으면 저장
## 평가지표: validation loss
best_score = torch.inf 
save_model_path = 'models/fashion_mnist_best_model.pth'

####### 조기 종료를 위한 변수: 특정 eopch동안 성능 개선이 없으면 학습을 중단
patience = 5 # 성능이 개선 될지를 기달릴 eopch 수. patience 번 만큼 개선이 안되면 중단.(보통 10이상 지정)
trigger_cnt = 0 # 성능 개선을 몇번 째 기다리는 지 정할 변수, patience == trigger_cnt: 중단

s = time.time()
for epoch in range(N_EPOCH):
    ############### train
    f_model.train()
    train_loss = 0.0 # 현재 eopch의 trainset의 loss

    for x, y in fmnist_trainloader:
        x, y = x.to(device), y.to(device) # device 옮기기
        pred = f_model(x) # 예측 - 순전파
        loss = loss_fn(pred, y) # loss 계산 -> (예측값, 정답)
        # 모델 파라미터 업데이트
        optimizer.zero_grad()     # gradient 초기화       
        loss.backward()           # grad 계산 - (오차) 역전파
        optimizer.step()          # 파라미터 업데이트
        train_loss += loss.item() # train loss 누적
    # 1 에폭 학습 종료 => train loss의 평균을 list에 저장
    train_loss /= len(fmnist_trainloader)  # 누적 train loss / step 수
    train_loss_list.append(train_loss)
    
    ############### validation
    f_model.eval()
    valid_loss = 0.0 # 현재 epoch의 대한 검증의 validation loss 저장 변수
    valid_acc = 0.0  # 현재 epoch의 대한 검증의 validation accuracy(정확도) 저장 변수
    
    with torch.no_grad(): # no_grad: 도함수 구할 필요없음
        for x_valid, y_valid in fmnist_testloader:
            x_valid, y_valid = x_valid.to(device), y_valid.to(device)
            #  예측
            pred_valid = f_model(x_valid)          # 라벨별 정답일 가능성 출력 (batch, 10)
            pred_label = pred_valid.argmax(dim=-1) # 정답 class 조회 (pred_valid에서 가장 큰값을 가진 index)
            # 평가 
            ## loss 계산
            loss_valid = loss_fn(pred_valid, y_valid) ## loss_fn() batch만큼 평균을 계산.
            valid_loss += loss_valid
            ## 정확도(accuarcy) 계산
            valid_acc += torch.sum(pred_label == y_valid).item() 
        
        valid_loss /= len(fmnist_testloader)        # step 수로 나눠서 평균 계산
        valid_acc /= len(fmnist_testloader.dataset) # test set의 총 데이터 개수로 나눔
        # 1 eopch에 대한 vudrk dhksfy => valid_loss_list, valid_acc_list 추가
        valid_loss_list.append(valid_loss)
        valid_acc_list.append(valid_acc)
    
    print(f"[{epoch+1}/{N_EPOCH}] Train Loss: {train_loss}, Valid Loss: {valid_loss}, Valid Accuracy: {valid_acc}")
    


    ###############################
    # 조기종료여부, 모델 저장 처리
    #   저장: 현 epoch valid_loss 가 best_score보다 개성도니 경우 저장(작으면 개선)
    ###############################
    if valid_loss < best_score: # 성능이 개선된 경우.
        # 저장로그 출력
        print(f'======> 모델저장: {epoch+1} Epoch - 이전 valid_loss: {best_score}, 현재 valid_loss: {valid_loss}')
        # best_score 교체
        best_score = valid_loss
        # 저장
        torch.save(f_model, save_model_path)
        # trigger_cnt 0으로 초기화
        trigger_cnt = 0
    else: # 성능개선이 안된경우.
        # trigger_cnt 1 증가
        trigger_cnt += 1 
        if patience == trigger_cnt: # patience 만큼 대기 ==> 조기 종료
            # 로그
            print(f'======> {epoch+1} Epoch에서 조기종료-{best_score}에서 개선 안됨')
            break
            
e = time.time()
```

    [1/1000] Train Loss: 0.6529126517538332, Valid Loss: 0.4727691411972046, Valid Accuracy: 0.8294
    ======> 모델저장: 1 Epoch - 이전 valid_loss: inf, 현재 valid_loss: 0.4727691411972046
    [2/1000] Train Loss: 0.40235120112187844, Valid Loss: 0.3901809751987457, Valid Accuracy: 0.8602
    ======> 모델저장: 2 Epoch - 이전 valid_loss: 0.4727691411972046, 현재 valid_loss: 0.3901809751987457
    [3/1000] Train Loss: 0.35432691598295146, Valid Loss: 0.3878174126148224, Valid Accuracy: 0.861
    ======> 모델저장: 3 Epoch - 이전 valid_loss: 0.3901809751987457, 현재 valid_loss: 0.3878174126148224
    [4/1000] Train Loss: 0.32579300019285107, Valid Loss: 0.3973783254623413, Valid Accuracy: 0.8629
    [5/1000] Train Loss: 0.30213577962583965, Valid Loss: 0.362692654132843, Valid Accuracy: 0.869
    ======> 모델저장: 5 Epoch - 이전 valid_loss: 0.3878174126148224, 현재 valid_loss: 0.362692654132843
    [6/1000] Train Loss: 0.2870442514331677, Valid Loss: 0.34652215242385864, Valid Accuracy: 0.8784
    ======> 모델저장: 6 Epoch - 이전 valid_loss: 0.362692654132843, 현재 valid_loss: 0.34652215242385864
    [7/1000] Train Loss: 0.2739411695327005, Valid Loss: 0.3382107615470886, Valid Accuracy: 0.8835
    ======> 모델저장: 7 Epoch - 이전 valid_loss: 0.34652215242385864, 현재 valid_loss: 0.3382107615470886
    [8/1000] Train Loss: 0.2633117822309335, Valid Loss: 0.32794469594955444, Valid Accuracy: 0.882
    ======> 모델저장: 8 Epoch - 이전 valid_loss: 0.3382107615470886, 현재 valid_loss: 0.32794469594955444
    [9/1000] Train Loss: 0.2518994159932829, Valid Loss: 0.3353148102760315, Valid Accuracy: 0.8784
    [10/1000] Train Loss: 0.23975782543739194, Valid Loss: 0.3369169235229492, Valid Accuracy: 0.8855
    [11/1000] Train Loss: 0.22965459651353523, Valid Loss: 0.35423359274864197, Valid Accuracy: 0.8825
    [12/1000] Train Loss: 0.22424998952664882, Valid Loss: 0.32045993208885193, Valid Accuracy: 0.8918
    ======> 모델저장: 12 Epoch - 이전 valid_loss: 0.32794469594955444, 현재 valid_loss: 0.32045993208885193
    [13/1000] Train Loss: 0.21439947546101534, Valid Loss: 0.34093621373176575, Valid Accuracy: 0.8922
    [14/1000] Train Loss: 0.20707031858400402, Valid Loss: 0.3220938444137573, Valid Accuracy: 0.89
    [15/1000] Train Loss: 0.19995766029589707, Valid Loss: 0.3759728670120239, Valid Accuracy: 0.8833
    [16/1000] Train Loss: 0.19504552721404111, Valid Loss: 0.32605788111686707, Valid Accuracy: 0.8944
    [17/1000] Train Loss: 0.18403026546773493, Valid Loss: 0.33446866273880005, Valid Accuracy: 0.8928
    ======> 17 Epoch에서 조기종료-0.32045993208885193에서 개선 안됨



```python
print(f'1 Epoch당 걸린 시간 {e - s}')
```

    1 Epoch당 걸린 시간 739.8945105075836



```python

```


```python
## 저장된 모델 로딩
best_model = torch.load(save_model_path)
```


```python
### test_dataloader로 평가
best_model = best_model.to(device)
best_model.eval()

valid_loss = 0.0 # 현재 epoch의 대한 검증의 validation loss 저장 변수
valid_acc = 0.0  # 현재 epoch의 대한 검증의 validation accuracy(정확도) 저장 변수
    
with torch.no_grad(): # no_grad: 도함수 구할 필요없음
    for x_valid, y_valid in fmnist_testloader:
        x_valid, y_valid = x_valid.to(device), y_valid.to(device)
        #  예측
        pred_valid = best_model(x_valid)          # 라벨별 정답일 가능성 출력 (batch, 10)
        pred_label = pred_valid.argmax(dim=-1) # 정답 class 조회 (pred_valid에서 가장 큰값을 가진 index)
        # 평가 
        ## loss 계산
        loss_valid = loss_fn(pred_valid, y_valid) ## loss_fn() batch만큼 평균을 계산.
        valid_loss += loss_valid
        ## 정확도(accuarcy) 계산
        valid_acc += torch.sum(pred_label == y_valid).item() 
    
    valid_loss /= len(fmnist_testloader)        # step 수로 나눠서 평균 계산
    valid_acc /= len(fmnist_testloader.dataset) # test set의 총 데이터 개수로 나눔
    # 1 eopch에 대한 vudrk dhksfy => valid_loss_list, valid_acc_list 추가
    valid_loss_list.append(valid_loss)
    valid_acc_list.append(valid_acc)
        
```


```python
valid_loss
```




    tensor(0.3205)




```python
valid_acc
```




    0.8918



## 위스콘신 유방암 데이터셋 - 이진분류(Binary Classification) 문제

- **이진 분류 문제 처리 모델의 두가지 방법**
    1. positive(1)일 확률을 출력하도록 구현
        - output layer: units=1, activation='sigmoid'
        - loss: binary_crossentropy
    2. negative(0)일 확률과 positive(1)일 확률을 출력하도록 구현 => 다중분류 처리 방식으로 해결       
        - output layer: units=2, activation='softmax', y(정답)은 one hot encoding 처리
        - loss: categorical_crossentropy
    
- 위스콘신 대학교에서 제공한 종양의 악성/양성여부 분류를 위한 데이터셋
- Feature
    - 종양에 대한 다양한 측정값들
- Target의 class
    - 0 - malignant(악성종양)
    - 1 - benign(양성종양)


```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

import numpy as np
import matplotlib.pyplot as plt
import time
```


```python
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device
```




    'cpu'



### DataSet, DataLoader 생성


```python
x, y = load_breast_cancer(return_X_y=True)
print(type(x), type(y))
print(x.shape, y.shape)
print(np.unique(y))
```

    <class 'numpy.ndarray'> <class 'numpy.ndarray'>
    (569, 30) (569,)
    [0 1]



```python
index_to_class = np.array(['악성', '양성'])
class_to_index = dict(악성=0, 양성=1)
index_to_class, class_to_index
```




    (array(['악성', '양성'], dtype='<U2'), {'악성': 0, '양성': 1})




```python
# y shape을 2차원으로 변경 ==> 모델 출력 shape과 맞춰준다.
# (batch_size, 1)
y = y.reshape(-1, 1)
y.shape
```




    (569, 1)




```python
# train / test set 분리
x_train, x_test, y_train, y_test = train_test_split(x, y, # 나눌 대상 x, y
                                                    test_size = 0.25, # 나눌 비율 => train set: 0.75, test set: 0.25
                                                    stratify=y, # class 별 비율을 맞춰서 나눔
                                                   )
x_train.shape, x_test.shape, y_train.shape, y_test.shape
```




    ((426, 30), (143, 30), (426, 1), (143, 1))




```python
# 전처리 - Feature Scaling (컬럼의 scale(척도, 단위)를 맞춘다.)
## StandarScaler => 평군: 0, 표준편차: 1 을 기준으로 맞춤
scaler = StandardScaler()

x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test) # Trainset으로 fit한 scaler를 이용해 변환
```


```python
# ndarray => Tensor 변환 ==> Dataset 구성 ==> Dataloader 구성
```


```python
# ndarray => Tensor 변환
x_train_tensor = torch.tensor(x_train_scaled, dtype=torch.float32)
x_test_tensor  = torch.tensor(x_test_scaled, dtype=torch.float32)

y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
y_test_tensor  = torch.tensor(y_test, dtype=torch.float32)
```


```python
# Dataset 생성 => TensorDataset
trainset = TensorDataset(x_train_tensor, y_train_tensor)
testset = TensorDataset(x_test_tensor, y_test_tensor)
print(f"DataSet의 데이터개수 TrainSet: {len(trainset)}, TestSet: {len(testset)}")
```

    DataSet의 데이터개수 TrainSet: 426, TestSet: 143



```python
# DataLoader 생성
trainloader = DataLoader(trainset, batch_size=200, shuffle=True, drop_last=True)
testloader = DataLoader(testset, batch_size=len(testset))
print(f"Epoch당 Step 수 TrainLoader: {len(trainloader)}, TestLoader: {len(testloader)}")
```

    Epoch당 Step 수 TrainLoader: 2, TestLoader: 1


### Model 클래스 정의


```python
x_train.shape
```




    (426, 30)




```python
y_test.shape
```




    (143, 1)




```python
class BCModel(nn.Module):

    def __init__(self):
        super().__init__() # nn.Module __init__() 초기화

        self.lr1 = nn.Linear(30, 32)
        self.lr2 = nn.Linear(32, 8)
        self.lr3 = nn.Linear(8, 1) # 출력 Layer: 이진분류 - positive의 확률 값 한개를 출력: out_features=1
        
    def forward(self, x):
        # x (입력) shape: (batchsize, 30)
        out = nn.ReLU()(self.lr1(x))
        out = nn.ReLU()(self.lr2(out))
        # 이진분류 출력값 처리 -> Linear()는 한개의 값을 출력 => 확률값으로 변경 ==> Sigmoid/Logistic 함수를 Activation함수 사용
        out = self.lr3(out) 
        out = nn.Sigmoid()(out) # nn.Sigmoid/Logistic(): 확률값으로 변경. Activation 함수사용
        
        return out
```


```python
model = BCModel()
tmp_x = torch.ones(5, 30)
print(tmp_x.shape)
tmp_y = model(tmp_x)
tmp_y
# 0.4990 -> 1(양성-positive)일 확률
# Tensor객체.tpye(타입을지정) ==> Tensor 데이터타입 변환
# bool -> int: False: 0, True: 1
(tmp_y > 0.5).type(torch.int32) == 0, 1 # label 뽑기
```

    torch.Size([5, 30])





    (tensor([[True],
             [True],
             [True],
             [True],
             [True]]),
     1)




```python
torchinfo.summary(model, (1, 30))
```




    ==========================================================================================
    Layer (type:depth-idx)                   Output Shape              Param #
    ==========================================================================================
    BCModel                                  [1, 1]                    --
    ├─Linear: 1-1                            [1, 32]                   992
    ├─Linear: 1-2                            [1, 8]                    264
    ├─Linear: 1-3                            [1, 1]                    9
    ==========================================================================================
    Total params: 1,265
    Trainable params: 1,265
    Non-trainable params: 0
    Total mult-adds (M): 0.00
    ==========================================================================================
    Input size (MB): 0.00
    Forward/backward pass size (MB): 0.00
    Params size (MB): 0.01
    Estimated Total Size (MB): 0.01
    ==========================================================================================




```python
trainset[0]
```




    (tensor([ 0.4094,  0.1785,  0.4586,  0.2805,  1.4519,  1.1197,  1.3839,  1.1532,
              0.4889,  0.7177,  0.9248,  0.2452,  0.9596,  0.6623,  0.1024,  0.2789,
              0.6563,  0.7683, -0.2065,  0.1975,  1.0319,  0.6142,  1.0585,  0.8590,
              1.6372,  0.9781,  1.4728,  1.5402,  0.5459,  1.0845]),
     tensor([0.]))



### Train(학습/훈련)


```python
# 하이퍼파라미터
LR = 0.001
N_EPOCH = 1000

# 조기 종료
## 모델 저장 변수 
best_score = torch.inf
save_model_path = 'models/bc_best_model.pth'
## 특정 epoch동안 성능개선 없으면 학습중단
patience = 20   # 성능이 개선 될떄 까지 몇 에폭 기다릴 것인지
trigger_cnt = 0 # 성능이 개선 될때 까지 현재 몇번째 기달렸는지

# 모델 생성
model = BCModel().to(device)
# loss 함수
loss_fn = nn.BCELoss() # Binary Cross Entropy Loss
# Optimizer 정의
optimizer = torch.optim.Adam(model.parameters(), lr=LR)
```


```python
##########################################################################
# Epoch 별 검즘 -> Train Loss, Validation Loss, Validation Acccuray
# 조기종료(Enarly Stopping) - 성능 개선이 안되면 학습을 중단
# 가장 좋은 성능을 내는 에폭의 모델을 저장.
# 조기종료/모델 저장 ==> Validation loss 기준
##########################################################################

train_loss_list = []
valid_loss_list = []
valid_acc_list  = []

s = time.time()

for epoch in range(N_EPOCH):
    ######################
    #      Train
    ######################
    model.to(device) 
    train_loss = 0.0

    for x, y in trainloader:
        x, y = x.to(device), y.to(device)

        pred = model(x) # 예측 - 순전파
        loss = loss_fn(pred, y) # loss 계산 -> (예측값, 정답)
        optimizer.zero_grad() # gradient 초기화
        loss.backward() # grad 계산 - 오차 역전파
        optimizer.step() # 파라미터 업데이트
        train_loss += loss.item() # train loss 누적

    train_loss /= len(trainloader)
    train_loss_list.append(train_loss)

    ######################
    #     Validation
    ######################
    model.eval()
    valid_loss = 0.0
    valid_acc  = 0.0

    with torch.no_grad():
        for x_valid, y_valid in testloader:
            x_valid, y_valid = x_valid.to(device), y_valid.to(device)

            # 예측
            pred_valid = model(x_valid)
            pred_label = (pred_valid > 0.5).type(torch.int32)

            # 평가
            ## Loss 계산
            loss_valid = loss_fn(pred_valid, y_valid)
            valid_loss += loss_valid.item()

            ## Accuracy 계산
            valid_acc += torch.sum(pred_label == y_valid).item()

        valid_loss /= len(testloader)
        valid_acc /= len(testloader.dataset)

    valid_loss_list.append(valid_loss)
    valid_acc_list.append(valid_acc)

    print(f"[{epoch+1}/{N_EPOCH}] Train Loss: {train_loss}, Validation Loss: {valid_loss}, Validation Accuracy: {valid_acc}")

    ##############################
    # 조기종료, 모델 저장처리
    #     저장: 현 EPOCH valide Loss가 best_score보다 개선된 경우 저장
    ##############################
    if valid_loss < best_score:
        print(f"====> 모델저장: {epoch+1}, Epoch - 이전 valid_loss: {best_score}, 현재 valid_loss: {valid_loss}")
        best_score = valid_loss
        torch.save(model, save_model_path)
        trigger_cnt = 0
    else:
        trigger_cnt += 1
        if patience == trigger_cnt:
            print(f"====> {epoch+1} Epoch에서 조기종료-{best_score}에서 개선 안됨")
            break

e = time.time()
print(f"학습시간: {e - s}")
```

    [1/1000] Train Loss: 0.6741175055503845, Validation Loss: 0.6708158850669861, Validation Accuracy: 0.6293706293706294
    ====> 모델저장: 1, Epoch - 이전 valid_loss: inf, 현재 valid_loss: 0.6708158850669861
    [2/1000] Train Loss: 0.6678630113601685, Validation Loss: 0.6614448428153992, Validation Accuracy: 0.6363636363636364
    ====> 모델저장: 2, Epoch - 이전 valid_loss: 0.6708158850669861, 현재 valid_loss: 0.6614448428153992
    [3/1000] Train Loss: 0.6614936292171478, Validation Loss: 0.6521831750869751, Validation Accuracy: 0.6433566433566433
    ====> 모델저장: 3, Epoch - 이전 valid_loss: 0.6614448428153992, 현재 valid_loss: 0.6521831750869751
    [4/1000] Train Loss: 0.6519761383533478, Validation Loss: 0.6430516839027405, Validation Accuracy: 0.6573426573426573
    ====> 모델저장: 4, Epoch - 이전 valid_loss: 0.6521831750869751, 현재 valid_loss: 0.6430516839027405
    [5/1000] Train Loss: 0.6439206302165985, Validation Loss: 0.6342710852622986, Validation Accuracy: 0.6643356643356644
    ====> 모델저장: 5, Epoch - 이전 valid_loss: 0.6430516839027405, 현재 valid_loss: 0.6342710852622986
    [6/1000] Train Loss: 0.6359294950962067, Validation Loss: 0.6255669593811035, Validation Accuracy: 0.6713286713286714
    ====> 모델저장: 6, Epoch - 이전 valid_loss: 0.6342710852622986, 현재 valid_loss: 0.6255669593811035
    [7/1000] Train Loss: 0.6280315816402435, Validation Loss: 0.6170052886009216, Validation Accuracy: 0.7202797202797203
    ====> 모델저장: 7, Epoch - 이전 valid_loss: 0.6255669593811035, 현재 valid_loss: 0.6170052886009216
    [8/1000] Train Loss: 0.6209920048713684, Validation Loss: 0.6082406044006348, Validation Accuracy: 0.7552447552447552
    ====> 모델저장: 8, Epoch - 이전 valid_loss: 0.6170052886009216, 현재 valid_loss: 0.6082406044006348
    [9/1000] Train Loss: 0.6122882068157196, Validation Loss: 0.5992779731750488, Validation Accuracy: 0.7762237762237763
    ====> 모델저장: 9, Epoch - 이전 valid_loss: 0.6082406044006348, 현재 valid_loss: 0.5992779731750488
    [10/1000] Train Loss: 0.6038097143173218, Validation Loss: 0.5899872779846191, Validation Accuracy: 0.7972027972027972
    ====> 모델저장: 10, Epoch - 이전 valid_loss: 0.5992779731750488, 현재 valid_loss: 0.5899872779846191
    [11/1000] Train Loss: 0.5941876471042633, Validation Loss: 0.5802217721939087, Validation Accuracy: 0.8321678321678322
    ====> 모델저장: 11, Epoch - 이전 valid_loss: 0.5899872779846191, 현재 valid_loss: 0.5802217721939087
    [12/1000] Train Loss: 0.5875888466835022, Validation Loss: 0.5697325468063354, Validation Accuracy: 0.8461538461538461
    ====> 모델저장: 12, Epoch - 이전 valid_loss: 0.5802217721939087, 현재 valid_loss: 0.5697325468063354
    [13/1000] Train Loss: 0.5770463645458221, Validation Loss: 0.5581168532371521, Validation Accuracy: 0.8601398601398601
    ====> 모델저장: 13, Epoch - 이전 valid_loss: 0.5697325468063354, 현재 valid_loss: 0.5581168532371521
    [14/1000] Train Loss: 0.5681036114692688, Validation Loss: 0.5456867814064026, Validation Accuracy: 0.8741258741258742
    ====> 모델저장: 14, Epoch - 이전 valid_loss: 0.5581168532371521, 현재 valid_loss: 0.5456867814064026
    [15/1000] Train Loss: 0.557398647069931, Validation Loss: 0.5325140953063965, Validation Accuracy: 0.8951048951048951
    ====> 모델저장: 15, Epoch - 이전 valid_loss: 0.5456867814064026, 현재 valid_loss: 0.5325140953063965
    [16/1000] Train Loss: 0.5435732305049896, Validation Loss: 0.5188500881195068, Validation Accuracy: 0.9090909090909091
    ====> 모델저장: 16, Epoch - 이전 valid_loss: 0.5325140953063965, 현재 valid_loss: 0.5188500881195068
    [17/1000] Train Loss: 0.5307561755180359, Validation Loss: 0.5047910213470459, Validation Accuracy: 0.9230769230769231
    ====> 모델저장: 17, Epoch - 이전 valid_loss: 0.5188500881195068, 현재 valid_loss: 0.5047910213470459
    [18/1000] Train Loss: 0.517554372549057, Validation Loss: 0.49033787846565247, Validation Accuracy: 0.9370629370629371
    ====> 모델저장: 18, Epoch - 이전 valid_loss: 0.5047910213470459, 현재 valid_loss: 0.49033787846565247
    [19/1000] Train Loss: 0.5020811855792999, Validation Loss: 0.4755590558052063, Validation Accuracy: 0.9300699300699301
    ====> 모델저장: 19, Epoch - 이전 valid_loss: 0.49033787846565247, 현재 valid_loss: 0.4755590558052063
    [20/1000] Train Loss: 0.4903675466775894, Validation Loss: 0.4606221914291382, Validation Accuracy: 0.9300699300699301
    ====> 모델저장: 20, Epoch - 이전 valid_loss: 0.4755590558052063, 현재 valid_loss: 0.4606221914291382
    [21/1000] Train Loss: 0.47660303115844727, Validation Loss: 0.4456084668636322, Validation Accuracy: 0.9370629370629371
    ====> 모델저장: 21, Epoch - 이전 valid_loss: 0.4606221914291382, 현재 valid_loss: 0.4456084668636322
    [22/1000] Train Loss: 0.45938338339328766, Validation Loss: 0.43059617280960083, Validation Accuracy: 0.951048951048951
    ====> 모델저장: 22, Epoch - 이전 valid_loss: 0.4456084668636322, 현재 valid_loss: 0.43059617280960083
    [23/1000] Train Loss: 0.44604532420635223, Validation Loss: 0.4156356453895569, Validation Accuracy: 0.951048951048951
    ====> 모델저장: 23, Epoch - 이전 valid_loss: 0.43059617280960083, 현재 valid_loss: 0.4156356453895569
    [24/1000] Train Loss: 0.435753658413887, Validation Loss: 0.40084460377693176, Validation Accuracy: 0.951048951048951
    ====> 모델저장: 24, Epoch - 이전 valid_loss: 0.4156356453895569, 현재 valid_loss: 0.40084460377693176
    [25/1000] Train Loss: 0.41685838997364044, Validation Loss: 0.38615837693214417, Validation Accuracy: 0.951048951048951
    ====> 모델저장: 25, Epoch - 이전 valid_loss: 0.40084460377693176, 현재 valid_loss: 0.38615837693214417
    [26/1000] Train Loss: 0.40195851027965546, Validation Loss: 0.37165987491607666, Validation Accuracy: 0.9440559440559441
    ====> 모델저장: 26, Epoch - 이전 valid_loss: 0.38615837693214417, 현재 valid_loss: 0.37165987491607666
    [27/1000] Train Loss: 0.38718321919441223, Validation Loss: 0.35732731223106384, Validation Accuracy: 0.9440559440559441
    ====> 모델저장: 27, Epoch - 이전 valid_loss: 0.37165987491607666, 현재 valid_loss: 0.35732731223106384
    [28/1000] Train Loss: 0.3756188303232193, Validation Loss: 0.34318527579307556, Validation Accuracy: 0.9440559440559441
    ====> 모델저장: 28, Epoch - 이전 valid_loss: 0.35732731223106384, 현재 valid_loss: 0.34318527579307556
    [29/1000] Train Loss: 0.36218634247779846, Validation Loss: 0.3293122947216034, Validation Accuracy: 0.9440559440559441
    ====> 모델저장: 29, Epoch - 이전 valid_loss: 0.34318527579307556, 현재 valid_loss: 0.3293122947216034
    [30/1000] Train Loss: 0.34152472019195557, Validation Loss: 0.3156229257583618, Validation Accuracy: 0.9440559440559441
    ====> 모델저장: 30, Epoch - 이전 valid_loss: 0.3293122947216034, 현재 valid_loss: 0.3156229257583618
    [31/1000] Train Loss: 0.33133769035339355, Validation Loss: 0.302140474319458, Validation Accuracy: 0.951048951048951
    ====> 모델저장: 31, Epoch - 이전 valid_loss: 0.3156229257583618, 현재 valid_loss: 0.302140474319458
    [32/1000] Train Loss: 0.3215331882238388, Validation Loss: 0.2890358567237854, Validation Accuracy: 0.958041958041958
    ====> 모델저장: 32, Epoch - 이전 valid_loss: 0.302140474319458, 현재 valid_loss: 0.2890358567237854
    [33/1000] Train Loss: 0.29992568492889404, Validation Loss: 0.2762816250324249, Validation Accuracy: 0.958041958041958
    ====> 모델저장: 33, Epoch - 이전 valid_loss: 0.2890358567237854, 현재 valid_loss: 0.2762816250324249
    [34/1000] Train Loss: 0.2927548587322235, Validation Loss: 0.26399102807044983, Validation Accuracy: 0.958041958041958
    ====> 모델저장: 34, Epoch - 이전 valid_loss: 0.2762816250324249, 현재 valid_loss: 0.26399102807044983
    [35/1000] Train Loss: 0.2798386812210083, Validation Loss: 0.2521766126155853, Validation Accuracy: 0.958041958041958
    ====> 모델저장: 35, Epoch - 이전 valid_loss: 0.26399102807044983, 현재 valid_loss: 0.2521766126155853
    [36/1000] Train Loss: 0.2743297666311264, Validation Loss: 0.240827277302742, Validation Accuracy: 0.958041958041958
    ====> 모델저장: 36, Epoch - 이전 valid_loss: 0.2521766126155853, 현재 valid_loss: 0.240827277302742
    [37/1000] Train Loss: 0.26038219034671783, Validation Loss: 0.22999005019664764, Validation Accuracy: 0.958041958041958
    ====> 모델저장: 37, Epoch - 이전 valid_loss: 0.240827277302742, 현재 valid_loss: 0.22999005019664764
    [38/1000] Train Loss: 0.24617457389831543, Validation Loss: 0.21968670189380646, Validation Accuracy: 0.958041958041958
    ====> 모델저장: 38, Epoch - 이전 valid_loss: 0.22999005019664764, 현재 valid_loss: 0.21968670189380646
    [39/1000] Train Loss: 0.23627957701683044, Validation Loss: 0.20990131795406342, Validation Accuracy: 0.965034965034965
    ====> 모델저장: 39, Epoch - 이전 valid_loss: 0.21968670189380646, 현재 valid_loss: 0.20990131795406342
    [40/1000] Train Loss: 0.22506674379110336, Validation Loss: 0.20059525966644287, Validation Accuracy: 0.965034965034965
    ====> 모델저장: 40, Epoch - 이전 valid_loss: 0.20990131795406342, 현재 valid_loss: 0.20059525966644287
    [41/1000] Train Loss: 0.2195119708776474, Validation Loss: 0.19180943071842194, Validation Accuracy: 0.965034965034965
    ====> 모델저장: 41, Epoch - 이전 valid_loss: 0.20059525966644287, 현재 valid_loss: 0.19180943071842194
    [42/1000] Train Loss: 0.21259357035160065, Validation Loss: 0.18349049985408783, Validation Accuracy: 0.965034965034965
    ====> 모델저장: 42, Epoch - 이전 valid_loss: 0.19180943071842194, 현재 valid_loss: 0.18349049985408783
    [43/1000] Train Loss: 0.2008635625243187, Validation Loss: 0.17563655972480774, Validation Accuracy: 0.965034965034965
    ====> 모델저장: 43, Epoch - 이전 valid_loss: 0.18349049985408783, 현재 valid_loss: 0.17563655972480774
    [44/1000] Train Loss: 0.19014614075422287, Validation Loss: 0.16817213594913483, Validation Accuracy: 0.965034965034965
    ====> 모델저장: 44, Epoch - 이전 valid_loss: 0.17563655972480774, 현재 valid_loss: 0.16817213594913483
    [45/1000] Train Loss: 0.18756898492574692, Validation Loss: 0.16113337874412537, Validation Accuracy: 0.965034965034965
    ====> 모델저장: 45, Epoch - 이전 valid_loss: 0.16817213594913483, 현재 valid_loss: 0.16113337874412537
    [46/1000] Train Loss: 0.18165917694568634, Validation Loss: 0.15450641512870789, Validation Accuracy: 0.972027972027972
    ====> 모델저장: 46, Epoch - 이전 valid_loss: 0.16113337874412537, 현재 valid_loss: 0.15450641512870789
    [47/1000] Train Loss: 0.17236723005771637, Validation Loss: 0.14822958409786224, Validation Accuracy: 0.972027972027972
    ====> 모델저장: 47, Epoch - 이전 valid_loss: 0.15450641512870789, 현재 valid_loss: 0.14822958409786224
    [48/1000] Train Loss: 0.16983720660209656, Validation Loss: 0.14233353734016418, Validation Accuracy: 0.9790209790209791
    ====> 모델저장: 48, Epoch - 이전 valid_loss: 0.14822958409786224, 현재 valid_loss: 0.14233353734016418
    [49/1000] Train Loss: 0.16643912345170975, Validation Loss: 0.1367732137441635, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 49, Epoch - 이전 valid_loss: 0.14233353734016418, 현재 valid_loss: 0.1367732137441635
    [50/1000] Train Loss: 0.15189404785633087, Validation Loss: 0.1315813809633255, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 50, Epoch - 이전 valid_loss: 0.1367732137441635, 현재 valid_loss: 0.1315813809633255
    [51/1000] Train Loss: 0.1564871370792389, Validation Loss: 0.12665173411369324, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 51, Epoch - 이전 valid_loss: 0.1315813809633255, 현재 valid_loss: 0.12665173411369324
    [52/1000] Train Loss: 0.14721008390188217, Validation Loss: 0.12198711931705475, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 52, Epoch - 이전 valid_loss: 0.12665173411369324, 현재 valid_loss: 0.12198711931705475
    [53/1000] Train Loss: 0.14368975162506104, Validation Loss: 0.11760080605745316, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 53, Epoch - 이전 valid_loss: 0.12198711931705475, 현재 valid_loss: 0.11760080605745316
    [54/1000] Train Loss: 0.13746348023414612, Validation Loss: 0.11348062753677368, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 54, Epoch - 이전 valid_loss: 0.11760080605745316, 현재 valid_loss: 0.11348062753677368
    [55/1000] Train Loss: 0.13617891818284988, Validation Loss: 0.10956874489784241, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 55, Epoch - 이전 valid_loss: 0.11348062753677368, 현재 valid_loss: 0.10956874489784241
    [56/1000] Train Loss: 0.1283860094845295, Validation Loss: 0.10589202493429184, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 56, Epoch - 이전 valid_loss: 0.10956874489784241, 현재 valid_loss: 0.10589202493429184
    [57/1000] Train Loss: 0.12722620368003845, Validation Loss: 0.10247378796339035, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 57, Epoch - 이전 valid_loss: 0.10589202493429184, 현재 valid_loss: 0.10247378796339035
    [58/1000] Train Loss: 0.1211077980697155, Validation Loss: 0.09928875416517258, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 58, Epoch - 이전 valid_loss: 0.10247378796339035, 현재 valid_loss: 0.09928875416517258
    [59/1000] Train Loss: 0.1249229796230793, Validation Loss: 0.09629734605550766, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 59, Epoch - 이전 valid_loss: 0.09928875416517258, 현재 valid_loss: 0.09629734605550766
    [60/1000] Train Loss: 0.12163551524281502, Validation Loss: 0.09351880103349686, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 60, Epoch - 이전 valid_loss: 0.09629734605550766, 현재 valid_loss: 0.09351880103349686
    [61/1000] Train Loss: 0.11694461107254028, Validation Loss: 0.09095652401447296, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 61, Epoch - 이전 valid_loss: 0.09351880103349686, 현재 valid_loss: 0.09095652401447296
    [62/1000] Train Loss: 0.11012277007102966, Validation Loss: 0.08852499723434448, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 62, Epoch - 이전 valid_loss: 0.09095652401447296, 현재 valid_loss: 0.08852499723434448
    [63/1000] Train Loss: 0.1156475841999054, Validation Loss: 0.08626413345336914, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 63, Epoch - 이전 valid_loss: 0.08852499723434448, 현재 valid_loss: 0.08626413345336914
    [64/1000] Train Loss: 0.11127766594290733, Validation Loss: 0.08410678058862686, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 64, Epoch - 이전 valid_loss: 0.08626413345336914, 현재 valid_loss: 0.08410678058862686
    [65/1000] Train Loss: 0.10898396372795105, Validation Loss: 0.08211714029312134, Validation Accuracy: 0.9790209790209791
    ====> 모델저장: 65, Epoch - 이전 valid_loss: 0.08410678058862686, 현재 valid_loss: 0.08211714029312134
    [66/1000] Train Loss: 0.10059763118624687, Validation Loss: 0.0802692398428917, Validation Accuracy: 0.9790209790209791
    ====> 모델저장: 66, Epoch - 이전 valid_loss: 0.08211714029312134, 현재 valid_loss: 0.0802692398428917
    [67/1000] Train Loss: 0.10426374897360802, Validation Loss: 0.0785137191414833, Validation Accuracy: 0.9790209790209791
    ====> 모델저장: 67, Epoch - 이전 valid_loss: 0.0802692398428917, 현재 valid_loss: 0.0785137191414833
    [68/1000] Train Loss: 0.10317957401275635, Validation Loss: 0.07690529525279999, Validation Accuracy: 0.9790209790209791
    ====> 모델저장: 68, Epoch - 이전 valid_loss: 0.0785137191414833, 현재 valid_loss: 0.07690529525279999
    [69/1000] Train Loss: 0.09973451495170593, Validation Loss: 0.07538805156946182, Validation Accuracy: 0.9790209790209791
    ====> 모델저장: 69, Epoch - 이전 valid_loss: 0.07690529525279999, 현재 valid_loss: 0.07538805156946182
    [70/1000] Train Loss: 0.09990551695227623, Validation Loss: 0.07396455109119415, Validation Accuracy: 0.9790209790209791
    ====> 모델저장: 70, Epoch - 이전 valid_loss: 0.07538805156946182, 현재 valid_loss: 0.07396455109119415
    [71/1000] Train Loss: 0.09642726182937622, Validation Loss: 0.072609543800354, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 71, Epoch - 이전 valid_loss: 0.07396455109119415, 현재 valid_loss: 0.072609543800354
    [72/1000] Train Loss: 0.09503374993801117, Validation Loss: 0.07134933024644852, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 72, Epoch - 이전 valid_loss: 0.072609543800354, 현재 valid_loss: 0.07134933024644852
    [73/1000] Train Loss: 0.09588029608130455, Validation Loss: 0.07014207541942596, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 73, Epoch - 이전 valid_loss: 0.07134933024644852, 현재 valid_loss: 0.07014207541942596
    [74/1000] Train Loss: 0.08599577471613884, Validation Loss: 0.06898954510688782, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 74, Epoch - 이전 valid_loss: 0.07014207541942596, 현재 valid_loss: 0.06898954510688782
    [75/1000] Train Loss: 0.08852285519242287, Validation Loss: 0.06786073744297028, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 75, Epoch - 이전 valid_loss: 0.06898954510688782, 현재 valid_loss: 0.06786073744297028
    [76/1000] Train Loss: 0.08933375030755997, Validation Loss: 0.06680620461702347, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 76, Epoch - 이전 valid_loss: 0.06786073744297028, 현재 valid_loss: 0.06680620461702347
    [77/1000] Train Loss: 0.08811774477362633, Validation Loss: 0.06574998795986176, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 77, Epoch - 이전 valid_loss: 0.06680620461702347, 현재 valid_loss: 0.06574998795986176
    [78/1000] Train Loss: 0.0884055569767952, Validation Loss: 0.06482389569282532, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 78, Epoch - 이전 valid_loss: 0.06574998795986176, 현재 valid_loss: 0.06482389569282532
    [79/1000] Train Loss: 0.08322933316230774, Validation Loss: 0.06395028531551361, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 79, Epoch - 이전 valid_loss: 0.06482389569282532, 현재 valid_loss: 0.06395028531551361
    [80/1000] Train Loss: 0.08357486128807068, Validation Loss: 0.06313171982765198, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 80, Epoch - 이전 valid_loss: 0.06395028531551361, 현재 valid_loss: 0.06313171982765198
    [81/1000] Train Loss: 0.08680471405386925, Validation Loss: 0.06233074516057968, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 81, Epoch - 이전 valid_loss: 0.06313171982765198, 현재 valid_loss: 0.06233074516057968
    [82/1000] Train Loss: 0.08348387107253075, Validation Loss: 0.06161367520689964, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 82, Epoch - 이전 valid_loss: 0.06233074516057968, 현재 valid_loss: 0.06161367520689964
    [83/1000] Train Loss: 0.08275306969881058, Validation Loss: 0.06093236058950424, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 83, Epoch - 이전 valid_loss: 0.06161367520689964, 현재 valid_loss: 0.06093236058950424
    [84/1000] Train Loss: 0.08187570050358772, Validation Loss: 0.06028825044631958, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 84, Epoch - 이전 valid_loss: 0.06093236058950424, 현재 valid_loss: 0.06028825044631958
    [85/1000] Train Loss: 0.081184271723032, Validation Loss: 0.059705086052417755, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 85, Epoch - 이전 valid_loss: 0.06028825044631958, 현재 valid_loss: 0.059705086052417755
    [86/1000] Train Loss: 0.07916170358657837, Validation Loss: 0.0591803714632988, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 86, Epoch - 이전 valid_loss: 0.059705086052417755, 현재 valid_loss: 0.0591803714632988
    [87/1000] Train Loss: 0.0782342441380024, Validation Loss: 0.058634284883737564, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 87, Epoch - 이전 valid_loss: 0.0591803714632988, 현재 valid_loss: 0.058634284883737564
    [88/1000] Train Loss: 0.07785648852586746, Validation Loss: 0.05814041569828987, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 88, Epoch - 이전 valid_loss: 0.058634284883737564, 현재 valid_loss: 0.05814041569828987
    [89/1000] Train Loss: 0.07750289514660835, Validation Loss: 0.057722967118024826, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 89, Epoch - 이전 valid_loss: 0.05814041569828987, 현재 valid_loss: 0.057722967118024826
    [90/1000] Train Loss: 0.07606060802936554, Validation Loss: 0.05734610930085182, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 90, Epoch - 이전 valid_loss: 0.057722967118024826, 현재 valid_loss: 0.05734610930085182
    [91/1000] Train Loss: 0.07399865612387657, Validation Loss: 0.056982867419719696, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 91, Epoch - 이전 valid_loss: 0.05734610930085182, 현재 valid_loss: 0.056982867419719696
    [92/1000] Train Loss: 0.06503233499825001, Validation Loss: 0.05662105977535248, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 92, Epoch - 이전 valid_loss: 0.056982867419719696, 현재 valid_loss: 0.05662105977535248
    [93/1000] Train Loss: 0.0720178410410881, Validation Loss: 0.05633613467216492, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 93, Epoch - 이전 valid_loss: 0.05662105977535248, 현재 valid_loss: 0.05633613467216492
    [94/1000] Train Loss: 0.07242501527070999, Validation Loss: 0.05592881143093109, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 94, Epoch - 이전 valid_loss: 0.05633613467216492, 현재 valid_loss: 0.05592881143093109
    [95/1000] Train Loss: 0.0733112096786499, Validation Loss: 0.055534541606903076, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 95, Epoch - 이전 valid_loss: 0.05592881143093109, 현재 valid_loss: 0.055534541606903076
    [96/1000] Train Loss: 0.06985883601009846, Validation Loss: 0.05524216964840889, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 96, Epoch - 이전 valid_loss: 0.055534541606903076, 현재 valid_loss: 0.05524216964840889
    [97/1000] Train Loss: 0.07127485796809196, Validation Loss: 0.05495191738009453, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 97, Epoch - 이전 valid_loss: 0.05524216964840889, 현재 valid_loss: 0.05495191738009453
    [98/1000] Train Loss: 0.06984798982739449, Validation Loss: 0.05468336492776871, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 98, Epoch - 이전 valid_loss: 0.05495191738009453, 현재 valid_loss: 0.05468336492776871
    [99/1000] Train Loss: 0.06973106041550636, Validation Loss: 0.054332684725522995, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 99, Epoch - 이전 valid_loss: 0.05468336492776871, 현재 valid_loss: 0.054332684725522995
    [100/1000] Train Loss: 0.0691126137971878, Validation Loss: 0.05408832058310509, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 100, Epoch - 이전 valid_loss: 0.054332684725522995, 현재 valid_loss: 0.05408832058310509
    [101/1000] Train Loss: 0.06760479509830475, Validation Loss: 0.05388537794351578, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 101, Epoch - 이전 valid_loss: 0.05408832058310509, 현재 valid_loss: 0.05388537794351578
    [102/1000] Train Loss: 0.06681676208972931, Validation Loss: 0.053635451942682266, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 102, Epoch - 이전 valid_loss: 0.05388537794351578, 현재 valid_loss: 0.053635451942682266
    [103/1000] Train Loss: 0.06774579733610153, Validation Loss: 0.053449273109436035, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 103, Epoch - 이전 valid_loss: 0.053635451942682266, 현재 valid_loss: 0.053449273109436035
    [104/1000] Train Loss: 0.06614266708493233, Validation Loss: 0.053271982818841934, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 104, Epoch - 이전 valid_loss: 0.053449273109436035, 현재 valid_loss: 0.053271982818841934
    [105/1000] Train Loss: 0.0630611702799797, Validation Loss: 0.053148940205574036, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 105, Epoch - 이전 valid_loss: 0.053271982818841934, 현재 valid_loss: 0.053148940205574036
    [106/1000] Train Loss: 0.06608235277235508, Validation Loss: 0.05302254110574722, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 106, Epoch - 이전 valid_loss: 0.053148940205574036, 현재 valid_loss: 0.05302254110574722
    [107/1000] Train Loss: 0.06524216756224632, Validation Loss: 0.0529605858027935, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 107, Epoch - 이전 valid_loss: 0.05302254110574722, 현재 valid_loss: 0.0529605858027935
    [108/1000] Train Loss: 0.06572580896317959, Validation Loss: 0.05289063602685928, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 108, Epoch - 이전 valid_loss: 0.0529605858027935, 현재 valid_loss: 0.05289063602685928
    [109/1000] Train Loss: 0.06497154012322426, Validation Loss: 0.05277571454644203, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 109, Epoch - 이전 valid_loss: 0.05289063602685928, 현재 valid_loss: 0.05277571454644203
    [110/1000] Train Loss: 0.05475056543946266, Validation Loss: 0.052610550075769424, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 110, Epoch - 이전 valid_loss: 0.05277571454644203, 현재 valid_loss: 0.052610550075769424
    [111/1000] Train Loss: 0.05959320813417435, Validation Loss: 0.05228496342897415, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 111, Epoch - 이전 valid_loss: 0.052610550075769424, 현재 valid_loss: 0.05228496342897415
    [112/1000] Train Loss: 0.05971934646368027, Validation Loss: 0.05203291028738022, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 112, Epoch - 이전 valid_loss: 0.05228496342897415, 현재 valid_loss: 0.05203291028738022
    [113/1000] Train Loss: 0.061487214639782906, Validation Loss: 0.05177511274814606, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 113, Epoch - 이전 valid_loss: 0.05203291028738022, 현재 valid_loss: 0.05177511274814606
    [114/1000] Train Loss: 0.06093236990272999, Validation Loss: 0.051537156105041504, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 114, Epoch - 이전 valid_loss: 0.05177511274814606, 현재 valid_loss: 0.051537156105041504
    [115/1000] Train Loss: 0.061283178627491, Validation Loss: 0.05133724957704544, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 115, Epoch - 이전 valid_loss: 0.051537156105041504, 현재 valid_loss: 0.05133724957704544
    [116/1000] Train Loss: 0.0606058593839407, Validation Loss: 0.05121348053216934, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 116, Epoch - 이전 valid_loss: 0.05133724957704544, 현재 valid_loss: 0.05121348053216934
    [117/1000] Train Loss: 0.06068597920238972, Validation Loss: 0.051164660602808, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 117, Epoch - 이전 valid_loss: 0.05121348053216934, 현재 valid_loss: 0.051164660602808
    [118/1000] Train Loss: 0.05308140069246292, Validation Loss: 0.05114025995135307, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 118, Epoch - 이전 valid_loss: 0.051164660602808, 현재 valid_loss: 0.05114025995135307
    [119/1000] Train Loss: 0.058262865990400314, Validation Loss: 0.051101572811603546, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 119, Epoch - 이전 valid_loss: 0.05114025995135307, 현재 valid_loss: 0.051101572811603546
    [120/1000] Train Loss: 0.05805416777729988, Validation Loss: 0.051069919019937515, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 120, Epoch - 이전 valid_loss: 0.051101572811603546, 현재 valid_loss: 0.051069919019937515
    [121/1000] Train Loss: 0.059048961848020554, Validation Loss: 0.05096794664859772, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 121, Epoch - 이전 valid_loss: 0.051069919019937515, 현재 valid_loss: 0.05096794664859772
    [122/1000] Train Loss: 0.059061404317617416, Validation Loss: 0.05090993642807007, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 122, Epoch - 이전 valid_loss: 0.05096794664859772, 현재 valid_loss: 0.05090993642807007
    [123/1000] Train Loss: 0.05537532642483711, Validation Loss: 0.0508354976773262, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 123, Epoch - 이전 valid_loss: 0.05090993642807007, 현재 valid_loss: 0.0508354976773262
    [124/1000] Train Loss: 0.05099874176084995, Validation Loss: 0.05065483599901199, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 124, Epoch - 이전 valid_loss: 0.0508354976773262, 현재 valid_loss: 0.05065483599901199
    [125/1000] Train Loss: 0.05679813772439957, Validation Loss: 0.05050450935959816, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 125, Epoch - 이전 valid_loss: 0.05065483599901199, 현재 valid_loss: 0.05050450935959816
    [126/1000] Train Loss: 0.05569666996598244, Validation Loss: 0.05038188770413399, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 126, Epoch - 이전 valid_loss: 0.05050450935959816, 현재 valid_loss: 0.05038188770413399
    [127/1000] Train Loss: 0.05543568730354309, Validation Loss: 0.050310976803302765, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 127, Epoch - 이전 valid_loss: 0.05038188770413399, 현재 valid_loss: 0.050310976803302765
    [128/1000] Train Loss: 0.05482260324060917, Validation Loss: 0.05028534308075905, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 128, Epoch - 이전 valid_loss: 0.050310976803302765, 현재 valid_loss: 0.05028534308075905
    [129/1000] Train Loss: 0.052823975682258606, Validation Loss: 0.050181303173303604, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 129, Epoch - 이전 valid_loss: 0.05028534308075905, 현재 valid_loss: 0.050181303173303604
    [130/1000] Train Loss: 0.04687358997762203, Validation Loss: 0.049921583384275436, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 130, Epoch - 이전 valid_loss: 0.050181303173303604, 현재 valid_loss: 0.049921583384275436
    [131/1000] Train Loss: 0.05468953400850296, Validation Loss: 0.049734003841876984, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 131, Epoch - 이전 valid_loss: 0.049921583384275436, 현재 valid_loss: 0.049734003841876984
    [132/1000] Train Loss: 0.05397849716246128, Validation Loss: 0.04961743950843811, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 132, Epoch - 이전 valid_loss: 0.049734003841876984, 현재 valid_loss: 0.04961743950843811
    [133/1000] Train Loss: 0.05403509736061096, Validation Loss: 0.049540527164936066, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 133, Epoch - 이전 valid_loss: 0.04961743950843811, 현재 valid_loss: 0.049540527164936066
    [134/1000] Train Loss: 0.053347526118159294, Validation Loss: 0.04952680319547653, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 134, Epoch - 이전 valid_loss: 0.049540527164936066, 현재 valid_loss: 0.04952680319547653
    [135/1000] Train Loss: 0.052017830312252045, Validation Loss: 0.0495179146528244, Validation Accuracy: 0.986013986013986
    ====> 모델저장: 135, Epoch - 이전 valid_loss: 0.04952680319547653, 현재 valid_loss: 0.0495179146528244
    [136/1000] Train Loss: 0.05281858518719673, Validation Loss: 0.04953191429376602, Validation Accuracy: 0.986013986013986
    [137/1000] Train Loss: 0.05282251164317131, Validation Loss: 0.04953896254301071, Validation Accuracy: 0.986013986013986
    [138/1000] Train Loss: 0.05204973928630352, Validation Loss: 0.04957572743296623, Validation Accuracy: 0.986013986013986
    [139/1000] Train Loss: 0.05195312015712261, Validation Loss: 0.04960506036877632, Validation Accuracy: 0.986013986013986
    [140/1000] Train Loss: 0.05121877044439316, Validation Loss: 0.04977471008896828, Validation Accuracy: 0.986013986013986
    [141/1000] Train Loss: 0.04983697831630707, Validation Loss: 0.0498589351773262, Validation Accuracy: 0.986013986013986
    [142/1000] Train Loss: 0.050653666257858276, Validation Loss: 0.050016943365335464, Validation Accuracy: 0.986013986013986
    [143/1000] Train Loss: 0.04653195757418871, Validation Loss: 0.0500168651342392, Validation Accuracy: 0.986013986013986
    [144/1000] Train Loss: 0.05047500878572464, Validation Loss: 0.05001133307814598, Validation Accuracy: 0.986013986013986
    [145/1000] Train Loss: 0.04914079234004021, Validation Loss: 0.049958087503910065, Validation Accuracy: 0.986013986013986
    [146/1000] Train Loss: 0.04404107294976711, Validation Loss: 0.04984227940440178, Validation Accuracy: 0.986013986013986
    [147/1000] Train Loss: 0.04802554100751877, Validation Loss: 0.0498347170650959, Validation Accuracy: 0.986013986013986
    [148/1000] Train Loss: 0.048466918990015984, Validation Loss: 0.049879979342222214, Validation Accuracy: 0.986013986013986
    [149/1000] Train Loss: 0.04388879518955946, Validation Loss: 0.04980580508708954, Validation Accuracy: 0.986013986013986
    [150/1000] Train Loss: 0.048354990780353546, Validation Loss: 0.04971960559487343, Validation Accuracy: 0.986013986013986
    [151/1000] Train Loss: 0.045813657343387604, Validation Loss: 0.049756355583667755, Validation Accuracy: 0.986013986013986
    [152/1000] Train Loss: 0.03726680763065815, Validation Loss: 0.049874868243932724, Validation Accuracy: 0.986013986013986
    [153/1000] Train Loss: 0.047502001747488976, Validation Loss: 0.0499417781829834, Validation Accuracy: 0.986013986013986
    [154/1000] Train Loss: 0.04689234681427479, Validation Loss: 0.04999479278922081, Validation Accuracy: 0.986013986013986
    [155/1000] Train Loss: 0.0438510999083519, Validation Loss: 0.04990578070282936, Validation Accuracy: 0.9790209790209791
    ====> 155 Epoch에서 조기종료-0.0495179146528244에서 개선 안됨
    학습시간: 2.3831734657287598



```python
# 결과 시각화
plt.rcParams['font.family'] = 'Malgun gothic'

plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.plot(train_loss_list, label='Train')
plt.plot(valid_loss_list, label='Valid')
plt.title('Epoch별 Loss 변화')
plt.ylim(0, 0.6)
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(valid_acc_list)
plt.title('Validation Accuracy')
plt.tight_layout()
plt.show()
```


​    
![png](/../../images/2023-10-14-01.모델저장 문제유형별 모델생성/output_133_0.png)
​    


### 모델 Train 결과 확인/평가


```python
# 저장된 모델 로딩
best_model = torch.load(save_model_path)
```


```python
# 모델이 학습한 데이터는 전처리 된것 (Standard Scaling)
## 예측(추론) 핳 데이터도 같은 전처리를 해야한다.
```


```python
x_test_tensor.shape
```




    torch.Size([143, 30])




```python
pred_new = best_model(x_test_tensor)
pred_new.shape
```




    torch.Size([143, 1])




```python
pred_new[:10] # positive(1)일 확률
```




    tensor([[9.9620e-01],
            [1.8350e-05],
            [9.9765e-01],
            [9.9989e-01],
            [9.9965e-01],
            [1.5421e-01],
            [4.7193e-04],
            [9.9993e-01],
            [9.9931e-01],
            [9.7072e-05]], grad_fn=<SliceBackward0>)




```python
# 확률 -> class index
pred_new_label = (pred_new > 0.5).type(torch.int32)
pred_new_label[:10]
```




    tensor([[1],
            [0],
            [1],
            [1],
            [1],
            [0],
            [0],
            [1],
            [1],
            [0]], dtype=torch.int32)




```python
y_test_tensor[:10]
```




    tensor([[1.],
            [0.],
            [1.],
            [1.],
            [1.],
            [1.],
            [0.],
            [1.],
            [1.],
            [0.]])




```python
# test_dataloader 평가
best_model = best_model.to(device)
best_model.eval()

valid_loss = 0.0
valid_acc = 0.0

with torch.no_grad():
        for x_valid, y_valid in testloader:
            x_valid, y_valid = x_valid.to(device), y_valid.to(device)

            # 예측
            pred_valid = model(x_valid)
            pred_label = (pred_valid > 0.5).type(torch.int32)

            # 평가
            ## Loss 계산
            loss_valid = loss_fn(pred_valid, y_valid)
            valid_loss += loss_valid

            ## Accuracy 계산
            valid_acc += torch.sum(pred_label == y_valid).item()

        valid_loss /= len(testloader)
        valid_acc /= len(testloader.dataset)

        valid_loss_list.append(valid_loss)
        valid_acc_list.append(valid_acc)
```


```python
valid_loss
```




    tensor(0.0499)




```python
valid_acc
```




    0.9790209790209791

